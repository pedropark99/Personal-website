{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Writing Spark DataFrames to Elasticsearch/Opensearch databases\n",
        "description: \"Elasticsearch and Opensearch are two very popular No-SQL databases. In this post, I want to address how can you write data from a Spark DataFrame into an Elasticsearch/Opensearch database.\"\n",
        "date: \"2025-03-16\"\n",
        "image: \"../spark-elasticsearch-cover.webp\"\n",
        "---\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Recently, I had to develop a simple connector (i.e. a Python function) that allows a user to upload/write\n",
        "data from a Spark DataFrame into both an Elasticsearch and an Opensearch database. If you are not familiar\n",
        "with Elasticsearch or Opensearch, they are both No-SQL databases. Which means that they are databases that\n",
        "store a collection of JSON documents, instead of storing a collection of tables as you would expect from\n",
        "a traditional SQL database like PostgreSQL.\n",
        "\n",
        "Our data lake is processed by Apache Spark and Databricks, and some of the data that is being processed\n",
        "needs to be written into an Elasticsearch/Opensearch database, and it is best if we use the high scalability\n",
        "of Apache Spark to write this data. Thus, in this post, I want to describe how you can use the Spark to write data into these databases.\n",
        "\n",
        "## Why this post?\n",
        "\n",
        "Just to be clear, I'm writing this post specially because the existing documentation about the connection between\n",
        "Spark and Elasticsearch/Opensearch is very, I mean, veeeery poor. Therefore, this post serves as\n",
        "an extra documentation, or, an extra resource that you can rely on when writing data from Spark into\n",
        "Elasticsearch/Opensearch.\n",
        "\n",
        "\n",
        "# Spark native data sources\n",
        "\n",
        "If you want to write data from Spark fast and with high scalability, you want to use a Spark native data source\n",
        "to write your data. Maybe you don't know what are Spark native data sources, so let's discuss them now.\n",
        "\n",
        "Spark data sources are described in details at the [Spark documentation](https://spark.apache.org/docs/3.5.3/sql-data-sources.html).\n",
        "Spark supports many different data sources out of the box. For example, it supports static files like CSV, JSON and Parquet files.\n",
        "But, you can also install Maven packages in your environment to use other data sources that were developed by the community.\n",
        "\n",
        "For example, by installing the `spark-sql-kafka` Maven package[^spark-kafka], a new Spark data source becomes available to you, which is the\n",
        "Kafka data source. With this data source, Spark supports streaming with Apache Kafka queues. Therefore, Spark supports multiple data\n",
        "sources out of the box, but you can add more data sources to the list by installing Maven packages that include new data sources\n",
        "to the Spark engine.\n",
        "\n",
        "[^spark-kafka]: <https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10>\n",
        "\n",
        "\n",
        "## Selecting a data source\n",
        "\n",
        "For example, if you are writing a Spark application through `pyspark`, you can choose the specific data source that\n",
        "you want to use in Spark by using the\n",
        "[`format()` method from a `DataFrameWriter` class](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.format.html#pyspark.sql.DataFrameWriter.format).\n",
        "In the example below, I'm using the JSON data source to write data from Spark into a JSON file.\n"
      ],
      "id": "0fbf1b3c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "df = spark.range(1)\n",
        "df.write.format('json')"
      ],
      "id": "6d91d93d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Elasticsearch/Opensearch native Spark data sources\n",
        "\n",
        "\n",
        "Thankfully, both Elasticsearch and Opensearch have a free/open-source Spark native\n",
        "driver that we can use.\n",
        "\n",
        "These drivers are both available as packages in the [Maven repository](https://mvnrepository.com/).\n",
        "If you want to write data to Elasticsearch, then, you are looking for the `elasticsearch-spark` package.\n",
        "But, if you want to write data to Opensearch instead, then, you are looking for the\n",
        "`opensearch-spark` package.\n",
        "\n",
        "You can see more info about these packages in the links below:\n",
        "\n",
        "- <https://mvnrepository.com/artifact/org.opensearch.client/opensearch-spark-30>.\n",
        "- <https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-30>.\n",
        "\n",
        "Now, you need to use the specific version of these packages that match your specific environment.\n",
        "For my example, my environment is this:\n",
        "\n",
        "- Databricks Runtime 15.4 LTS.\n",
        "- Apache Spark 3.5.0.\n",
        "- Scala 2.12.18.\n",
        "- Python 3.11.0.\n",
        "\n",
        "The version of Spark and Scala are the most important infos in this case. Since the version of Spark\n",
        "is 3.5.0, we want to use a version of these packages that are compatible with the 3.* versions of Spark,\n",
        "which are `opensearch-spark-30` and `elasticsearch-spark-30`.\n",
        "\n",
        "And we also need to use a version of these packages that are compatible with the 2.12 version of Scala.\n",
        "This is why, I've installed the `opensearch-spark-30_2.12` and `elasticsearch-spark-30_2.12` versions of\n",
        "these packages in my specific environment.\n",
        "\n",
        "Your specific environment might have different versions, and, therefore, you might need\n",
        "to use a different version of these packages for your specific case. Just be aware of that.\n",
        "\n",
        "In Databricks, you can install these Maven packages in your cluster to make these\n",
        "native Spark data sources available in your environment. If you don't know how to do this, checkout the\n",
        "[Databricks documentation](https://docs.databricks.com/aws/en/libraries/package-repositories#maven-or-spark-package)\n",
        "for this.\n"
      ],
      "id": "16cc423c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}