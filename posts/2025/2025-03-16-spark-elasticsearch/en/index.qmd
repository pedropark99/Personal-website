---
title: Writing Spark DataFrames to Elasticsearch/Opensearch databases
description: "Elasticsearch and Opensearch are two very popular No-SQL databases. In this post, I want to address how can you write data from a Spark DataFrame into an Elasticsearch/Opensearch database."
date: "2025-03-16"
image: "../spark-elasticsearch-cover.webp"
---

# Introduction

Recently, I had to develop a simple connector (i.e. a Python function) that allows a user to upload/write
data from a Spark DataFrame into both an Elasticsearch and an Opensearch database. If you are not familiar
with Elasticsearch or Opensearch, they are both No-SQL databases. Which means that they are databases that
store a collection of JSON documents, instead of storing a collection of tables as you would expect from
a traditional SQL database like PostgreSQL.

Our data lake is processed by Apache Spark and Databricks, and some of the data that is being processed
needs to be written into an Elasticsearch/Opensearch database, and it is best if we use the high scalability
of Apache Spark to write this data. Thus, in this post, I want to describe how you can use the Spark to write data into these databases.

## Why this post?

Just to be clear, I'm writing this post specially because the existing documentation about the connection between
Spark and Elasticsearch/Opensearch is very, I mean, veeeery poor. Therefore, this post serves as
an extra documentation, or, an extra resource that you can rely on when writing data from Spark into
Elasticsearch/Opensearch.


# Spark native data sources

If you want to write data from Spark fast and with high scalability, you want to use a Spark native data source
to write your data. Maybe you don't know what are Spark native data sources, so let's discuss them now.

Spark data sources are described in details at the [Spark documentation](https://spark.apache.org/docs/3.5.3/sql-data-sources.html).
Spark supports many different data sources out of the box. For example, it supports static files like CSV, JSON and Parquet files.
But, you can also install Maven packages in your environment to use other data sources that were developed by the community.

For example, by installing the `spark-sql-kafka` Maven package[^spark-kafka], a new Spark data source becomes available to you, which is the
Kafka data source. With this data source, Spark supports streaming with Apache Kafka queues. Therefore, Spark supports multiple data
sources out of the box, but you can add more data sources to the list by installing Maven packages that include new data sources
to the Spark engine.

[^spark-kafka]: <https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10>


## Selecting a data source

For example, if you are writing a Spark application through `pyspark`, you can choose the specific data source that
you want to use in Spark by using the
[`format()` method from a `DataFrameWriter` class](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.format.html#pyspark.sql.DataFrameWriter.format).
In the example below, I'm using the JSON data source to write data from Spark into a JSON file.

```python
df = spark.range(1)
df.write.format('json')
```



## Elasticsearch/Opensearch native Spark data sources

Both Elasticsearch and Opensearch have a free/open-source Spark native data source
that we can use. These data sources are both available as packages in the [Maven repository](https://mvnrepository.com/).
If you want to write data to Elasticsearch, then, you are looking for the `elasticsearch-spark` package.
But, if you want to write data to Opensearch instead, then, you are looking for the
`opensearch-spark` package.

You can see more info about these packages in the links below:

- <https://mvnrepository.com/artifact/org.opensearch.client/opensearch-spark-30>.
- <https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-30>.

Now, you need to use the specific version of these packages that match your specific environment.
For example, my environment is this:

- Databricks Runtime 15.4 LTS.
- Apache Spark 3.5.0.
- Scala 2.12.18.
- Python 3.11.0.

The version of Spark and Scala are the most important infos in this case. Since the version of Spark
is 3.5.0, we want to use a version of these packages that are compatible with the 3.* versions of Spark,
which are `opensearch-spark-30` and `elasticsearch-spark-30`.

And we also need to use a version of these packages that are compatible with the 2.12 version of Scala.
This is why, I've installed the `opensearch-spark-30_2.12` and `elasticsearch-spark-30_2.12` versions of
these packages in my specific environment.

Your specific environment might have different versions, and, therefore, you might need
to use a different version of these packages for your specific case. Just be aware of that.

In Databricks, you can install these Maven packages in your cluster to make these
native Spark data sources available in your environment. If you don't know how to do this, checkout the
[Databricks documentation](https://docs.databricks.com/aws/en/libraries/package-repositories#maven-or-spark-package)
for this.


## Using the Elasticsearch/Opensearch data source

After you install these Maven packages, you should be able to select the Elasticsearch/Opensearch
data source by using the strings `"org.opensearch.spark.sql"` and `"org.elasticsearch.spark.sql"`
in the `format()` method.

Now, in order to use the data source, you have to set some options with the `option()`
method of the `DataFrameWriter` class. These are the mandatory options that you have to set
in the Opensearch driver:

- `opensearch.nodes`: the hostname where your Opensearch server is hosted.
- `opensearch.port`: the port to use in the connection with the Opensearch server.
- `opensearch.resource`: the name of the target index in the Opensearch database to write the data to.
- `opensearch.net.http.auth.user`: the username to login into the Opensearch server.
- `opensearch.net.http.auth.pass`: the password to login into the Opensearch server.
- `opensearch.write.operation`: the type of operation (e.g. `index`, `create`, `upsert`, etc.) that you want to use when writing the data to the Opensearch server.


In the example below, we are selecting the Opensearch data source, and setting these mandatory
options. Of course, the values of these options are not real, they are fictitious.

```python
df = spark.range(1)
index = "my_index"
host = "https://my-hostname.com"
port = 1234
user = "my_username"
password = "my_secret_pass"

df.write.format("org.opensearch.spark.sql")\
    .option("opensearch.nodes", host)\
    .option("opensearch.port", port)\
    .option("opensearch.resource", index)\
    .option("opensearch.net.http.auth.user", user)\
    .option("opensearch.net.http.auth.pass", password)\
    .option("opensearch.write.operation", "index")\
    .mode("append")\
    .save()
```


Now, if you are trying to write data to an Elasticsearch server instead, then, these
are the mandatory options that you have to set:


- `es.nodes`: the hostname where your Elasticsearch server is hosted.
- `es.port`: the port to use in the connection with the Elasticsearch server.
- `es.resource`: the name of the target index in the Elasticsearch database to write the data to.
- `es.write.operation`: the type of operation (e.g. `index`, `create`, `upsert`, etc.) that you want to use when writing the data to the Elasticsearch server.
- `es.net.http.header.Authorization`: if you are using an API key to authenticate in the Elasticsearch server, you should use this option to set the authorization header that will be used in the HTTP request that is made to the Elasticsearch server.


In the example below, I'm setting the mandatory options for the Elasticsearch driver:

```python
df = spark.range(1)
index = "my_index"
host = "https://my-hostname.com"
port = 1234
api_key = "my_secret_api_key"

df.write.format("org.opensearch.spark.sql")\
    .option("es.nodes", host)\
    .option("es.port", port)\
    .option("es.resource", index)\
    .option("es.net.http.header.Authorization", f"ApiKey {api_key}")\
    .option("es.write.operation", "index")\
    .mode("append")\
    .save()
```


# Spark write mode

Usually, you don't care about which Spark write mode is being used when using
the Elasticsearch/Opensearch data source. But, depending on what you want to do, you might
have to change the Spark write mode to meet your goals.
As it will become more clear further in this article, you want to use
the Spark write mode `"append"` in most situations, i.e. it should be your "default".

But, before we continue, let me explain the differences between the Spark write mode,
and the type of write operation that you set through the `es.write.operation` and `opensearch.write.operation` options.

With the `es.write.operation` and `opensearch.write.operation` options, you specify which type
of operation you want to use to insert the new data into the Opensearch database. That is, if
the new data should be inserted into the Opensearch index using an `upsert` operation, or maybe,
a `create` operation, or an `index` operation, etc.

On the other hand, by setting the Spark write mode, you are basically adding a new effect into the process.
Usually, you want to use the Spark write mode `"append"` in most situations, because this
write mode does not introduces any new effect into the mix. In other words, the write mode
`"append"` does nothing, it "means nothing" to the Elasticsearch/Opensearch Spark data source.

But, if you set the Spark write mode to anything else, then, you start to introduce new
operations in the mix. In the list below, you can see which effects each Spark write mode
has on the process:

- `overwrite`: a `delete` operation is performed in the Elasticsearch/Opensearch index before the Spark data is written
    into the Elasticsearch/Opensearch index.
- `error` or `errorifexists`: if the Elasticsearch/Opensearch index is empty (i.e. document count is zero, or, the index doesn't even exist),
    nothing happens, the Spark data is written into the Elasticsearch/Opensearch index as expected. Otherwise, an exception is raised.
- `ignore`: if the Elasticsearch/Opensearch index is empty (i.e. document count is zero, or, the index doesn't even exist),
    the Spark data is written into the Elasticsearch/Opensearch index as expected. Otherwise, the Spark data is completely
    ignored, that is, it is not written into the Elasticsearch/Opensearch index.



# Potential problems you might face

## Forbidden database operations

While using these Spark native data sources you might face some problems.
The most likely error that you might encounter while using these data sources is a "403/Forbidden" error message,
which basically means that, with the inputs that you provided, the Spark data source is asking the Elasticsearch/Opensearch
server to perform an operation that you don't have enough authorization to perform. However, this is kind of
generic, i.e. this "forbidden" can mean a lot of things. That is, there are many different operations
that you may or may not have enough authorization to perform in the database.

For example, maybe, the data that you have provided does not follow the schema of the JSON documents that are
already indexed in the database index that you are using. If that is your case, then, you will
likely cause a "refresh the index" operation in the database, and this operation essentially
needs to redefine/recreate the index in the database from scratch, and, you might not have enough authorization
to perform this "refresh the index" operation, therefore, causing a "403/Forbidden" error.

This is just one example, but there are many other examples of operations
that you might be silently causing/triggering through these Spark data sources,
and that you do not have enough authorization to perform. Unfortunately,
the error messages provided by these Spark data sources are really poor in details in some cases.
So you will probably need to do a lot of testing, until you find the perfect combination of
inputs and data that work for your case.



