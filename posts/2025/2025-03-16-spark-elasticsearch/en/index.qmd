---
title: Writing Spark DataFrames to Elasticsearch/Opensearch databases
description: "Elasticsearch and Opensearch are two very popular No-SQL databases. In this post, I want to address how can you write data from a Spark DataFrame into an Elasticsearch/Opensearch database."
date: "2025-03-16"
image: "../spark-elasticsearch-cover.webp"
---

# Introduction

Recently, I had to develop a simple connector (i.e. a Python function) that allows a user to upload/write
data from a Spark DataFrame into both an Elasticsearch and an Opensearch database. If you are not familiar
with Elasticsearch or Opensearch, they are both No-SQL databases. Which means that they are databases that
store a collection of JSON documents, instead of storing a collection of tables as you would expect from
a traditional SQL database like PostgreSQL.

Our data lake is processed by Apache Spark and Databricks, and some of the data that is being processed
needs to be written into an Elasticsearch/Opensearch database, and it is best if we use the high scalability
of Apache Spark to write this data. Thus, in this post, I want to describe how you can use the Spark to write data into these databases.

## Why this post?

Just to be clear, I'm writing this post specially because the existing documentation about the connection between
Spark and Elasticsearch/Opensearch is very, I mean, veeeery poor. Therefore, this post serves as
an extra documentation, or, an extra resource that you can rely on when writing data from Spark into
Elasticsearch/Opensearch.


# Spark native data sources

If you want to write data from Spark fast and with high scalability, you want to use a Spark native data source
to write your data. Maybe you don't know what are Spark native data sources, so let's discuss them now.

Spark data sources are described in details at the [Spark documentation](https://spark.apache.org/docs/3.5.3/sql-data-sources.html).
Spark supports many different data sources out of the box. For example, it supports static files like CSV, JSON and Parquet files.
But, you can also install Maven packages in your environment to use other data sources that were developed by the community.

For example, by installing the `spark-sql-kafka` Maven package[^spark-kafka], a new Spark data source becomes available to you, which is the
Kafka data source. With this data source, Spark supports streaming with Apache Kafka queues. Therefore, Spark supports multiple data
sources out of the box, but you can add more data sources to the list by installing Maven packages that include new data sources
to the Spark engine.

[^spark-kafka]: <https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10>


## Selecting a data source

For example, if you are writing a Spark application through `pyspark`, you can choose the specific data source that
you want to use in Spark by using the
[`format()` method from a `DataFrameWriter` class](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.format.html#pyspark.sql.DataFrameWriter.format).
In the example below, I'm using the JSON data source to write data from Spark into a JSON file.

```python
df = spark.range(1)
df.write.format('json')
```



## Elasticsearch/Opensearch native Spark data sources

Both Elasticsearch and Opensearch have a free/open-source Spark native data source
that we can use. These data sources are both available as packages in the [Maven repository](https://mvnrepository.com/).
If you want to write data to Elasticsearch, then, you are looking for the `elasticsearch-spark` package.
But, if you want to write data to Opensearch instead, then, you are looking for the
`opensearch-spark` package.

You can see more info about these packages in the links below:

- <https://mvnrepository.com/artifact/org.opensearch.client/opensearch-spark-30>.
- <https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-30>.

Now, you need to use the specific version of these packages that match your specific environment.
For example, my environment is this:

- Databricks Runtime 15.4 LTS.
- Apache Spark 3.5.0.
- Scala 2.12.18.
- Python 3.11.0.

The version of Spark and Scala are the most important infos in this case. Since the version of Spark
is 3.5.0, we want to use a version of these packages that are compatible with the 3.* versions of Spark,
which are `opensearch-spark-30` and `elasticsearch-spark-30`.

And we also need to use a version of these packages that are compatible with the 2.12 version of Scala.
This is why, I've installed the `opensearch-spark-30_2.12` and `elasticsearch-spark-30_2.12` versions of
these packages in my specific environment.

Your specific environment might have different versions, and, therefore, you might need
to use a different version of these packages for your specific case. Just be aware of that.

In Databricks, you can install these Maven packages in your cluster to make these
native Spark data sources available in your environment. If you don't know how to do this, checkout the
[Databricks documentation](https://docs.databricks.com/aws/en/libraries/package-repositories#maven-or-spark-package)
for this.


## Using the Elasticsearch/Opensearch data source

After you install these Maven packages, you should be able to select the Elasticsearch/Opensearch
data source by using the strings `"org.opensearch.spark.sql"` and `"org.elasticsearch.spark.sql"`
in the `format()` method.

Now, in order to use the data source, you have to set some options with the `option()`
method of the `DataFrameWriter` class. These are the mandatory options that you have to set
in the Opensearch driver:

- `opensearch.nodes`: the hostname where your Opensearch server is hosted.
- `opensearch.port`: the port to use in the connection with the Opensearch server.
- `opensearch.resource`: the name of the target index in the Opensearch database to write the data to.
- `opensearch.net.http.auth.user`: the username to login into the Opensearch server.
- `opensearch.net.http.auth.pass`: the password to login into the Opensearch server.
- `opensearch.write.operation`: the type of operation (e.g. `index`, `create`, `upsert`, etc.) that you want to use when writing the data to the Opensearch server.


In the example below, we are selecting the Opensearch data source, and setting these mandatory
options. Of course, the values of these options are not real, they are fictitious.

```python
df = spark.range(1)
index = "my_index"
host = "https://my-hostname.com"
port = 1234
user = "my_username"
password = "my_secret_pass"

df.write.format("org.opensearch.spark.sql")\
    .option("opensearch.nodes", host)\
    .option("opensearch.port", port)\
    .option("opensearch.resource", index)\
    .option("opensearch.net.http.auth.user", user)\
    .option("opensearch.net.http.auth.pass", password)\
    .option("opensearch.write.operation", "index")\
    .mode("append")\
    .save()
```


Now, if you are trying to write data to an Elasticsearch server instead, then, these
are the mandatory options that you have to set:


- `es.nodes`: the hostname where your Elasticsearch server is hosted.
- `es.port`: the port to use in the connection with the Elasticsearch server.
- `es.resource`: the name of the target index in the Elasticsearch database to write the data to.
- `es.write.operation`: the type of operation (e.g. `index`, `create`, `upsert`, etc.) that you want to use when writing the data to the Elasticsearch server.
- `es.net.http.header.Authorization`: if you are using an API key to authenticate in the Elasticsearch server, you should use this option to set the authorization header that will be used in the HTTP request that is made to the Elasticsearch server.


In the example below, I'm setting the mandatory options for the Elasticsearch driver:

```python
df = spark.range(1)
index = "my_index"
host = "https://my-hostname.com"
port = 1234
api_key = "my_secret_api_key"

df.write.format("org.opensearch.spark.sql")\
    .option("es.nodes", host)\
    .option("es.port", port)\
    .option("es.resource", index)\
    .option("es.net.http.header.Authorization", f"ApiKey {api_key}")\
    .option("es.write.operation", "index")\
    .mode("append")\
    .save()
```
