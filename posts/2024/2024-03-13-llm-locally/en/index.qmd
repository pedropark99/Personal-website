---
title: Running a ChatGPT assistant locally
description: "Some people might want to run AI assistants (such as ChatGPT) locally on their system. In this article, I explain how you can do this."
date: "2024-03-16"
---

# Introduction

Despite the current success of AI assistants (such as ChatGPT), the infrastructure,
and the knowledge behind the services around these assistants are concentrated on a very small subset of companies (e.g. OpenAI, Nvidia, Meta and Microsoft).
This fact might represent a barrier for very large adoption of the technology. Mainly
because of two factors:

- Privacy.
- Costs.

In other words, because we have such a strong monopoly right now,
some corporations might have difficulties of adopting AI assistants, because of
costs and privacy limits. The privacy or costs factors might become a "deal-breaker"
for these corporations.

Banks for example, are usually very careful with sharing sensible user-information with anyone.
As a consequence, for example, using OpenAI services might be a deal-breaker for this bank,
because the bank cannot share user-information with OpenAI in any circumstance. The bank
could be exposed to legal penalties otherwise.

In this situation, it might be a good idea for the bank, to host and run the AI
assistant inside their own infrastructure, on a very restrict and secure place.
So, in other words, this bank would host and run it's own ChatGPT on
their own servers.

This article explain how you can download and use
LLMs (Large Language Models) locally on your own machine, with [Ollama](https://ollama.com/).
By running a LLM model locally in your own machine, you can apply this knowledge to run LLM
models on your own servers and private services.


# Overview

In essence, running a LLM model locally in your machine, consists
of following these steps:

- Make sure to download the LLM model you want to use.
- Make sure that the Ollama server is up and running.
- Send your prompt to the local API, and get the response back.


# Introducing Ollama

Ollama is a command line util that helps you download, manage
and use LLM models. It is almost like a package manager, but for LLM models.

The success of LLM models were so big that scientists, companies,
and the general community developed many different LLM models,
and made them publicly available at GitHub, or
at HuggingFace.

So, in other words, the infrastructure and the technology behind AI services
is very monopolized right now. But the LLM technology per se, is
rather accesible and widespread.

Now, we can easily download and use these models
through Ollama. The complete list of LLM models
available is large, and you can see
the full list at the [Models page of Ollama's website.](https://ollama.com/library)

In order to use Ollama, you have to download and install it.
By [visiting Ollama's website](https://ollama.com/) you will find
instructions to install it in each OS.

After you download and install, you can start to use `ollama` in the command line
of your OS. There are two main commands in `ollama`, which are:

- `pull`: downloads a LLM model.
- `serve`: starts the Ollama service as a HTTP server.
- `run`: run a LLM model with an input prompt.



## Download a model

First, we need to download the LLM model we want to use,
so that is locally available to use in our machine.
You do this by using the `pull` command.

For the examples in this article, I will be using the
CodeLlama model, which is based on the Llama 2 model.
It is a LLM model refined and tuned for
code related questions and assignments.

```
ollama pull codellama:7b
```

## Be careful with the size of the model

A commom way to classifying LLM models is measuring them by their size.
The size of a LLM model is normally measured by the number of parameters
of the model, and they are normally in the level of billions of parameters. So a LLM model
usually have 1, 7, 20, 50, 100 billions parameters.

That sad. Be careful with the size of model you choose! In
my example here, I am running on very standard laptop,
literally nothing fancy. It has 8GB RAM,
and a normal Intel i5 Core CPU.

Because of that, I do not want to select a very large
model. I want to choose the smallest model as possible,
with the smallest number of parameters. That is
why I choose specifically
the version of CodeLlama with 7 billion parameters.

There are different versions of CodeLlama that go
up to 70 billion parameters. But, since
LLM models are very heavy to run by default, and,
I am using a small laptop, I will definetly use
the smallest model.

You should take the power of your machine in consideration
when choosing your LLM model.



## Start the Ollama server

```
ollama serve
```