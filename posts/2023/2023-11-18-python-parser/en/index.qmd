---
title: "Developing a parser for Python with Python"
description: "This past week I had to develop a parser for Python expressions with Python. In this article, I want to use this experience to introduce the subject of parsing to beginners."
image: "./../image-pic.png"
number-sections: true
date: "2023-11-18"
format:
    html:
        css: "./../style.css"
---

# Introduction

Me and my team are currently working in a massive migration (similar to a cloud migration). This migration involves many process, but one of them is to redirect every table reference that we find in more than **130 thousand lines of Python code**.

However, this task proved to be so complex that I had to develop a small parser for Python expressions. This article use this experiment to introduce beginners to the subject of parsing expressions.


# Context about what we have to do {#sec-context}

Most of these 130 thousand lines of Python code are [`pyspark`](https://spark.apache.org/docs/latest/api/python/index.html) code to extract, transform and load data using the `Apache Spark` engine.

Let's consider the following example:

```{#lst-table .python lst-cap="Example of pyspark code that we might find in our codebase"}
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.table("blip.events")
df.show()
```

You can see at @lst-table that we have a `spark.table()` call to acess the SQL table `blip.events`. However, in the new infrastructure, all table references will change. As a result, me and my team need to rewrite every table reference that we find across our codebase.

For example, let's suppose that the new table reference is `platform.blipraw.events`. This means that I need to alter the above snippet of code to:

```{#lst-table2 .python lst-cap="Example with the new reference"}
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.table("platform.blipraw.events")
df.show()
```

## The quick and dirty approach

This does not look so bad, right? I mean, considering the above example, I could just use a simple REGEX (*regular expression*) to find the places where I have an `spark.table()` call, capture the reference given as input, alter it to the new reference, and replace the text with the new reference.

A quick example of this code would be similar to this:

```{python}
import re
spark_table_regex = r'spark[.]table[(][a-zA-Z0-9.\'\"]+[)]'
notebook_content = '''from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.table("blip.events")
df.show()
'''

new_table_call = "spark.table(\"platform.blipraw.events\")"
new_notebook_content = re.sub(spark_table_regex, new_table_call, notebook_content)
print(new_notebook_content)
```


## The size of the challenge {#sec-challenge}

It would be great if it was that simple, but unfortunately, it is not. What makes this problem so challenging is that the table reference used in `spark.table()` appears in too many different formats across the 130 thousand lines in our codebase. For example, we might use a *formatted string* to actually compute the table reference:

```python
database = "blip"
table_name = "events"
df = spark.table(f"{database}.{table_name}")
```

Or maybe, we call a variable that contains the computed reference:

```python
table_ref = database + '.' + table_name
df = spark.table(table_ref)
```

These two examples demonstrates that too much variation exists in the use of a table reference. So much variation, that using multiple REGEX's to solve this problem would be impractical, and probably too much complex.

Here is where the parser comes into place.

# Introducing the parser

## What is a parser?

Parsers (or the process of parsing expressions) are core components of every existing compiler, like `gcc` or `rustc`, as well as the R and Python compilers. In essence, a parser is a piece of software that analyzes expressions following the rules of a grammar. A parser is the main part of compilers responsible for analyzing and comprehend the structure of your source code.

The process of parsing is usually made in two steps, which are: 1) breaking (or "splitting") the input expression into smaller pieces, building a list of tokens, or a list of small components; 2) analyzing this sequence of tokens to build a tree that is equivalent to the input expression. The first step above is usually made by a component called *lexer* or *tokenizer* (both names are commom to find), and the second step is made by the parser itself.

Basically, the process of parsing takes a string (which contains the expression, or the source code your want to parse) as input. Then, the lexer (or tokenizer) breaks the input string into smaller pieces, which are usually called *tokens*. Then, the parser receives this stream of tokens produced by the tokenizer as input, and starts to analyze this sequence of tokens, to understand the structure of the input expression (or source code). As output, the parser produces a tree that is equivalent to the input expression, which is usually called *abstract syntax tree* (or AST for short). @fig-parser-pic below exposes this process.

![The process of parsing](./../parser-pic.png){#fig-parser-pic}

So the process of parsing takes an expression as input, and builds a tree that is equivalent to that expression as output. This process of parsing is always one of the first operations that a compiler performs. Because trees are a much more suitable and efficient data structure for the different tasks a compiler performs such as: type and syntax checking, evaluating the result of expressions and assignments, or compiling the input tree into machine code to be executed.

Probably, trees and stacks are the most important data structures for every compiler.

## What kind of expressions we want to parse? {#sec-kind}

Just to be clear, we neither need or want to develop a complete parser capable of parsing every expression in the Python language. That would be a much larger task, that would involve a great amount of effort. We just want to build a small parser capable of parsing a very small subset of Python expressions.

We are particularly interested in the expressions that are related to the table references that we find in our codebase. We already showed examples of these expressions at @sec-context and @sec-challenge.

But just to state clearly, we want to build a parser capable of parsing:

1. expressions that involves only string constants (other types of constants like lists, integers, booleans are not important for us, so let's ignore them). Example: `"platform.blipraw.events"`.
2. expressions that concatenate strings with the plus operator. Example: `"blip" + "." + "events"`.
3. expressions that contains identifiers (that is, variable names). Example: `database + "." + table_name`.
4. formatted strings which contain expressions that fit the above cases. Example: `f"{database}.{table_name}"`.

Lets store these examples of expressions inside a list that we can easily access:

```{python}
EXPRESSION_EXAMPLES = [
    "'platform.blipraw.events'",
    '"blip" + "." + "events"',
    "database + \".\" + table_name",
    'f"{database}.{table_name}"'
]
```


## Building the Lexer (or Tokenizer)

Lets begin by building a lexer (or tokenizer) for our parser. From now on, I will use the term tokenizer, instead of lexer. Just be aware of this.

But how can we split our input string into small pieces? There are different approaches to do this. However, one particular approach that fit's perfectly our example here is to iterate through the characters of our input string, and look for single characters that represent "elegible break points", or points where we can split the string.

This approach is probably the easiest of all to implement, and it fit's perfectly our example here because we are interested in parsing just a very small subset of simple Python expressions. If we wanted to parse more complex expressions, then, it would probably be better to change our approach.

So, the tokenizer will iterate through each character in the input string, and will mark any place that contains a single character the we interpret as an elegible place to break the string. Considering the type of expressions we stated at @sec-kind, the characters `"`, `'`, `+`, `{`, `}` are good candidates for "elegible break points". Also, the character `f` is important for identifying formatted strings, as a consequence, he is also a good candidate.

Let's consider the following tokenizer:

```{python}
from typing import List
import re

is_not_blank = lambda x: x != "" and not re.search(r"^ +$", x)
def tokenizer(input_string: str) -> List[str]:
    candidates = ["'", '"', '+', '{', '}']
    break_points = list()
    for i in range(len(input_string)):
        current_char = input_string[i]
        if current_char in candidates:
            break_points.append(i)
        if current_char == "f" and (i + 1) < len(input_string): #<1>
            next_char = input_string[i + 1]
            if next_char in ['"', "'"]:
                break_points.append(i)

    if len(break_points) == 0: #<2>
        return [input_string]

    tokens = list()
    last_index = 0
    for index in break_points:
        tokens.append(input_string[last_index:index]) #<3>
        tokens.append(input_string[index:(index+1)])
        last_index = index + 1

    tokens.append(input_string[last_index:])
    return list(filter(is_not_blank, tokens)) #<4>
```

1. If current character is `f` check if the next character is the beginning of a string (characters `"` and `'`), if it is the beginning of a string, then, it is a formatted string and should be included in the "break points". If the next character is not the beginning of a string, then, we should not consider it as an elegible breakpoint, because it probably is just a letter "f" inside a variable name, such as `platform_database`.

2. If no break point position was found, then, the input expression is likely an expression with a single component. For example, a single string constant (e.g. `"blip.events"`) or a single variable name (`events_table`). In this case, the tokenizer should return this single component itself as the only token present in the input string.

3. Every iteration of the loop generates two different tokens, which are: 1) a token with the part of the string from the previous break point index until the current elegible break point index; 2) and another token containing the single character that identifies the current elegible breakpoint. For example, the text `database"` will generate the break point index `8`, so, in the first iteration of the loop, the tokens `'database'` and `'"'` will be generated.

4. Empty tokens (i.e. tokens that are empty strings, or, that contains only spaces) can be generated during the process. So we use `filter()` with a `lambda` function to eliminate them from the output.

This `tokenizer()` function generates a list of tokens to be analyzed by the parser:

```{python}
for example in EXPRESSION_EXAMPLES:
    tokens = tokenizer(example)
    print("====================================")
    print("  * Input expression: ", example)
    print("  * Tokens produced: ", tokens)
```


## Building the parser