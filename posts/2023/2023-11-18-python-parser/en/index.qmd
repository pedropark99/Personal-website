---
title: "Developing a parser for Python with Python"
description: "This past week I had to develop a parser for Python expressions with Python. In this article, I want to use this experience to introduce the subject of parsing to beginners."
image: "./../image-pic.png"
number-sections: true
bibliography: "./../references.bib"
date: "2023-11-18"
format:
    html:
        css: "./../style.css"
---

# Introduction

Me and my team are currently working in a massive migration (similar to a cloud migration). This migration involves many process, but one of them is to redirect every table reference that we find in more than **130 thousand lines of Python code**.

However, this task proved to be so complex that I had to develop a small parser for Python expressions. This article use this experiment to introduce beginners to the subject of parsing expressions.


# Context about what we have to do {#sec-context}

Most of these 130 thousand lines of Python code are [`pyspark`](https://spark.apache.org/docs/latest/api/python/index.html) code to extract, transform and load data using the `Apache Spark` engine.

Let's consider the following example:

```{#lst-table .python lst-cap="Example of pyspark code that we might find in our codebase"}
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.table("blip.events")
df.show()
```

You can see at @lst-table that we have a `spark.table()` call to access the SQL table `blip.events`. However, in the new infrastructure, all table references will change. As a result, me and my team need to rewrite every table reference that we find across our codebase.

For example, let's suppose that the new table reference is `platform.blipraw.events`. This means that I need to alter the above snippet of code to:

```{#lst-table2 .python lst-cap="Example with the new reference"}
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.table("platform.blipraw.events")
df.show()
```

## The quick and dirty approach

This does not look so bad, right? I mean, considering the above example, I could just use a simple REGEX (*regular expression*) to find the places where I have an `spark.table()` call, capture the reference given as input, alter it to the new reference, and replace the text with the new reference.

A quick example of this code would be similar to this:

```{python}
import re
spark_table_regex = r'spark[.]table[(][a-zA-Z0-9.\'\"]+[)]'
notebook_content = '''from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.table("blip.events")
df.show()
'''

new_table_call = "spark.table(\"platform.blipraw.events\")"
new_notebook_content = re.sub(spark_table_regex, new_table_call, notebook_content)
print(new_notebook_content)
```


## The size of the challenge {#sec-challenge}

It would be great if it was that simple, but unfortunately, it is not. What makes this problem so challenging is that the table reference used in `spark.table()` appears in too many different formats across the 130 thousand lines in our codebase. For example, we might use a *formatted string* to actually compute the table reference:

```python
database = "blip"
table_name = "events"
df = spark.table(f"{database}.{table_name}")
```

Or maybe, we call a variable that contains the computed reference:

```python
table_ref = database + '.' + table_name
df = spark.table(table_ref)
```

These two examples demonstrates that too much variation exists in the use of a table reference. So much variation, that using multiple REGEX's to solve this problem would be impractical, and probably too much complex.

Here is where the parser comes into place.

# Introducing the parser

## What is a parser?

Parsers (or the process of parsing expressions) are core components of every existing compiler, like `gcc` or `rustc`, as well as the R and Python compilers. In essence, a parser is a piece of software that analyzes expressions following the rules of a grammar. A parser is the main part of compilers responsible for analyzing and comprehend the structure of your source code.

The process of parsing is usually made in two steps, which are: 1) breaking (or "splitting") the input expression into smaller pieces, building a list of tokens, or a list of small components; 2) analyzing this sequence of tokens to build a tree that is equivalent to the input expression. The first step above is usually made by a component called *lexer* or *tokenizer* (both names are commom to find), and the second step is made by the parser itself.

Basically, the process of parsing takes a string (which contains the expression, or the source code your want to parse) as input. Then, the lexer (or tokenizer) breaks the input string into smaller pieces, which are usually called *tokens*. Then, the parser receives this stream of tokens produced by the tokenizer as input, and starts to analyze this sequence of tokens, to understand the structure of the input expression (or source code). As output, the parser produces a tree that is equivalent to the input expression, which is usually called *abstract syntax tree* (or AST for short). @fig-parser-pic below exposes this process.

![The process of parsing](./../parser-pic.png){#fig-parser-pic}

So the process of parsing takes an expression as input, and builds a tree that is equivalent to that expression as output. This process of parsing is always one of the first operations that a compiler performs. Because trees are a much more suitable and efficient data structure for the different tasks a compiler performs such as: type and syntax checking, evaluating the result of expressions and assignments, or compiling the input tree into machine code to be executed.

Probably, trees and stacks are the most important data structures for every compiler.

## What kind of expressions we want to parse? {#sec-kind}

Just to be clear, we neither need or want to develop a complete parser capable of parsing every expression in the Python language. That would be a much larger task, that would involve a great amount of effort. We just want to build a small parser capable of parsing a very small subset of Python expressions.

We are particularly interested in the expressions that are related to the table references that we find in our codebase. We already showed examples of these expressions at @sec-context and @sec-challenge.

But just to state clearly, we want to build a parser capable of parsing:

1. expressions that involves only string constants (other types of constants like lists, integers, booleans are not important for us, so let's ignore them). Example: `"platform.blipraw.events"`.
2. expressions that concatenate strings with the plus operator. Example: `"blip" + "." + "events"`.
3. expressions that contains identifiers (that is, variable names). Example: `database + "." + table_name`.
4. formatted strings which contain expressions that fit the above cases. Example: `f"{database}.{table_name}"`.

Lets store these examples of expressions inside a list that we can easily access:

```{python}
EXPRESSION_EXAMPLES = [
    "'platform.blipraw.events'",
    '"blip" + "." + "events"',
    "database + \".\" + table_name",
    'f"{database}.{table_name}"'
]
```


## Building the Lexer (or Tokenizer)

Lets begin by building a lexer (or tokenizer) for our parser. From now on, I will use the term tokenizer, instead of lexer. Just be aware of this.

But how can we split our input string into small pieces? There are different approaches to do this. However, one particular approach that fit's perfectly our example here is to iterate through the characters of our input string, and look for single characters that represent "elegible break points", or points where we can split the string.

This approach is probably the easiest of all to implement, and it fit's perfectly our example here because we are interested in parsing just a very small subset of simple Python expressions. If we wanted to parse more complex expressions, then, it would probably be better to change our approach.

So, the tokenizer will iterate through each character in the input string, and will mark any place that contains a single character the we interpret as an elegible place to break the string. Considering the type of expressions we stated at @sec-kind, the characters `"`, `'`, `+`, `{`, `}` are good candidates for "elegible break points". Also, the character `f` is important for identifying formatted strings, as a consequence, he is also a good candidate.

Let's consider the following tokenizer:

```{python}
from typing import List
import re

is_not_blank = lambda x: x != "" and not re.search(r"^ +$", x)
def tokenizer(input_string: str) -> List[str]:
    candidates = ["'", '"', '+', '{', '}']
    break_points = list()
    for i in range(len(input_string)):
        current_char = input_string[i]
        if current_char in candidates:
            break_points.append(i)
        if current_char == "f" and (i + 1) < len(input_string): #<1>
            next_char = input_string[i + 1]
            if next_char in ['"', "'"]:
                break_points.append(i)

    if len(break_points) == 0: #<2>
        return [input_string]

    tokens = list()
    last_index = 0
    for index in break_points:
        tokens.append(input_string[last_index:index]) #<3>
        tokens.append(input_string[index:(index+1)])
        last_index = index + 1

    tokens.append(input_string[last_index:])
    return list(filter(is_not_blank, tokens)) #<4>
```

1. If current character is `f` check if the next character is the beginning of a string (characters `"` and `'`), if it is the beginning of a string, then, it is a formatted string and should be included in the "break points". If the next character is not the beginning of a string, then, we should not consider it as an elegible breakpoint, because it probably is just a letter "f" inside a variable name, such as `platform_database`.

2. If no break point position was found, then, the input expression is likely an expression with a single component. For example, a single string constant (e.g. `"blip.events"`) or a single variable name (`events_table`). In this case, the tokenizer should return this single component itself as the only token present in the input string.

3. Every iteration of the loop generates two different tokens, which are: 1) a token with the part of the string from the previous break point index until the current elegible break point index; 2) and another token containing the single character that identifies the current elegible breakpoint. For example, the text `database"` will generate the break point index `8`, so, in the first iteration of the loop, the tokens `'database'` and `'"'` will be generated.

4. Empty tokens (i.e. tokens that are empty strings, or, that contains only spaces) can be generated during the process. So we use `filter()` with a `lambda` function to eliminate them from the output.

This `tokenizer()` function generates a list of tokens to be analyzed by the parser:

```{python}
for example in EXPRESSION_EXAMPLES:
    tokens = tokenizer(example)
    print("====================================")
    print("  * Input expression: ", example)
    print("  * Tokens produced: ", tokens)
```


## Building the actual parser

Building the actual parser is definitely the hard part. Because a parser usually involves: 1) having a variable to store the current state of the parser, or, the current state of AST; 2) and also some level of recursion to traverse the AST, or to decide which move or production rule should be applied. Making these two components working well together can be difficult depending on how you implement it.

### The different types of strategies

Remember, the end goal of a parser, is to build an abstract syntax tree (AST) by analizing a sequence of tokens. There are different strategies to build this tree. These strategies are usually divided into two categories, which are: 1) *top-down*, which are the strategies that builds the tree from the root, and work they way to the bottom of the tree, which contains the "leaves of the tree"; 2) *bottom-up*, that starts to build the tree from the leaves, and work they way up until it hits the root of the tree.

In the subject of parsing, *top-down* strategies are the most popular (notice that this does not mean that *bottom-up* strategies are bad). The most popular strategy of all is the *Recursive Descent*, which is a *top-down* strategy. Anyway, just be aware that different strategies exists.

### The smaller components of a parser

Every parser usually involves two smaller components: 1) a cache that holds the current state of the parser. This cache stores variables that are essential to the parsing process, such as the AST builded by the parser; 2) a control (or "decision-making") mechanism, which is usually a collection of methods or functions that decides which move the parser should make considering the current state of it (@gries2008).

Some degree of recursion is found very frequently on some of these methods or functions that decides which move the parser should make (although this recursion is not something mandatory).

### The cache to store the current parser's state

Lets build a Python class to represent the cache of the parser. This `ParserCache` class will be responsible for storing all variables that are vital to the parsing process. These variables are:

- `ast`: stores the current state of the AST.
- `tokens`: stores the full sequence of tokens that the parser needs to analyze.
- `current_index`: stores the index that parser is currently in on the sequence of tokens.
- `current_token`: stores a copy of the current token that the parser is analyzing at the moment.

It is not really vital to have the `current_token` variable, because we could easily access the current token by using `tokens[current_index]`. But... having a variable called `current_token` makes most of the parser's code more readable, so, it is woth it.

So while the parser is analyzing the sequence of tokens stored at the `tokens` variable, he will slowly grow (or complete) the AST, by replacing and adding elements to the `ast` variable. At the end, when the parser finishs the parsing process, the `ast` variable will store the complete AST that is equivalent to the input expression.

You can see below at the `__init__` method that to initialize a new object of class `ParserCache`, you need to give the sequence of tokens to analyze as input. After that, all variables of the class are initialized, and you can start to use the two methods of the class, which are `len()`, `current_token()` and `next_token()`. The `len()` method returns the length of the tokens sequence that the parser is analyzing, `current_token()` method returns the current token that the parser is analyzing at the moment, and the `next_token()` method will effectively advance the parser to the next token in the sequence.

```{python}
class ParserCache:
    '''Class that represents a cache to store the current state of the parser.'''
    def __init__(self, tokens: List[str]) -> None:
        self.ast = list()
        self.tokens = tokens
        self.index = 0
        if len(tokens) > 0:
            self.token = tokens[self.index]
        else:
            self.token = None
        return

    def len(self) -> int:
        return len(self.tokens)

    def current_token(self) -> str:
        return self.token

    def current_index(self) -> int:
        return self.index
        
    def next_token(self) -> None:
        self.index += 1
        if self.index < len(self.tokens):
            self.token = self.tokens[self.index]
        return
```


### The main function or entrypoint for the parser

We need a main function for the parser, and that is the `parse()` function. In other words, this is the public API or the main entrypoint of the parser. That is the function that we should use to effectively parse any input and get the resulting AST.

You can see below that all of this function does is: get the input string; generate the tokens with `tokenizer()`; initialize the parser's cache with these tokens; and then, it calls a private function called `_parse_input()` to initiate the parsing process.

Once `_parse_input()` is done with the parsing process, `parse()` gets the parsing results and simply return the complete AST produced, which is stored in the `ast` variable of the parser's cache.

```{python}
from typing import Dict, Any
def parse(input_string: str) -> List[Dict[str, Any]]:
    tokens = tokenizer(input_string)
    parser_cache = ParserCache(tokens)
    parsing_result = _parse_input(parser_cache)
    return parsing_result.ast
```


### Building the main logic of the parser

The true magic starts with the private function `_parse_input()`. Basically, `_parse_input()` starts by looking at the current of token in the sequence that is being analyzed.

If this current token represents the beginning of a string (characters `"` and `'`), `_parse_string()` is called to effectively parse the next tokens in the sequence, which (very likely) are the contents of that string. But if this token is a plus sign, then, he will call `_parse_addition()` to parse the previous and the next tokens, which are involved in this addition operation. But if the current token is neither the beginning of a string or a plus sign, then, `_parse_input()` will assume that this current token is an identifier, and he will call `_parse_identifier()` to parse it.

Notice that all of these three parsing functions (`_parse_string()`, `_parse_addition()` and `_parse_identifier()`) returns the `ParserCache` object back as output. These functions receives the parser cache as input, they use it to parse the current token, and they add new elements (or replace the existing ones) to the AST, which is stored inside this cache object. Once they finished their businesses, they simply return this cache back as their output.

After we know that we parsed the current token, then, we ask the parser to advance to the next token, with `next_token()` method. Then, we check if there is really a next token in the sequence to parse/analyze, by checking if the current index is greater than the number of tokens in the sequence.

If there are still some remaining tokens in the sequence to analyze, then, `_parse_input()` will call itself recursively to parse the remaining tokens in the sequence. However, if there is not elements to analyze anymore, then, the function finally returns the parser cache (which contains the complete AST inside of the `ast` variable) as output.

```{python}
def _parse_input(parser_cache: ParserCache) -> ParserCache:
    if parser_cache.len() == 0: #<1>
        return parser_cache

    if parser_cache.current_token() in ['"', "'"]:
        parser_cache = _parse_string(parser_cache)
    elif parser_cache.current_token() == '+':
        parser_cache = _parse_addition(parser_cache)
    else:
        parser_cache = _parse_identifier(parser_cache)

    parser_cache.next_token()
    if parser_cache.current_index() <= parser_cache.len() - 1:
        parser_cache = _parse_input(parser_cache)

    return parser_cache
```

1. If the sequence of tokens is empty, then, we have no tokens to analyze, and the function should simply return right away.

Yes! I know what you are thinking. If the current token is neither the beginning of a string or a plus sign, then, he could be anything else. For example, he could be the beginning of a function call, or, a logic comparison, or it could be the beginning of an assignment expression. Anyway, it could be many things! So in theory, it is not safe to simply assume that it is an identifier.

However, you need to remember what we proposed at @sec-kind. We want to parse only a very small subset of Python expressions. We do not want (or care) to parse other kinds of expressions. If you look at the expressions examples that fitted the description in @sec-kind, you will see that the only components that appears in those expressions are identifiers, additions and string constants.

That is why we care only about these components. That is why, **for this specific case**, it is safe to assume that the last option has to be an identifier.


### Parsing identifiers

Lets begin by discussing what the `_parse_identifier()` function should do. Because parsing an identifier component is by far the simplest of all three components. I mean... how would you parse an identifier? To answer this question, is probably best to think what an identifier is.

Well, an identifier is just a name to a variable, right? So, the value of an identifier object should be the identifier itself! In other words, the name of the variable is the identifier itself. So why not just simply add to the AST an object of type `IDENTIFIER` and the value of that object being the current token which is the identifier (or the variable name) itself? Right? That should be sufficient.

So we have the following body for `_parse_identifier()`:

```{python}
def _parse_identifier(parser_cache: ParserCache) -> ParserCache:
    parser_cache.ast.append({
        'type': 'IDENTIFIER',
        'value': str(parser_cache.current_token())
    })
    return parser_cache
```


### Parsing addition operations

Now, things get a little bit more complicated when we find a plus sign in the middle of the sequence of tokens. Because if we do find a plus sign, that means that this plus sign should be surrounded by operands. In other words, this plus sign connects the left operand to the right operand, and they both are the arguments to this addition operation.

If we think about it, that means that the moment that we find a plus sign in the sequence of tokens, that means that the previous token is the left operand to the addition operation. But how can we get this previous token? I mean, the `ParserCache` class has only methods for current and next token, not a method for the previous token.

Well, we do not need a method to get the previous token, because the previous token is obviously at the top of the AST. In other words, the last element in the AST is the previous token already parsed, and we can easily access this last element by calling `parser_cache.ast[-1]`.

Perfect, afeter we saved a copy of the left operand inside a `left_operand` variable, we can delete him from the AST in the parser cache, with `del parser_cache.ast[-1]`. If we do not delete him now, then, he would become duplicated in the AST at the moment that we add our object for the addition operation to the AST, which contains both left and right operand.

After we deleted the last element in the AST (which contained the left operand), we can now look for the right operand, which is the next token in the sequence. That is why we call the `next_token()` method, to advance the parser.

After that, all we need to do, is to add to the AST an object that represents this addition operation.

```{python}
def _parse_addition(parser_cache: ParserCache) -> ParserCache:
    left_operand = parser_cache.ast[-1]
    del parser_cache.ast[-1]
    parser_cache.next_token()
    right_operand = parser_cache.current_token()
    parser_cache.ast.append({
        'type': 'ADDITION',
        'left': left_operand,
        'right': right_operand
    })
    return parser_cache
```


### Parsing strings

Now, things get worse when it is time to parse strings. Not because strings are complicated, but because we need to account for *formatted strings*. In other words, the string that we analyzing might contain expressions inside of it. And if it does have them, we need to parse these expressions separately.

However, if you come back and look at the body of `_parse_input()` you will see that we call `_parse_string()` right after we encounter a `"` or `'` character. But we do not need to parse this single character exactly. We just know that these characters delimit the beginning of a string, so everything that goes after it should be tokens that together composes the actual content of the string.

That is why, we start `_parse_string()` by calling the `next_token()` method, to skip this `"` or `'` character that we are not particularly interested in. Now, after that, all we need to do, is to accumulate all tokens that are inside of the string, until we hit the next token that contains the `"` or `'` character, because these tokens will represent the end of the string.

Why we do this? Because at the moment that we find the the `"` or `'` character, and we understand that we hitted the end of the string, and we break the loop, then, all elements contained inside the `stack` will be the tokens that together composes the content of the current string. And having all of these elements together at one place makes life easier for us. Because we can simply append to the AST a new object of type `STRING` containing this stack with all of the components of this string element.

Now... if we do find a opening bracket (`{`) inside the string, that likely means that the current string that we are analyzing now is a formatted string, and whatever sequence of tokens that is right after this opening bracket, they are a new expression to be parsed. That is why we call the `_parse_formatted_string()` in case we do find an opening bracket inside the string.

```{python}
def _parse_string(parser_cache: ParserCache) -> ParserCache:
    char_that_opens_the_string = parser_cache.current_token() #<1>
    parser_cache.next_token()
    # Add a placeholder in the top of the AST
    parser_cache.ast.append({ #<2>
        'type': 'STRING',
        'value': list()
    })

    while parser_cache.current_index() < parser_cache.len() - 1:
        if parser_cache.current_token() in ['"', "'"]:
            break
        if parser_cache.current_token() == '{':
            parser_cache.next_token()
            parser_cache = _parse_formatted_string(parser_cache)
            continue
        
        elem_ref = parser_cache.ast[-1]
        elem_ref['value'].append(parser_cache.current_token())
        parser_cache.next_token()
    
    return parser_cache
```

1. We copy the character (`'` or `"`) that openned the string that we are analyzing now, because then, we can search for a new occurence of this same character in the tokens sequence to detect the end of the string.

2. We add a placeholder at the top level of the AST. At each iteration of the `while` loop we update the `value` field of this placeholder by adding the current token to it. By doing this, at the end of the `while` loop we will have a string object at the top of the AST, and its `value` field will contain the full/complete list of contents of that string.

### Parsing expressions inside *formatted strings*

We call `_parse_formatted_string()` if we find a `{` character inside sequence of tokens that are inside a string. Inside the pair of brackets (`{}`) we will always have an expression. And because this is a new expression, we need to parse it too, in a separate process.

To do that, we create a new `ParserCache` object with the sequence of tokens that represents this expression, or, in other words, with all of the tokens that are between the openning and closing brackets. Then, we just call the `_parse_input()` over this expression to effectively parse this expression.

After that, we return from `_parse_formatted_string()` with a new expression object (`EXPR`) that contains the parsed expression inside of it.

```{python}
def _parse_formatted_string(parser_cache: ParserCache) -> ParserCache:
    stack = list()
    while parser_cache.current_index() < parser_cache.len() - 1:
        if parser_cache.current_token() == '}':
            parser_cache.next_token()
            break
        stack.append(parser_cache.current_token())
        parser_cache.next_token()

    parsed_subexpression = ParserCache(stack)
    parsed_subexpression = _parse_input(parsed_subexpression)
    elem_ref = parser_cache.ast[-1]
    elem_ref['value'].append({
        'type': 'EXPR',
        'value': parsed_subexpression.ast
    })
    return parser_cache
```


## Testing our parser

```{python}
for example in EXPRESSION_EXAMPLES:
    parsed_expr = parse(example)
    print("====================================")
    print("  * Input expression: ", example)
    print("  * Parsed AST: ", parsed_expr)
```

## The full source code

You can see the full source code of the parser at [`parser.py`](./../parser.py).