---
title: "Developing a parser for Python with Python"
description: "I had to develop a parser for Python expressions with Python. This article describes my experience developing this parser and use it to introduce this subject to beginners."
number-sections: true
date: "2023-11-18"
format:
    html:
        css: "./../style.css"
---

# Introduction

Me and my team are currently working in a massive migration (similar to a cloud migration). This migration involves many process, but one of them is to redirect every table reference that we find in more than **130 thousand lines of Python code**.

However, this task proved to be so complex that I had to develop a small parser for Python expressions. This article use this experiment to introduce beginners to the subject of parsing expressions.


# Context about what we have to do {#sec-context}

Most of these 130 thousand lines of Python code are [`pyspark`](https://spark.apache.org/docs/latest/api/python/index.html) code to extract, transform and load data using the `Apache Spark` engine.

Let's consider the following example:

```{#lst-table .python lst-cap="Example of pyspark code that we might find in our codebase"}
# This is just an random example of pyspark code. It is not real
# lines of Python code that exists inside our codebase.
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.table("blip.events")
df.show()
```

You can see at @lst-table that we have a `spark.table()` call to acess the SQL table `blip.events`. However, in the new infrastructure, all table references will change. That is why, me and my team need to rewrite every table reference that we find across our codebase.

For example, let's suppose that the new table reference is `platform.blipraw.events`. This means that I need to alter the above snippet of code to:

```{#lst-table2 .python lst-cap="Example with the new reference"}
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.table("platform.blipraw.events")
df.show()
```

## The quick and dirty approach

This does not look so bad, right? I mean, considering the above example, I could just use a simple REGEX (*regular expression*) to find the places where I have an `spark.table()` call, capture the reference given as input, alter it to the new reference, and replace the text with the new reference.

A quick example of this code would be similar to this:

```{python}
import re
spark_table_regex = r'spark[.]table[(][a-zA-Z0-9.\'\"]+[)]'
notebook_content = '''from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.table("blip.events")
df.show()
'''

new_table_call = "spark.table(\"platform.blipraw.events\")"
new_notebook_content = re.sub(spark_table_regex, new_table_call, notebook_content)
print(new_notebook_content)
```


# The size of the challenge

It would be great if it was that simple, but unfortunately, it is not. The problem is that the table reference used in `spark.table()` appears in too many different formats across the 130 thousand lines in our Python codebase. For example, we might use a *formatted string* to actually compute the table reference:

```python
database = "blip"
table_name = "events"
df = spark.table(f"{database}.{table_name}")
```

Or maybe, we call a variable that contains the computed reference:

```python
table_ref = database + '.' + table_name
df = spark.table(table_ref)
```

These two examples demonstrates that there is too much variation exists in the use of a table reference. So much that using multiple REGEX's to solve this problem would be impractical, and probably too much complex.

Here is where the parser comes into place.


# How a parser works?

A parser is a core component of every existing compiler, like `gcc` or the Python compiler. In essence, a parser is a piece of software that analyzes expressions following the rules of a grammar. A parser is the core piece of compilers responsible for analyzing and comprehend the structure of your source code.

The process of parsing is usually made in two steps, which are: 1) breaking (or "splitting") the input expression into smaller pieces, building a list of tokens, or a list of small components; 2) analyzing this sequence of tokens to build a tree that is equivalent to the input expression. The first step is usually made by a component called *tokenizer*, and the second step is made by the parser itself. Both are components of the compiler.

Basically, the process of parsing takes a string (which contains the expression, or the source code your want to parse) as input. Then, the tokenizer breaks the input string into smaller pieces, which are usually called *tokens*. Then, the parser receives this stream of tokens produced by the tokenizer as input, and starts to analyze this sequence of tokens, to understand the structure of the input expression (or source code). As output, the parser produces a tree that is equivalent to the input expression, which is usually called *abstract syntax tree* (or AST for short).

In other words, the process of parsing takes an expression as input, and builds a tree that is equivalent to that expression as output. This process of parsing is always one of the first tasks that a compiler does. Because trees are a much more suitable and efficient data structure for the different tasks a compiler performs such as: type and syntax checking, evaluating the result of expressions and assignments, or compiling the input tree into machine code to be executed.

Probably, trees and stacks are the most important data structures for every compiler.

