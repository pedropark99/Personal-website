[
  {
    "objectID": "donate.html",
    "href": "donate.html",
    "title": "Donate or Sponsor Me",
    "section": "",
    "text": "Currently, I only support direct transfers made via Pix. Which is a brazilian payment method available in most banks operating in the Brazilian market.\n\n\n\nTo get my Pix key, you can click in the button below to copy the key to your clipboard:\n\nCopy Pix Key\n\n\nOr, if you prefer, you can also get my Pix key by scanning the QR Code below:"
  },
  {
    "objectID": "posts/2024/2024-01-22-did-you-ever-ssh/en/index.html",
    "href": "posts/2024/2024-01-22-did-you-ever-ssh/en/index.html",
    "title": "Have you ever SSH before?",
    "section": "",
    "text": "SSH stands for Secure Socket Shell, and it is probably the most used protocol today to establish a secure connection between two computers.\nIn this article, I want to explain how can access a remote computer through a SSH connection."
  },
  {
    "objectID": "posts/2024/2024-01-22-did-you-ever-ssh/en/index.html#ipv4-vs-ipv6",
    "href": "posts/2024/2024-01-22-did-you-ever-ssh/en/index.html#ipv4-vs-ipv6",
    "title": "Have you ever SSH before?",
    "section": "IPv4 vs IPv6",
    "text": "IPv4 vs IPv6\nThere are two styles (or models) of IP addresses, IPv4 and IPv6. The IPv4 model is the one that you are looking for! So search for an IP address that have the IPv4 format, which are 4 numbers separated by dots. Examples: 127.0.0.1 (also known as the IP address for localhost) and 172.16.254.1."
  },
  {
    "objectID": "posts/2024/2024-01-22-did-you-ever-ssh/en/index.html#private-vs-public-ip-address",
    "href": "posts/2024/2024-01-22-did-you-ever-ssh/en/index.html#private-vs-public-ip-address",
    "title": "Have you ever SSH before?",
    "section": "Private vs public IP address",
    "text": "Private vs public IP address\nThere is also the distinction between private and public IP addresses that you need to be aware of. Some IP addresses are private in the meaning that they can only be acessed from a very specific location, which is your local network. That is, private IP addresses are visible only within the same WiFi/Ethernet you are connected in.\nIn other words, if you are like in Italy for example, and you are trying to access a server that is in France, using the private IP address of that server, then, you will never succeed, it is impossible. Because a private IP address is only accesible if you were connected to the same WiFi/Ethernet that the server is. So you would need to be in the same phisical location of the server in France, and connected to the same network (WiFi/Ethernet) as the server to access it using it‚Äôs private IP address.\nAll of this means that, if you are trying to connect to a remote computer that is far away from you, then you certainly need the public IP address of this computer, not the private one. So you should always look for the public IP address of the computer you are trying to login in."
  },
  {
    "objectID": "posts/2024/2024-01-22-did-you-ever-ssh/en/index.html#where-to-find-it",
    "href": "posts/2024/2024-01-22-did-you-ever-ssh/en/index.html#where-to-find-it",
    "title": "Have you ever SSH before?",
    "section": "Where to find it?",
    "text": "Where to find it?\nIf, and only if, your SSH Server is actually a Windows machine (which is not commom for servers in general), then, you could open the Windows Settings Menu, and look for the section Network & internet, and look for the ‚ÄúProperties‚Äù window of the WiFi/Ethernet that your server is connected to.\nIn the print below, I used my Windows machine as an example, and I covered the sensible information with a red rectangle. But the IPv4 address can be found where I am pointing with the blue arrow (sorry, my Windows machine is in Portuguese):\n\n\n\nLooking the IP Address in Windows\n\n\nBut in contrast, if your server is a Linux machine, then, you can open a terminal, and run the command ifconfig, then look for the IPv4 address that will appear after the inet word.\nifconfig"
  },
  {
    "objectID": "posts/2024/2024-01-22-did-you-ever-ssh/en/index.html#the-ip-address-of-my-ubuntu-ssh-server",
    "href": "posts/2024/2024-01-22-did-you-ever-ssh/en/index.html#the-ip-address-of-my-ubuntu-ssh-server",
    "title": "Have you ever SSH before?",
    "section": "The IP address of my Ubuntu SSH Server",
    "text": "The IP address of my Ubuntu SSH Server\nI cannot share the public IP address of Ubuntu machine, because if I did, then, I could suffer from remote attacks. If people know where my machine is, then, they can try to access it and break it.\nThat is why, in this article, I‚Äôm going to use a fictituous IP address over the next examples. Let‚Äôs suppose I looked for the IPv4 of my Ubuntu computer, which is the SSH Server in this experiment, and I found the IP 171.0.0.1. So every time you see this IP 171.0.0.1, you know that this is the IP of my SSH Server."
  },
  {
    "objectID": "posts/2024/2024-01-22-did-you-ever-ssh/en/index.html#footnotes",
    "href": "posts/2024/2024-01-22-did-you-ever-ssh/en/index.html#footnotes",
    "title": "Have you ever SSH before?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.ssh.com/academy/ssh/sshd‚Ü©Ô∏é\nhttps://www.ssh.com/academy/ssh/keygen‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022/2022-04-06-3nd-edition-rbook/pt/index.html",
    "href": "posts/2022/2022-04-06-3nd-edition-rbook/pt/index.html",
    "title": "Novidades da 3¬∞ edi√ß√£o de Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica",
    "section": "",
    "text": "Introdu√ß√£o\n√â com muito prazer que venho compartilhar com voc√™ as novidades que estou trazendo para essa nova edi√ß√£o do livro Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica. O lan√ßamento desta nova edi√ß√£o deve ocorrer ainda no meio de Abril de 2022 üìÜ, portanto, fique atento üòâ.\n\n\nO que temos de novo?\nTemos seis novos cap√≠tulos ü§Ø, os quais comp√µe uma nova parte do livro e, trazem consigo, grandes avan√ßos sobre o processo de desenvolvimento de programas no R. Al√©m disso, v√°rias melhorias foram feitas em se√ß√µes espec√≠ficas do livro, especialmente sobre o cap√≠tulo 2.\nPrimeiro, houve uma reorganiza√ß√£o significativa do livro. Os cap√≠tulos pr√©-existentes foram reorganizados em quatro partes diferentes. J√° a quinta e nova parte (Fun√ß√µes e Loops: construindo os seus pr√≥prios programas e automatizando tarefas) √© composta pelos seis novos cap√≠tulos introduzidos nesta edi√ß√£o.\nSegundo, o cap√≠tulo de Fun√ß√µes e Loops foi completamente reescrito, expandido, e, repartido em dois novos cap√≠tulos ü•≥üéâüéâüéâ. Mais especificamente, os cap√≠tulos 14 (Fun√ß√µes) e 15 (Loops) desta edi√ß√£o.\nTerceiro, o cap√≠tulo 16 desta edi√ß√£o traz uma das novidades programadas desde a segunda edi√ß√£o: um novo (e robusto) cap√≠tulo sobre functional programming. Com essa adi√ß√£o, esta obra se torna uma refer√™ncia mais moderna e, se aproxima dos padr√µes adotados hoje pela comunidade internacional de R üòé.\nQuarto, environments e as regras de scoping da linguagem s√£o dois t√≥picos relativamente avan√ßados, e que costumam causar certa confus√£o em muitos iniciantes. Por isso, um novo cap√≠tulo foi produzido para descrever essas funcionalidades que sustentam partes essenciais da linguagem, assim como de alguns dos pacotes que introduzimos ao longo do livro (e.g.¬†dplyr e ggplot2).\nQuinto, v√°rias adi√ß√µes e melhorias foram feitas no cap√≠tulo 6 (Introdu√ß√£o a base de dados relacionais com dplyr) e, principalmente, no cap√≠tulo 2 (Fundamentos da Linguagem R). Dentre elas, temos: um novo estudo de caso (Importando os dados da PINTEC)Õæ novas se√ß√µes sobre as fun√ß√µes str() e is.*()Õæ melhorias significativas sobre as se√ß√µes de Coer√ß√£o no R e Valores especiais do R.\nSexto, foi introduzido um novo cap√≠tulo que descreve os controles condicionais de fluxo (if else statements e switch()) que a linguagem R oferece.\nS√©timo, um novo e pequeno cap√≠tulo foi adicionado √† segunda parte do livro, com o objetivo de introduzir o universo do tidyverse ao leitor de maneira mais clara e amig√°vel ü•∞.\nOitavo, o ap√™ndice contendo as respostas dos exerc√≠cios foi retirado, com o objetivo de reduzir o n√∫mero de p√°ginas do livro. Consequentemente, as respostas dos exerc√≠cios est√£o sendo disponibilizadas em um PDF separado, o qual pode ser baixado gratuitamente, a partir da p√°gina de publica√ß√£o do livro.\n\n\nSobre onde encontrar o livro\nAssim como ocorreu nas edi√ß√µes anteriores, voc√™ poder√° ler toda a obra de maneira gratuita e aberta atrav√©s de seu website üìñ.\nContudo, diferente das edi√ß√µes anteriores, a vers√£o em PDF da obra n√£o ser√° aberta. Voc√™ poder√° adquirir uma vers√£o em PDF ou f√≠sica do livro atrav√©s da loja da Amazon. Ao comprar essas vers√µes, voc√™ estar√° me ajudando a continuar contribuindo com a nossa comunidade ‚ù§Ô∏è.\n\n\nContribua para a obra ou fa√ßa sugest√µes!\nCaso seja de seu interesse, voc√™ pode contribuir diretamente para a obra, ao postar pull requests dentro de seu reposit√≥rio oficial. Para mais, voc√™ tamb√©m pode fazer sugest√µes ou coment√°rios, ao postar issues neste mesmo reposit√≥rio."
  },
  {
    "objectID": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html",
    "href": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html",
    "title": "Erro de mem√≥ria no Power BI Online? Hora de repensar o seu dashboard",
    "section": "",
    "text": "Em um dia recente, eu descobri que um dos dashboards que estavam publicados em nosso ambiente de produ√ß√£o do Power BI Online havia sofrido um erro de atualiza√ß√£o. Logo, eu prontamente parei o que estava fazendo e comecei a investigar o motivo do erro, at√© porque: 1) o nosso cliente depende dos dados desse dashboard ; e 2) erros em produ√ß√£o n√£o s√£o legais!\nNesse post, eu quero mostrar como esse tipo de erro de atualiza√ß√£o pode ser um forte sinal de que voc√™ precisa repensar o design de seu dashboard. Em outras palavras, meu objetivo √© mostrar que se voc√™ est√° puxando milh√µes e milh√µes de linhas de dados para um dashboard‚Ä¶ √© prov√°vel que voc√™ n√£o tenha entendido o que √© um dashboard e qual o seu prop√≥sito."
  },
  {
    "objectID": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#hora-de-explorar-o-terreno-desconhecido",
    "href": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#hora-de-explorar-o-terreno-desconhecido",
    "title": "Erro de mem√≥ria no Power BI Online? Hora de repensar o seu dashboard",
    "section": "Hora de explorar o terreno desconhecido",
    "text": "Hora de explorar o terreno desconhecido\nNesse dia, eu estava em minha primeira semana em uma nova equipe, e, eu n√£o conhecia esse dashboard. Em outras palavras, eu havia herdado esse dashboard, que foi criado pelos analistas anteriores dessa equipe. Logo, eu precisava abrir o .pbix desse dashboard e come√ßar a investigar, tentando descobrir que mist√©rios e perigos est√£o escondidos dentro dele.\nInicialmente, percebi oito tabelas associadas ao dashboard que foram puxadas diretamente dos nossos databases SQL (campanhas, cartoes_selecionados, usuarios_por_dia, formatos, inputs, usuarios_por_mes, perfis e tracks), as quais est√£o expostas na imagem abaixo1. Al√©m delas, temos outras duas tabelas calculadas no pr√≥prio .pbix, atrav√©s de DAX (dCalendario e Medidas).\n\n\n\n\n\nTabelas associadas ao arquivo .pbix\n\n\n\n\nDecidi simplesmente clicar no bot√£o de Atualizar. J√° que o erro ocorreu durante a atualiza√ß√£o, imaginei que seria mais simples descobrir a fonte do problema dessa forma.\nA maioria das tabelas atualizaram rapidamente. Por√©m, a tabela input ainda estava atualizando. Em resumo, essa tabela continha todas as mensagens digitadas por todos os usu√°rios que acessaram o nosso sistema.\nO tempo foi passando, e ap√≥s 3 horas com a atualiza√ß√£o rodando em meu computador, o Power BI j√° havia puxado mais de 80 milh√µes de linhas para essa √∫nica tabela. Decidi verificar se o recurso de Atualiza√ß√£o incremental estava ligado para essa tabela input, e, percebi que ele estava desligado."
  },
  {
    "objectID": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#a-fonte-do-problema",
    "href": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#a-fonte-do-problema",
    "title": "Erro de mem√≥ria no Power BI Online? Hora de repensar o seu dashboard",
    "section": "A fonte do problema",
    "text": "A fonte do problema\nPortanto, a fonte do problema estava claro. Como o recurso de ‚ÄúAtualiza√ß√£o incremental‚Äù estava desligado para essa tabela input, a cada atualiza√ß√£o, o Power BI Online estava recalculando toda a tabela input de uma vez s√≥.\nIsso significa que, todos os dias, o Power BI estava coletando todas as 80 milh√µes de linhas dessa tabela input. Devido a este volume monumental de dados, o servi√ßo do Power BI decidiu interromper a atualiza√ß√£o."
  },
  {
    "objectID": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#manuten√ß√£o-se-torna-um-peso-grande",
    "href": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#manuten√ß√£o-se-torna-um-peso-grande",
    "title": "Erro de mem√≥ria no Power BI Online? Hora de repensar o seu dashboard",
    "section": "Manuten√ß√£o se torna um peso grande",
    "text": "Manuten√ß√£o se torna um peso grande\nAtualizar 80 milh√µes de linhas n√£o √© pr√°tico, n√£o √© r√°pido, e √© dif√≠cil de manter e testar. Se por algum motivo, voc√™ precisar atualizar todos os dados de seu dashboard2, voc√™ vai muito provavelmente perder uma tarde, talvez um dia inteiro de trabalho s√≥ para completar a atualiza√ß√£o.\n\nInsight 2: Um dashboard deve ser f√°cil de se manter e expandir. Em contrapartida, manter um volume grande de dados em um dashboard √© trabalhoso demais."
  },
  {
    "objectID": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#vamos-pensar-um-pouco-sobre-user-experience",
    "href": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#vamos-pensar-um-pouco-sobre-user-experience",
    "title": "Erro de mem√≥ria no Power BI Online? Hora de repensar o seu dashboard",
    "section": "Vamos pensar um pouco sobre user experience",
    "text": "Vamos pensar um pouco sobre user experience\nPor um momento, vamos adotar o papel de um UX, e refletir sobre a experi√™ncia dos usu√°rios que consomem o nosso dashboard. √â esquisito pensar dessa forma, entretanto, √†s vezes, n√≥s nos esquecemos que pessoas de verdade usam o nosso produto (nesse caso, o dashboard) e se baseiam nele diariamente para desempenhar trabalhos e planejamentos importantes. Portanto, √© muito importante que eles tenham uma experi√™ncia agrad√°vel utilizando o nosso dashboard.\n\nDefinindo o p√∫blico-alvo: Primeiro, quem utiliza o nosso dashboard?\n\nNa maioria das vezes, quem est√° consumindo os nossos dashboards s√£o gerentes de alguma √°rea. Gente importante, que tem pouco tempo no dia, e que lidam com v√°rias tarefas e responsabilidades ao mesmo tempo.\n√â justamente por essa escassez de tempo e aten√ß√£o que, em geral, gerentes gostam muito de dashboards. Eles gostam de entrar num dashboard, e rapidamente visualizar todos os indicadores que eles precisam acompanhar. Com isso, eles n√£o precisam gastar horas e horas ca√ßando n√∫meros em diferentes lugares, e com diferentes pessoas. Est√° tudo concentrado em um lugar √∫nico e de f√°cil acesso.\nPor isso, um dashboard tem que ser r√°pido. Todo gerente tem pressa, ent√£o, a p√°gina inicial do dashboard precisa carregar r√°pido! Navegar pelas diferentes p√°ginas e vis√µes do dashboard tamb√©m precisa ser uma experi√™ncia r√°pida e fl√∫ida. Ningu√©m gosta de uma p√°gina que demora 5 minutos para carregar‚Ä¶ muito menos um gerente.\n√â por esse mesmo motivo, que cada p√°gina de um dashboard precisa ser focada em um tema central, e manter o m√≠nimo poss√≠vel de informa√ß√£o que o gerente precisa, da forma mais clara poss√≠vel. Se uma mesma p√°gina mistura diferentes temas, o leitor pode ter dificuldade em navegar pelos indicadores e encontrar o que ele est√° procurando (ou seja, misturar temas = confus√£o mental!).\n\nAh! Achei a p√°gina com os indicadores de vendas. Ok. Espera! Por que os indicadores de atendimento est√£o nessa p√°gina? Onde est√° o n√∫mero de vendas de maquininhas nesse m√™s? Ahhh achei! N√£o, espera‚Ä¶ Esse √© o n√∫mero de maquininhas vendidas somente no setor de atendimento, mas eu quero os n√∫meros de venda em TODOS os setores‚Ä¶\n\nPortanto, dashboards precisam ser r√°pidos, claros e bem dividos! E se voc√™ est√° puxando um volume muito grande de dados para dentro dele, voc√™ com certeza vai impactar negativamente a rapidez desse dashboard, pois ele precisa carregar o grande conjunto de dados que voc√™ inseriu l√° dentro. Al√©m disso, um grande volume de dados pode indicar que voc√™ est√° preenchendo esse dashboard com informa√ß√µes que s√£o irrelevante para o gerente.\n\nInsight 3: Dashboards precisam ser r√°pidos, claros e bem dividos!"
  },
  {
    "objectID": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#gerentes-querem-indicadores-e-agregados-n√£o-dados-brutos",
    "href": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#gerentes-querem-indicadores-e-agregados-n√£o-dados-brutos",
    "title": "Erro de mem√≥ria no Power BI Online? Hora de repensar o seu dashboard",
    "section": "Gerentes querem indicadores e agregados! N√£o dados brutos‚Ä¶",
    "text": "Gerentes querem indicadores e agregados! N√£o dados brutos‚Ä¶\nGerentes querem acompanhar indicadores e agregados que descrevam de maneira r√°pida a situa√ß√£o atual do neg√≥cio que eles gerem, e das pessoas que est√£o envolvidas nele.\nLogo, por que trazer dados brutos para o dashboard? Por que trazer para o dashboard uma tabela com a lista completa de todos os usu√°rios que visitaram o nosso servi√ßo em todos os dias do ano? Se eu posso simplesmente trazer uma tabela j√° agregada, com o n√∫mero de usu√°rios que visitaram esse servi√ßo dentro de cada dia do ano?\n\nInsight 4: Gerentes est√£o interessados em acompanhar indicadores e agregados, ao inv√©s de dados brutos. Portanto, importe os seus dados j√° agregados para dentro do dashboard."
  },
  {
    "objectID": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#isso-significa-que-dados-brutos-geralmente-n√£o-devem-estar-em-um-dashboard",
    "href": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#isso-significa-que-dados-brutos-geralmente-n√£o-devem-estar-em-um-dashboard",
    "title": "Erro de mem√≥ria no Power BI Online? Hora de repensar o seu dashboard",
    "section": "Isso significa que dados brutos geralmente n√£o devem estar em um dashboard",
    "text": "Isso significa que dados brutos geralmente n√£o devem estar em um dashboard\nIsso tudo n√£o significa que gerentes n√£o consomem dados brutos em momento algum. Mas isso significa que um dashboard √© geralmente o lugar errado para esses dados brutos.\n√â at√© comum em certos momentos um gerente pedir para n√≥s coletarmos um conjunto espec√≠fico de dados brutos para ele. Mas se voc√™ refletir sobre todas as ocasi√µes onde isso ocorreu, voc√™ talvez consiga perceber que essas situa√ß√µes caem em duas categorias:\n\no gerente queria investigar um problema bem espec√≠fico, e √© bem prov√°vel que esse problema n√£o se repita, logo, ele nunca mais vai precisar desses dados brutos espec√≠ficos novamente (i.e.¬†foi uma entrega pontual);\no gerente precisa desses dados brutos com certa frequ√™ncia para alimentar algum fluxo de trabalho que voc√™ n√£o conhece, ou est√° em uma equipe/setor diferente do seu;\n\nEssas duas categorias n√£o justificam incluir dados brutos em um dashboard. Para a primeira situa√ß√£o, voc√™ pode armazenar a query (ou o script) que voc√™ usou no momento para puxar os dados brutos que o gerente te pediu naquele momento. Desse modo, se l√° na frente, por algum motivo voc√™ precisar extrair novamente esses dados brutos, voc√™ precisa apenas retornar √† query e execut√°-la novamente. Al√©m disso, √© muito mais econ√¥mico armazenar centenas de queries distintas, do que armazenar os dados brutos produzidos em cada uma dessas queries (imagine ter centenas de CSV‚Äôs de v√°rios MB‚Äôs armazenados no seu computador‚Ä¶).\nEm minha equipe de trabalho, n√≥s utilizamos um board de cards no estilo kanban (como o Asana, ou o Azure DevOps, etc.) para organizar as nossas tarefas e demandas. Eu particularmente gosto de sempre salvar a query que eu utilizei para completar uma demanda, dentro do card correspondente a essa demanda. Sendo assim, caso eu precisar utilizar essa mesma query, eu procuro rapidamente pelo card dessa demanda no hist√≥rico do nosso board de cards, e, copio a query que est√° salva l√° dentro desse card.\nJ√° na segunda situa√ß√£o, faz mais sentido criar rotinas automatizadas de envio desses dados para o gerente, ou para qualquer que seja a pessoa que esteja precisando desses dados. Ou seja, se por exemplo, o Paulo precisa extrair toda semana, uma lista com todos os usu√°rios que s√£o eleg√≠veis a adquirir um empr√©stimo, eu posso, por exemplo, criar uma rotina automatizada em Python ou em R, que pega os dados brutos do nosso database SQL, filtra todos esses usu√°rios eleg√≠veis, e compila esses dados em um formato agrad√°vel e intuitivo, e por fim, envia esses dados para o Paulo, seja por email, ou talvez, para um servidor ou uma pasta na nuvem que o Paulo tem acesso."
  },
  {
    "objectID": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#dashboards-s√£o-ferramentas-de-uso-di√°rio",
    "href": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#dashboards-s√£o-ferramentas-de-uso-di√°rio",
    "title": "Erro de mem√≥ria no Power BI Online? Hora de repensar o seu dashboard",
    "section": "Dashboards s√£o ferramentas de uso di√°rio",
    "text": "Dashboards s√£o ferramentas de uso di√°rio\nO gerente depende desse dashboard todos os dias (ou talvez toda semana) para extrair informa√ß√µes importantes sobre o seu neg√≥cio. Portanto, dashboards possuem uma frequ√™ncia de uso alta, e por essa caracter√≠stica, √© essencial que um dashboard seja est√°vel e que esteja sempre o mais atualizado poss√≠vel.\nContudo, ao puxar milh√µes e milh√µes de linhas de dados para um dashboard, todos os dias, voc√™ est√° aumentando as chances desse dashboard enfrentar um erro de atualiza√ß√£o. Isso n√£o √© algo est√°vel!\nVoc√™ n√£o quer erros acontecendo no seu dashboard, pois voc√™ n√£o quer perder uma tarde inteira de trabalho investigando onde nas milh√µes e milh√µes de linhas que voc√™ puxou est√° a fonte do erro. Voc√™ tamb√©m n√£o quer perder horas e horas atualizando essas milh√µes de linhas. Isso est√° bastante relacionado tamb√©m com o insight 2, onde definimos que um dashboard deve ser f√°cil de se manter e expandir.\n\nInsight 5: Dashboards precisam ser est√°veis! Quanto menos erros ele gerar, melhor para voc√™ (que vai gastar menos tempo de debugging e manuten√ß√£o) e tamb√©m para o gerente que utiliza esse dash.\n\nVoc√™ quer um dashboard simples, leve, claro, e que funcione da forma como voc√™ esperava que ele funcionasse. Na empresa onde trabalho, temos um lema: ‚ÄúA simplicidade √© o mais alto n√≠vel de sofistica√ß√£o‚Äù. Portanto, leve essa simplicidade para os seus dashboards. N√£o tente fazer coisas complexas e confusas, que s√£o dif√≠ceis de se entender e de investigar (ou ‚Äúdesbugar‚Äù)."
  },
  {
    "objectID": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#as-vezes-n√£o-existe-maneira-simples-de-contornar-o-problema",
    "href": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#as-vezes-n√£o-existe-maneira-simples-de-contornar-o-problema",
    "title": "Erro de mem√≥ria no Power BI Online? Hora de repensar o seu dashboard",
    "section": "As vezes n√£o existe maneira simples de contornar o problema",
    "text": "As vezes n√£o existe maneira simples de contornar o problema\nEm certas ocasi√µes, os usu√°rios de um dashboard realmente precisam ver algum tipo de dado bruto dentro dele. Nesses casos, voc√™ tra√ßar algumas outras estrat√©gias e perguntas para reduzir ao m√°ximo o n√∫mero de linhas importadas. Por exemplo:\n\neu posso trazer para dentro dashboard uma parte bastante filtrada dos dados brutos?\n\nOu seja, ao inv√©s de trazer as 80 milh√µes de linhas, ser√° que eu consigo aplicar v√°rios filtros sobre esses dados brutos, antes de import√°-los para dentro do dashboard? Esses filtros podem te ajudar a reduzir drasticamente o n√∫mero de linhas carregadas.\n\nao inv√©s de trazer todos os dados, por que n√£o trazer uma amostra aleat√≥ria da popula√ß√£o?\n\n√â √∫til entender o porqu√™ exatamente o seu usu√°rio precisa ver algum dado bruto em seu dashboard. Ao entender o que esse usu√°rio est√° perseguindo, voc√™ talvez chegue a conclus√£o de que o seu usu√°rio j√° ficaria satisfeito ao ver pelo menos uma parte dos dados brutos (n√£o precisa trazer literalmente tudo). Portanto, voc√™ poderia selecionar uma amostra aleat√≥ria dos dados, e importar apenas essa amostra para dentro do dashboard.\n\nser√° que eu preciso manter o hist√≥rico desses dados brutos dentro do dash?\n\nSer√° que o seu cliente precisa frequentemente visualizar os dados brutos de 6 meses atr√°s? √â muito prov√°vel que n√£o. Ent√£o, por que n√£o manter apenas os dados brutos dos √∫ltimos 30 dias? Em outras palavras, os dados hist√≥ricos s√£o sempre limpos, e apenas os dados brutos mais recentes s√£o mantidos.\nAo reduzir o volume de dados mantidos dentro do dashboard, voc√™ talvez traga uma melhoria consider√°vel sobre o tempo de navega√ß√£o e carregamento do dashboard (lembre-se, um dashboard preciso ser r√°pido, claro e bem divido)."
  },
  {
    "objectID": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#footnotes",
    "href": "posts/2022/2022-12-14-power-bi-memory-error/pt/index.html#footnotes",
    "title": "Erro de mem√≥ria no Power BI Online? Hora de repensar o seu dashboard",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVale destacar que essas tabelas foram renomeadas com o objetivo de manter o anonimato do cliente e dos dados associados a elas.‚Ü©Ô∏é\nComo exemplo, talvez a fonte dos dados sofreu uma atualiza√ß√£o, e voc√™ quer trazer essa atualiza√ß√£o para o seu dashboard, ou ent√£o, porque houve uma perda de dados no Power BI Online e voc√™ deseja recuperar esses dados perdidos.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022/2022-11-06-figma-pkgv0.1.0/en/index.html",
    "href": "posts/2022/2022-11-06-figma-pkgv0.1.0/en/index.html",
    "title": "Introducing the {figma} R package",
    "section": "",
    "text": "I‚Äôm very happy to announce the realease of figma R package to CRAN! This package provides a R interface (or a web client/wrapper) to the Figma API. Below we have all the important links about this package:\n Official repository  Package website  Page on CRAN\nThis is the first release (or the first version) of the package, and for now, it have all the necessary functionality to get all data from a Figma file, and bring it to your R session. But in the future, the package will include more functionalities and endpoints. Please, if you can, test this package and give feedbacks or report bugs by sending Issues on the official repository."
  },
  {
    "objectID": "posts/2022/2022-11-06-figma-pkgv0.1.0/en/index.html#introduction",
    "href": "posts/2022/2022-11-06-figma-pkgv0.1.0/en/index.html#introduction",
    "title": "Introducing the {figma} R package",
    "section": "",
    "text": "I‚Äôm very happy to announce the realease of figma R package to CRAN! This package provides a R interface (or a web client/wrapper) to the Figma API. Below we have all the important links about this package:\n Official repository  Package website  Page on CRAN\nThis is the first release (or the first version) of the package, and for now, it have all the necessary functionality to get all data from a Figma file, and bring it to your R session. But in the future, the package will include more functionalities and endpoints. Please, if you can, test this package and give feedbacks or report bugs by sending Issues on the official repository."
  },
  {
    "objectID": "posts/2022/2022-11-06-figma-pkgv0.1.0/en/index.html#getting-started",
    "href": "posts/2022/2022-11-06-figma-pkgv0.1.0/en/index.html#getting-started",
    "title": "Introducing the {figma} R package",
    "section": "Getting started",
    "text": "Getting started\nFist of all, you need to install the package on your machine, and to do that, you can use this code:\n\ninstall.packages(\"figma\")\n\nOr, to install the development version directly from GitHub:\n\ndevtools::install_github(\"pedropark99/figma\")\n\nNow, to get the data of a Figma file through the Figma API, you have to collect two key variables about your file and your credentials. They are:\n\nfile_key: The ID (or the ‚Äúkey‚Äù) that identifies your Figma file;\ntoken: Your personal access token from the Figma platform;\n\nTo use specifically the figma::get_figma_page() function, you will need to collect a third information, which is the node_id, or the ID that identifies a canvas/page of your Figma file. I explain, in details, on how to collect these key variables on the main vignette package. For brevity reasons, lets assume in this blog post that you already have collected these variables."
  },
  {
    "objectID": "posts/2022/2022-11-06-figma-pkgv0.1.0/en/index.html#use-get_figma_file-to-get-your-figma-file",
    "href": "posts/2022/2022-11-06-figma-pkgv0.1.0/en/index.html#use-get_figma_file-to-get-your-figma-file",
    "title": "Introducing the {figma} R package",
    "section": "Use get_figma_file() to get your Figma file",
    "text": "Use get_figma_file() to get your Figma file\nNow that you have the key (or ID) that identifies your Figma file, and your personal token that identifies yourself, you can use figma::get_figma_file() to get your Figma file:\n\nlibrary(figma)\nfile_key &lt;- \"hch8YlkIrYbU3raDzjPvCz\"\n# Insert your personal token:\ntoken &lt;- \"Your personal token ...\"\n\n# Returns a `httr::response` object:\nfigma_file &lt;- figma::get_figma_file(\n  file_key, token\n)\n\nThe functions from figma package returns a httr::response object by default. But you can use the .output_format argument to fit the data into a more intuitive data strucuture. For example, a tibble::tibble object:\n\n# Returns a `tibble::tibble` object:\nfigma_file &lt;- figma::get_figma_file(\n  file_key, token,\n  .output_format = \"tibble\"\n)\n\nprint(figma_file)\n\n#&gt; # A tibble: 5 √ó 7\n#&gt;   canvas_id canvas_name canvas_type object_id object_name   objec‚Ä¶¬π object_att‚Ä¶¬≤\n#&gt;   &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;   &lt;list&gt;      \n#&gt; 1 0:1       Page 1      CANVAS      1:2       Background    RECTAN‚Ä¶ &lt;named list&gt;\n#&gt; 2 0:1       Page 1      CANVAS      5:2       Paragraph     TEXT    &lt;named list&gt;\n#&gt; 3 0:1       Page 1      CANVAS      5:3       Arrow         VECTOR  &lt;named list&gt;\n#&gt; 4 5:4       Page 2      CANVAS      5:5       BackgroundPa‚Ä¶ RECTAN‚Ä¶ &lt;named list&gt;\n#&gt; 5 5:4       Page 2      CANVAS      5:6       Texto da p√°g‚Ä¶ TEXT    &lt;named list&gt;\n#&gt; # ‚Ä¶ with abbreviated variable names ¬π‚Äãobject_type, ¬≤‚Äãobject_attributes\nEach row in the above data.frame is an object drawn in a canvas/page in your Figma file. Every object that you drawn in the canvas of your Figma file, can have a different type (like RECTANGLE, CIRCLE, TEXT, etc.). And objects of different types usually have different attributes.\nFor example, TEXT objects have a characters attribute, which have the exact text that is written in the text box. In the other hand, RECTANGLE objects does not have such attribute.\nAll the attributes of each object are in the ‚Äãobject_attributes column, which is a &lt;list&gt; column. Each row (or, if you prefer, each element) in this column is a list with all the attributes of the object described in the corresponding row. For example, to collect the characters attribute of the first TEXT object described in figma_file (which is in the 2nd row), you could do this:\n\nfigma_file$object_attributes[[2]][[\"characters\"]]\n\n[1] \"Um texto qualquer, que n√£o sei se vai dar certo\"\nOr, you could the functions from dplyr and purrr packages to extract the characters attribute from all TEXT objects of your Figma file, like this:\n\ntext_objects &lt;- figma_file |&gt; \n  dplyr::filter(object_type == 'TEXT') \n\ntext_objects[['object_attributes']] |&gt; \n  purrr::map_chr('characters')\n\n[1] \"Um texto qualquer, que n√£o sei se vai dar certo\" \"Texto da p√°gina 2\""
  },
  {
    "objectID": "posts/2023/2023-01-10-join-duplicates/pt/index.html",
    "href": "posts/2023/2023-01-10-join-duplicates/pt/index.html",
    "title": "JOINs s√£o importantes fontes de duplica√ß√µes em seus dados",
    "section": "",
    "text": "Quando estamos construindo uma nova tabela de dados, √© muito comum compararmos os nossos resultados com tabelas anteriores e nos depararmos com problemas de diverg√™ncia nos dados. Isto √©, situa√ß√µes como:\n\nUhmm‚Ä¶ a tabela antiga indica que o n√∫mero de maquininhas vendidas no m√™s de Dezembro foi de 1387. Por√©m, esse mesmo indicador na tabela nova est√° em 1824 para o mesmo m√™s de Dezembro. Porque esse aumento?\n\nAumentos desse tipo podem ocorrer por uma variedade de raz√µes. Por√©m, opera√ß√µes de JOIN tem sido uma raz√£o espec√≠fica que tenho enfrentado com muita frequ√™ncia em meu trabalho. Em outras palavras, JOINs s√£o fontes extremamente comuns de dados duplicados. Como resultado, esses dados duplicados acabam gerando um ‚Äúefeito expansivo‚Äù sobre os seus indicadores e suas tabelas.\nMuitos analistas n√£o compreendem o porqu√™, ou n√£o enxergam como isso pode ocorrer. Nesse post, busco justamente esclarecer como uma opera√ß√£o de JOIN pode causar esse ‚Äúefeito expansivo‚Äù em seus dados. Eu tamb√©m explico esse efeito em detalhes no cap√≠tulo 6 do meu livro introdut√≥rio da linguagem R. Mais especificamente, a partir da se√ß√£o Rela√ß√µes entre keys: primary keys s√£o menos comuns do que voc√™ pensa. Portanto, grande parte do conhecimento exposto aqui s√£o refer√™ncias diretas ao livro."
  },
  {
    "objectID": "posts/2023/2023-01-10-join-duplicates/pt/index.html#necessidade-inicial",
    "href": "posts/2023/2023-01-10-join-duplicates/pt/index.html#necessidade-inicial",
    "title": "JOINs s√£o importantes fontes de duplica√ß√µes em seus dados",
    "section": "Necessidade inicial",
    "text": "Necessidade inicial\nVamos supor que, no in√≠cio, voc√™ precisava manter um indicador de ‚Äún√∫mero de usu√°rios por cor de pele‚Äù em um relat√≥rio. Para isso, voc√™ simplesmente contava o n√∫mero de linhas na tabela cores_de_pele agrupado pelos valores da coluna Cor. Como demonstrado abaixo:\n\nlibrary(dplyr)\nusuarios_por_cor &lt;- cores_de_pele |&gt;\n    group_by(Cor) |&gt;\n    summarise(N_usuarios = n())\n\nusuarios_por_cor\n\n# A tibble: 3 √ó 2\n  Cor     N_usuarios\n  &lt;chr&gt;        &lt;int&gt;\n1 Amarelo          1\n2 Branco           2\n3 Pardo            1"
  },
  {
    "objectID": "posts/2023/2023-01-10-join-duplicates/pt/index.html#uma-nova-necessidade",
    "href": "posts/2023/2023-01-10-join-duplicates/pt/index.html#uma-nova-necessidade",
    "title": "JOINs s√£o importantes fontes de duplica√ß√µes em seus dados",
    "section": "Uma nova necessidade",
    "text": "Uma nova necessidade\nPor√©m, uma nova necessidade surge no time. Agora, voc√™ precisa calcular tamb√©m a ‚Äúaltura m√©dia por cor de pele‚Äù. Voc√™ sabe que as alturas dos usu√°rios est√£o armazenadas na tabela alturas, contudo, voc√™ precisa trazer essas alturas para dentro da tabela cores_de_pele, para que voc√™ possa de fato calcular a altura m√©dia para cada cor de pele.\nPortanto, voc√™ precisa realizar um JOIN entre essas tabelas, e √© isso que estamos fazendo no c√≥digo abaixo:\n\ndados &lt;- cores_de_pele |&gt;\n    left_join(alturas, by = \"ID\")\n\nAgora, temos uma nova tabela chamada dados que cont√©m todos os dados que precisamos para calcular ambos os indicadores (‚Äún√∫mero de usu√°rios‚Äù e ‚Äúaltura m√©dia‚Äù) para cada cor de pele. Entretanto, quando calculamos esses indicadores com essa tabela dados, perceba que o n√∫mero de usu√°rios (indicador N_usuarios) de cor ‚ÄúAmarelo‚Äù sofreu um aumento repentino.\n\nindicadores &lt;- dados |&gt;\n    group_by(Cor) |&gt;\n    summarise(\n        N_usuarios = n(),\n        Altura_media = mean(Altura, na.rm = TRUE)\n    )\n\nindicadores\n\n# A tibble: 3 √ó 3\n  Cor     N_usuarios Altura_media\n  &lt;chr&gt;        &lt;int&gt;        &lt;dbl&gt;\n1 Amarelo          3         1.74\n2 Branco           2         1.58\n3 Pardo            1       NaN   \n\n\nRepare que n√≥s n√£o mudamos a f√≥rmula de c√°lculo do indicador N_usuarios. N√≥s aplicamos a mesma fun√ß√£o n() que utilizamos anteriormente. N√≥s tamb√©m agrupamos a tabela dados pela coluna Cor com group_by(), da mesma forma que fizemos anteriormente. Porque esse aumento ocorreu?\nN√≥s n√£o mudamos nada na f√≥rmula de c√°lculo do indicador N_usuarios. Por√©m, n√≥s introduzimos um novo item na cadeia de transforma√ß√µes da tabela. Mais especificamente, um LEFT JOIN realizado pela fun√ß√£o left_join(). Ou seja, o JOIN √© o que mudou nesse c√≥digo, e ele √© o culpado por esse estranho e repentino aumento no n√∫mero de usu√°rios de cor ‚ÄúAmarelo‚Äù."
  },
  {
    "objectID": "posts/2023/2023-01-10-join-duplicates/pt/index.html#o-que-aconteceu",
    "href": "posts/2023/2023-01-10-join-duplicates/pt/index.html#o-que-aconteceu",
    "title": "JOINs s√£o importantes fontes de duplica√ß√µes em seus dados",
    "section": "O que aconteceu?",
    "text": "O que aconteceu?\nSe olharmos bem para a tabela resultado do JOIN (tabela dados) podemos come√ßar a compreender o que aconteceu. Perceba que temos 6 linhas nessa tabela, isto √©, temos 2 linhas a mais que a tabela cores_de_pele (que possui 4 linhas). Perceba tamb√©m que temos 3 linhas nessa tabela descrevendo o mesmo usu√°rio de ID 105. Ou seja, temos dados duplicados para esse usu√°rio.\n\ndados\n\n# A tibble: 6 √ó 4\n     ID Cor     DataRegistro Altura\n  &lt;dbl&gt; &lt;chr&gt;   &lt;date&gt;        &lt;dbl&gt;\n1   100 Pardo   NA            NA   \n2   102 Branco  2022-01-10     1.58\n3   104 Branco  NA            NA   \n4   105 Amarelo 2022-01-10     1.72\n5   105 Amarelo 2022-06-12     1.74\n6   105 Amarelo 2022-08-24     1.75\n\n\nO usu√°rio de ID 105 √© o √∫nico usu√°rio de cor ‚ÄúAmarelo‚Äù na tabela. Portanto, essas 3 linhas referentes ao ID 105 s√£o a causa da mudan√ßa repentina no indicador N_usuarios para a cor ‚ÄúAmarelo‚Äù. Por√©m, como podemos ver abaixo, a tabela cores_de_pele tem 1 √∫nica linha para o usu√°rio de ID 105. Como essa √∫nica linha se transformou em tr√™s?\n\ncores_de_pele\n\n# A tibble: 4 √ó 2\n     ID Cor    \n  &lt;dbl&gt; &lt;chr&gt;  \n1   100 Pardo  \n2   102 Branco \n3   104 Branco \n4   105 Amarelo"
  },
  {
    "objectID": "posts/2023/2023-01-10-join-duplicates/pt/index.html#entenda-o-processo-de-pareamento-de-um-join",
    "href": "posts/2023/2023-01-10-join-duplicates/pt/index.html#entenda-o-processo-de-pareamento-de-um-join",
    "title": "JOINs s√£o importantes fontes de duplica√ß√µes em seus dados",
    "section": "Entenda o processo de pareamento de um JOIN",
    "text": "Entenda o processo de pareamento de um JOIN\nA √∫nica linha de ID 105 na tabela cores_de_pele se transformou em 3 linhas devido ao processo de pareamento dos dados realizado pelo JOIN. Todo JOIN, independe do tipo que ele seja (left, inner, right, full), vai sempre realizar um processo de pareamento entre os dados das duas tabelas, utilizando as colunas que representam as ‚Äúchaves‚Äù do JOIN (Faria 2022).\nPodemos visualizar esse processo de pareamento dos dados em Figure¬†1. Perceba que, tanto no exemplo dessa imagem, quanto no exemplo desse post, as chaves do JOIN s√£o representadas pela coluna ID. Logo, o JOIN vai puxar os dados de uma tabela para a outra, utilizando os valores dessa coluna como guia.\n\n\n\n\n\n\nFigure¬†1: Processo de pareamento realizado em um JOIN\n\n\n\nNo exemplo de Figure¬†1, ambas as tabelas que est√£o sendo unidas possuem uma linha para cada ID. Ou seja, n√£o existe IDs duplicados em nenhuma das duas tabelas, formando assim, uma rela√ß√£o de ‚Äúum para um‚Äù entre as chaves do JOIN.\nContudo, no exemplo deste post, a tabela alturas possui tr√™s linhas diferentes para o mesmo ID 105, enquanto a tabela cores_de_pele n√£o apresenta IDs duplicados. Isso acaba formando uma rela√ß√£o de ‚Äúum para muitos‚Äù entre as chaves do JOIN. Nesse caso, como o processo de pareamento do JOIN deve se comportar? Bem, o seguinte vai acontecer‚Ä¶\n\nO JOIN vai pegar o ID 105 da tabela cores_de_pele e pesquisar por ele ao longo da tabela alturas. Como resultado, o JOIN vai localizar tr√™s linhas distintas para o ID 105 na tabela alturas.\nO JOIN percebe que h√° um desequil√≠brio (1 linha de cores_de_pele \\(\\times\\) 3 linhas de alturas).\nPara reequilibrar essa balan√ßa, o JOIN vai executar um produto cartesiano entre as linhas dessas duas tabelas.\n\nA Figure¬†2 apresenta de forma visual essa conex√£o:\n\n\n\n\n\n\nFigure¬†2: Uma representa√ß√£o visual do efeito expansivo sobre o ID 105\n\n\n\nOu seja, o JOIN vai retornar como resultado, todas as combina√ß√µes poss√≠veis entre a linha √∫nica de cores_de_pele e as 3 linhas de alturas. Ou seja, a linha √∫nica de cores_de_pele √© combinada com cada uma das 3 linhas de alturas. Como resultado, temos as 3 linhas de ID 105 na tabela dados:\n\ndados\n\n# A tibble: 6 √ó 4\n     ID Cor     DataRegistro Altura\n  &lt;dbl&gt; &lt;chr&gt;   &lt;date&gt;        &lt;dbl&gt;\n1   100 Pardo   NA            NA   \n2   102 Branco  2022-01-10     1.58\n3   104 Branco  NA            NA   \n4   105 Amarelo 2022-01-10     1.72\n5   105 Amarelo 2022-06-12     1.74\n6   105 Amarelo 2022-08-24     1.75"
  },
  {
    "objectID": "posts/2023/2023-02-06-python-pckg/en/index.html",
    "href": "posts/2023/2023-02-06-python-pckg/en/index.html",
    "title": "What I learned from developing my first Python package",
    "section": "",
    "text": "In 2022, I developed my first Python package called spark_map, and published it at PyPI. If you want to know more about this package üòâ, you can check a previous post here, where I introduced the package, and described it‚Äôs main features, and showed a small use case.\nNow, although spark_map is a small Python package, I had a hard time developing it. More specifically, the Python code was not hard at all to develop. But packaging it into a proper package üì¶ was hard. In this post, I want to share a few things that I learned about Python package development in this process.\nI hope these tips help you in some way üòâ. In summary, the main tips discussed here are:\n\nDo not change the sys.path or PYTHONPATH variable inside your package ‚ö†Ô∏è;\nDifferences between a package and a module;\nNailing the structure üèõÔ∏è of your package can be one the hardest parts;\nIf you use setuptools to build your package, avoid setup.py and use pyproject.toml instead;\nCheck if your Python modules are included on the built versions of your package;"
  },
  {
    "objectID": "posts/2023/2023-02-06-python-pckg/en/index.html#the-search-path-of-python",
    "href": "posts/2023/2023-02-06-python-pckg/en/index.html#the-search-path-of-python",
    "title": "What I learned from developing my first Python package",
    "section": "3.1 The search path of Python",
    "text": "3.1 The search path of Python\nWe usually import a package in Python, by including a import statement at the beginning of our script. For example, to import the pandas package to my Python session, I could do this:\n\nimport pandas\n\nWhen you import a package in Python, the Python interpreter starts a search process üîç through your computer, to find the specific package you called in your script. Every Python package you use must be installed in your machine. Otherwise, Python will never find it, and, as a consequence, you can not use it.\nThe Python interpreter will always look for the packages you import, inside a set of pre-defined locations of your computer. This pre-defined list is stored inside the sys.path variable (Foundation 2023c). In other words, when you import a package, Python looks for this package inside each one of the directories listed at sys.path variable.\n\nimport sys\nprint(sys.path)\n\n['/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/pedro-dev/.local/lib/python3.10/site-packages', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages']\n\n\nYou might also find contents about the PYTHONPATH variable when searching for this subject on the internet. In essence, PYTHONPATH is a environment variable that can contain a complementary list of directories to be added to sys.path (Foundation 2023a).\nAs you can imagine, the Python interpreter look into these directories in a sequential manner. That is, Python looks for the package at the first folder. If it does not find the package you called, then, it looks at the second folder. If it does not find the package again, it looks at the third folder. And goes on and on, until it hits the last folder of the list.\nIf does not find the package you called at this last folder, Python will automatically raise a ModuleNotFoundError error. As you expect, this error means that Python could not find the package you called at any of the directories listed at sys.path."
  },
  {
    "objectID": "posts/2023/2023-02-06-python-pckg/en/index.html#do-not-change-the-sys.path-or-pythonpath-variable",
    "href": "posts/2023/2023-02-06-python-pckg/en/index.html#do-not-change-the-sys.path-or-pythonpath-variable",
    "title": "What I learned from developing my first Python package",
    "section": "3.2 Do not change the sys.path or PYTHONPATH variable ‚ö†Ô∏è",
    "text": "3.2 Do not change the sys.path or PYTHONPATH variable ‚ö†Ô∏è\nThe sys.path variable is a standard Python list, and, as any other list, can be altered to include other directories that are not currently there. The same goes for the PYTHONPATH variable, which is an environment variable, and can be altered too.\nAs an example, when you try to import your package, which is stored at folder A, and, you face a ModuleNotFoundError error, you might be tempted to alter PYTHONPATH or sys.path, to add the folder A to this search path of Python. DO NOT DO IT! YOU SHOULD NEVER alter PYTHONPATH or sys.path ‚ö†Ô∏è ! I mean, at least not inside a Python package.\nIn other words, if at some point inside the source code of your Python package, you execute a code like this:\n\nimport sys\nsys.path.append('./../weird-unknow-folder')\n\njust erase this code! Python packages are made to be used by other peoples, and with a code like this above, you will change the search path of this user. Changing the search path of a user is a bad idea. Because you can accidentaly produce bad and confusing side effects to the user‚Äôs session, which can be hard to debug and solve.\nBesides, in the majority of times when you alter the sys.path, you are trying to overcome a bad structure of your files. In other words, you can always avoid altering the sys.path variable by changing the structure of your source files inside your project.\nJust to be clear, is a bad idea to alter the sys.path inside the source code of your package. However, it is ok to alter these variables outside of your package.\nAs a practical example, Apache Spark is written in Scala. But it have a Python API available through the pyspark package. If you look closely into the source code of the project, or, more specifically at the run-tests.py file, you can see that new paths (or new directories) are appended to sys.path in this file.\nHowever, this run-tests.py file IS NOT A PART of the pyspark package itself. It is just an auxiliary script (outside of the package) used to support the testing processes of pyspark. This means that run-tests.py contains code that is not intended to be executed by the users of the package, but by the developers of pyspark instead."
  },
  {
    "objectID": "posts/2023/2023-02-06-python-pckg/en/index.html#differences-between-a-package-and-a-module",
    "href": "posts/2023/2023-02-06-python-pckg/en/index.html#differences-between-a-package-and-a-module",
    "title": "What I learned from developing my first Python package",
    "section": "3.3 Differences between a package and a module",
    "text": "3.3 Differences between a package and a module\nThis is a very basic knowledge for a Python developer. However, until recently, I did not know the meaning of these two concepts. So, I will give it to you now, in case you do not have it yet.\nA Python module is a single Python file (i.e.¬†a file with extension .py). Every Python script you write, is a Python module. In contrast, a Python package is a set of Python modules gathered together inside a folder. This folder must contain a particular Python module named as __init__.py. This __init__.py is the file that ‚Äúinitialize‚Äù, or, ‚Äúidentifies‚Äù this folder as a Python package (Foundation 2023b).\nYou can have multiple Python packages inside a Python package. That is, inside the directory of your package, you can have multiple sub-directories with more Python modules and __init__.py files. In this case, these sub-directories become submodules of the package. You can interpret them as sub-packages, if you prefer."
  },
  {
    "objectID": "posts/2023/2023-02-06-python-pckg/en/index.html#structuring-the-package-was-one-of-the-hardest-parts",
    "href": "posts/2023/2023-02-06-python-pckg/en/index.html#structuring-the-package-was-one-of-the-hardest-parts",
    "title": "What I learned from developing my first Python package",
    "section": "3.4 Structuring the package was one of the hardest parts",
    "text": "3.4 Structuring the package was one of the hardest parts\nEvery Python package follows the same basic file/directory structure üèõÔ∏è. In other words, the files that compose a Python package are always structured in a standard way. But, understanding and using this structure effectively was one of the hardest parts for me. At this section, I want to explain this structure for you.\nIn a Python package project, you usually have these files:\n\nLICENSE.md or LICENSE.rst (or both): a text file with the license of your package. It can be a Markdown file (.md), or, a reStructuredText markup file (.rst);\nREADME.md: a Markdown file introducing your package. That is, a file that describes succintly the objective of the package, its main features, and showing a small example of use of the package;\nsetup.py or pyproject.toml or setup.cfg: these are files used by the build system you choose to build (or compile) your Python package into a compact and shareable format;\n\nAlso, a Python package project usually contains these folders (or directories):\n\nsrc/&lt;package-name&gt;/ or &lt;package-name&gt;/: inside this directory you store all Python modules of your package, that is, the source code of your package;\ntests/: inside this directory you store all unit tests of your package. In other words, the scripts and automated workflow used to test your package.\n\n\nYou must store the source code (or the python modules) of your package inside a folder with the same name as the package itself (i.e.¬†the &lt;package-name&gt;/ folder). So, for a package named spark_map we should keep the source files (i.e.¬†the .py files) of this package inside a folder called spark_map. As a practical example, if you look at the source code of the famous pandas package, you will see that all source code of the package is stored inside a folder called pandas.\nIn contrast, this &lt;package-name&gt;/ folder might be (or might be not) inside another folder called src/, that is, the path to the source code might be src/&lt;package-name&gt;/ instead of &lt;package-name&gt;/. The pandas package for example, do not uses the src/ folder, so the source code is stored inside the pandas/ folder. In contrast, the famous flask package uses the flask/ folder inside a src/ folder, so the path to the source code becomes src/flask. You can check this by looking at the source code of the package..\nSo the folder structure to store the source code of the package might change very slightly from package to package. There is no consensus about which one of these two structures is the best. But in general, the source is always stored inside a folder with the same name as the package (i.e.¬†the &lt;package-name&gt;/ folder). And this folder might be stored inside a src/ folder.\nFurthermore, every project of a Python package usually have one, two, or more files that control the build process of the package (like setup.py, pyproject.toml or setup.cfg). In other words, these files are not part of the package itself. But they are used by the build system to build (or compile) your package into a compact and shareable format. I talk more about these files at Section¬†3.5.\nHaving in mind all these files that we described until here, we can build a example of file structure for a package. For example, a possible file strucuture for a package named haven could be:\n.\n‚îú‚îÄ‚îÄ LICENSE.md\n‚îú‚îÄ‚îÄ LICENSE.rst\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ src\n‚îÇ   ‚îî‚îÄ‚îÄ haven\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ       ‚îú‚îÄ‚îÄ functions.py\n‚îÇ       ‚îú‚îÄ‚îÄ utils.py \n‚îÇ       ‚îî‚îÄ‚îÄ haven.py\n‚îÇ\n‚îî‚îÄ‚îÄ tests\n    ‚îú‚îÄ‚îÄ test_functions.py\n    ‚îî‚îÄ‚îÄ test_haven.py"
  },
  {
    "objectID": "posts/2023/2023-02-06-python-pckg/en/index.html#sec-build-systems",
    "href": "posts/2023/2023-02-06-python-pckg/en/index.html#sec-build-systems",
    "title": "What I learned from developing my first Python package",
    "section": "3.5 Introducing build systems",
    "text": "3.5 Introducing build systems\nWhen you are developing a package, you write multiple Python modules that perform all necessary tasks to solve the specific problem you want to solve with your package. However, in order to distribute this package to other peoples, that is, to make it available to the wide public, you need to compile (or build) your package into a compact and shareable format.\nThis is what a build system does. It compiles all archives of your package into a single and compact file that you can publish at PyPI, and distribute to other peoples. This means that, when a user downloads your package (through pip for example), it downloads this compact and shareable version of your package.\nOn the day of writing this article, there are four main build systems available on the market for Python packages, which are hatchling, setuptools, flit and PDM. No matter which one you use, just choose one, anyone. Because they do the same thing, and work in a very similar way.\nMost of them use various metadata stored at the pyproject.toml file to build your project. The setuptools build system is probably a exception to this rule, because this build system supports other kinds of files to store this metadata, which are setup.py and setup.cfg.\n\n3.5.1 The pyproject.toml file as the commom ground\nThe PEP517 and PEP518 were lauched to stablish the pyproject.toml file as the commom ground to every build system. Basically, the pyproject.toml is a file included at the root level of your project, and contains metadata about your package (e.g.¬†it‚Äôs name, version, license, description, author, dependencies, etc.).\nIt represents a ‚Äúcommom ground‚Äù because all build systems available can read and use this file to build (or compile) your package. As a consequence, with this file, you can change very easily the build system you use in your package, without having to change various configurantions and files. All you have to do is to change the options under the build-system table.\nThis build-system table is the part of pyproject.toml file where you can specify options and configurations for the build system. As an example, to use the setuptools build system, you would add these lines to pyproject.toml:\n[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\nIn the other hand, if you want to use the hatchling build system instead, you would change the above lines to:\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\nOr, maybe you prefer flit:\n[build-system]\nrequires = [\"flit_core\"]\nbuild-backend = \"flit_core.buildapi\"\nAnyway, you get it. You use the options requires and build-backend under the build-system table to specificy which system you want to use in the building process of your package. All of these different systems will use pyproject.toml to collect other very important metadata about your package, like these metadata below:\n[project]\nname = \"spark_map\"\nversion = \"0.2.77\"\nauthors = [\n  { name=\"Pedro Faria\", email=\"pedropark99@gmail.com\" }\n]\ndescription = \"Pyspark implementation of `map()` function for spark DataFrames\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.7\"\nlicense = { file = \"LICENSE.txt\" }\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n]\n\ndependencies = [\n    \"pyspark\",\n    \"setuptools\",\n    \"toml\"\n]\n\n\n3.5.2 The setuptools build system\nThe setuptools build system is the system that I use to build spark_map. It is kind of a more ‚Äúversatile‚Äù2 system than the others, only because it supports other kinds of files besides pyproject.toml.\nThe setup.py file is probably the most ‚Äúfamous‚Äù file, or, the most associable to setuptools. Although this is changing in recent years (more about this at the next section), the setup.py is still considered the traditional way of using the system, as the documentation itself says:\n\nThe traditional setuptools way of packaging Python modules uses a setup() function within the setup.py script. (Authority 2023)\n\nIn summary, you can replace pyproject.toml with the setup.py file. Just as an example, I could replace the pyproject.toml file for spark_map with a setup.py file similar to the file below. The whole setup.py file is usually composed of just a single call to the setuptools.setup() function, and nothing more. All of the various metadata about the package stored at pyproject.toml are translated to named arguments to this setup() function.\n\n# setup.py placed at root directory\nfrom setuptools import setup\nsetup(\n    name = 'spark_map'\n    version = '0.2.77',\n    author = 'Pedro Faria',\n    description = 'Pyspark implementation of `map()` function for spark DataFrames',\n    long_description = 'README.md',\n    license = { file = \"LICENSE.txt\" }\n    classifiers = [\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires = '&gt;=3.7',\n    install_requires = ['pyspark'],\n    extras_require = ['setuptools', 'toml']\n)\n\nHowever, if you use a setup.py file you become kind of locked into the setuptools framework. This one the reasons why many developers prefer to use the pyproject.toml file instead. Because it becomes so much more easy to change between build systems if you want to.\nAnother ‚Äúclash point‚Äù about the setup.py file is that it changes the way, or the steps (or the worflow) to build the package. The standard or the most supported strategy to build a package, is to use the build module command of Python at the terminal. This command will build your package, no matter which build system you use:\n# If you are on Windows:\npy -m build\n# If you are on Linux:\npython3 -m build\nIn contrast, when you use the setup.py file, you have to execute directly the script with some additional options, like sdist:\n# If you are on Windows:\npy setup.py sdist\n# If you are on Linux:\npython3 setup.py sdist\nAgain, this is bad, because it forces you into a certain framework, or, a certain workflow that is different than the standard way of building a package.\n\n\n3.5.3 Avoid setup.py and use pyproject.toml instead\nThe Python community understood this problem, and now, executing the setup.py script directly is heavily discouraged (Ganssle 2021). Actually, the setuptools project itself started a movement to migrate it‚Äôs functionality to pyproject.toml and setup.cfg files.\nThis concern is clearly documented at the ‚ÄúQuick Start‚Äù section of the official documentation for the project:\n\nSetuptools offers first class support for setup.py files as a configuration mechanism ‚Ä¶ It is important to remember, however, that running this file as a script (e.g.¬†python setup.py sdist) is strongly discouraged, and that the majority of the command line interfaces are (or will be) deprecated (e.g.¬†python setup.py install, python setup.py bdist_wininst, ‚Ä¶) ‚Ä¶ We also recommend users to expose as much as possible configuration in a more declarative way via the pyproject.toml or setup.cfg, and keep the setup.py minimal with only the dynamic parts (or even omit it completely if applicable).\n\nIn resume, avoid using setup.py in your project, and use pyproject.toml instead with the python -m build command. This a much more recommended workflow for building your package."
  },
  {
    "objectID": "posts/2023/2023-02-06-python-pckg/en/index.html#sec-weird-problem",
    "href": "posts/2023/2023-02-06-python-pckg/en/index.html#sec-weird-problem",
    "title": "What I learned from developing my first Python package",
    "section": "4.1 How did this problem occur",
    "text": "4.1 How did this problem occur\nThere was no error or warning messages at the building stage of the package, neither in the installation process as well. But something was definitely wrong with the package.\nIn essence, the source of this weird and confusing problem was at the building stage of the package. Basically, the build system (in that case, the setuptools) was not able to automatically find all source files (i.e.¬†the .py files) of the package. As a result, the build system was building the package, but, not including the source files inside this built version of the package. Very weird! Isn‚Äôt it?\nI mean, if the build system did not find any source files‚Ä¶ it should definitely raise a warning or error during the build process. Because this is definitely an error in the process. How or why can you share a Python package, which have no Python source code? If the package does not contain any Python code in it, it is probably not a Python package. Right?\nThat is why I could not import the package after I installed it on my machine, trough pip. Because the version of the package that was being installed on my machine, was a version of the package that had no Python source code that could be imported. That is why the ModuleNotFoundError error was being raised by the Python interpreter when I tried to import the package."
  },
  {
    "objectID": "posts/2023/2023-02-06-python-pckg/en/index.html#how-to-identify-this-problem",
    "href": "posts/2023/2023-02-06-python-pckg/en/index.html#how-to-identify-this-problem",
    "title": "What I learned from developing my first Python package",
    "section": "4.2 How to identify this problem",
    "text": "4.2 How to identify this problem\nFirst, when you build your package, usually two main files are created, which are: 1) the wheel distribution of your package; 2) the source distribution of your package. These are just two different formats in which you can ship and share your package with another human.\nThe source distribution is a TAR file (with .tar or .tar.gz extension). Is just a compressed file (like ZIP files) with all of the essential files that compose your package (i.e.¬†the metadata of the package, the source files - .py files, README and LICENSE files, etc.).\nOn the other hand, the wheel distribution (files with .whl extension), is ‚Äúpre-compiled‚Äù version of your package. This means that a wheel version of your package comes in a ready-to-install format, and, as a consequence, is much faster to install this version of your package, in comparison to source distributions, which must be compiled before being installed on your system.\nTo identify if the problem I described at Section¬†4.1 is happening inside your package project, I recommend you to focus your attention on the source distribution of your package (i.e.¬†the TAR file - .tar or .tar.gz). Because you can easily open this kind of file with decompressing tools, like WinRar or 7-zip, and it is much more easy to analyze than the wheel distribution.\nIn resume, if you open the TAR file, and, you do not find any Python modules (i.e.¬†.py files) in it, this means that the problem I described at Section¬†4.1 is unfortunately happening inside your package. In contrast, if you do find all Python modules of your package inside this TAR file, than, you are safe and good to go forward.\nTo fix this problem inside my package project, I had to add the lines below to my pyproject.toml file. These lines tell setuptools which are the packages available inside the project, and, which is the folder inside the project where all the source code of the package was stored.\n[tool.setuptools]\npackages = [\"spark_map\"]\npackage-dir = {\"\" = \"src\"}\nIt was so hard to find this bug, yet, it was quite simple and straightforward to fix it üòë (poker face). With these options, setuptools could finally find all the Python modules of my package, and, as a consequence, include them in the built versions of the package (in the source and in the wheel distributions).\nTherefore, if you face a ModuleNotFoundError error when you try to import your package, after you installed it in your computer with pip, I recommend you to check if your Python modules are being included on the built versions of your package."
  },
  {
    "objectID": "posts/2023/2023-02-06-python-pckg/en/index.html#footnotes",
    "href": "posts/2023/2023-02-06-python-pckg/en/index.html#footnotes",
    "title": "What I learned from developing my first Python package",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProbably a bad name for a team. But, that is just my opinion.‚Ü©Ô∏é\nAnd I mean these quotation marks, because I am not sure to call a system which can be out of the standard as ‚Äúbeing more versatile‚Äù, or not.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023/2023-07-31-git-mark-decisions/pt/index.html",
    "href": "posts/2023/2023-07-31-git-mark-decisions/pt/index.html",
    "title": "Melhorando a tomada de decis√£o com Git e porqu√™ voc√™ deveria se importar",
    "section": "",
    "text": "Na semana passada, eu e minha equipe nos deparamos com uma situa√ß√£o em que precis√°vamos descobrir por que e como ocorreu um determinado problema. Precis√°vamos entender quais decis√µes levaram a esse problema. Algo bem normal, n√£o √©?\nMas entender o que aconteceu s√≥ foi poss√≠vel para n√≥s porque rastreamos cada mudan√ßa e cada decis√£o que tomamos com o Git, assinando commits e escrevendo Pull Requests (ou PRs para abreviar). Este artigo usa essa situa√ß√£o do mundo real que enfrentamos para mostrar como o Git e os processos formais para registrar altera√ß√µes na base de c√≥digo (como PRs) s√£o uma parte cr√≠tica para melhorar a tomada de decis√µes e entender como suas decis√µes passadas est√£o afetando voc√™ no presente.\nCome√ßo o artigo descrevendo qual foi o problema que n√≥s enfrentamos e, na sequ√™ncia, explico quais erros foram cometidos e como o Git nos ajudou a identificar esses erros."
  },
  {
    "objectID": "posts/2023/2023-07-31-git-mark-decisions/pt/index.html#footnotes",
    "href": "posts/2023/2023-07-31-git-mark-decisions/pt/index.html#footnotes",
    "title": "Melhorando a tomada de decis√£o com Git e porqu√™ voc√™ deveria se importar",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nO que √© um pipeline? Um pipeline √© apenas uma sequ√™ncia de etapas (ou tarefas) a serem realizada em uma hora espec√≠fica do dia (ou em um dia espec√≠fico da semana ou do m√™s, etc.). E um pipeline de dados √© um pipeline que cont√©m tarefas que carregam, transformam, enviam ou ingerem dados de alguma forma.‚Ü©Ô∏é\nEm resumo, um Pull Request (ou PR) √© uma proposta para realizar uma opera√ß√£o git merge. Em outras palavras, voc√™ cria um PR quando deseja mesclar as altera√ß√µes feitas em uma branch em outra branch (na maioria das vezes a main branch). O aspecto de ‚Äúproposta‚Äù de uma PR significa que este PR precisa ser aprovado para ser efetivamente executado. Um PR n√£o √© um recurso do Git. Na verdade, √© um recurso padr√£o da maioria dos provedores de servi√ßos Git. Ent√£o voc√™ cria um PR dentro de uma plataforma de servi√ßos Git como GitHub, GitLab e Azure DevOps, e n√£o dentro do pr√≥prio Git. Se voc√™ n√£o est√° familiarizado com PRs, a documenta√ß√£o do GitHub tem um excelente artigo sobre isso.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023/2023-08-31-data-decisions/pt/index.html",
    "href": "posts/2023/2023-08-31-data-decisions/pt/index.html",
    "title": "Um pequeno exemplo sobre como utilizar dados para tomar melhores decis√µes",
    "section": "",
    "text": "Este artigo descreve um exemplo real em que eu e meu time usamos dados para convencer o nosso cliente na √©poca (que √© um dos maiores bancos do mercado brasileiro) a tomar uma melhor decis√£o. Agradecimentos especiais ao Guilherme Goes e ao Paulo Gon√ßalves. Ambos me ajudaram a montar e apresentar essas ideias e insigths para o nosso cliente.\nEm resumo, tomar decis√µes √© dif√≠cil. Mas voc√™ sempre toma uma decis√£o melhor quando voc√™ tem dados para gui√°-lo em dire√ß√£o a um resultado melhor e mais seguro. Quando voc√™ n√£o tem dados para te defender, voc√™ est√° basicamente no escuro. Ou seja, voc√™ toma a decis√£o, mas n√£o sabe antecipadamente quais s√£o os poss√≠veis resultados dessa decis√£o. Voc√™ apenas espera o melhor, e esta √© sempre uma posi√ß√£o dif√≠cil de se estar.\n\n\n\n\n\n\nImportant\n\n\n\nTodos os dados, gr√°ficos e imagens mostrados neste artigo s√£o meramente ilustrativos. Portanto, eles n√£o representam os dados reais de Blip ou do banco envolvido de nenhuma forma ou dimens√£o!"
  },
  {
    "objectID": "posts/2023/2023-08-31-data-decisions/pt/index.html#o-produto-de-empr√©stimo-com-garantia",
    "href": "posts/2023/2023-08-31-data-decisions/pt/index.html#o-produto-de-empr√©stimo-com-garantia",
    "title": "Um pequeno exemplo sobre como utilizar dados para tomar melhores decis√µes",
    "section": "3.1 O produto de empr√©stimo com garantia",
    "text": "3.1 O produto de empr√©stimo com garantia\nO empr√©stimo com garantia era um dos diversos produtos oferecidos pelo banco no WhatsApp. Para adquirir este produto, o usu√°rio precisava responder diversas perguntas, e tamb√©m, se enquadrar em alguns crit√©rios.\nDependendo do tipo de setor em que voc√™ atua, voc√™ pode chamar isso de ‚Äúfluxo de vendas‚Äù, ou ‚Äúcaminho de vendas‚Äù, que nada mais √© do que o caminho (ou as etapas) que o usu√°rio precisa seguir para adquirir o produto que voc√™ est√° vendendo.\nA maioria das empresas deseja tornar esse caminho o mais curto poss√≠vel para que o usu√°rio chegue ao produto com mais rapidez. Por√©m, estamos falando de um empr√©stimo, ent√£o o banco certamente precisa de muitas informa√ß√µes pessoais e financeiras do usu√°rio antes de conceder o empr√©stimo.\nOu seja, neste exemplo, o usu√°rio precisava responder uma quantidade consider√°vel de perguntas pelo WhatsApp para chegar √† etapa final do caminho de vendas para adquirir o empr√©stimo.\nA maioria dessas perguntas solicitava algumas informa√ß√µes pessoais, para verificar se esse usu√°rio espec√≠fico se enquadrava ou n√£o em alguns crit√©rios importantes. A maioria desses crit√©rios eram bastante comuns (ou o standard) para qualquer tipo de empr√©stimo. Por exemplo, a pessoa n√£o deveria ter nenhuma d√≠vida legal com o governo. Alguns outros crit√©rios eram puramente financeiros e patrimoniais, e tamb√©m eram uma pr√°tica muito comum entre os bancos, como‚Ä¶ a pessoa precisa estar totalmente empregada, precisa ter um carro totalmente pago, e, esse carro precisa ser uma propriedade pessoal da pessoa, ou seja, n√£o poderia ser um carro emprestado de outra pessoa."
  },
  {
    "objectID": "posts/2023/2023-08-31-data-decisions/pt/index.html#um-crit√©rio-estranho",
    "href": "posts/2023/2023-08-31-data-decisions/pt/index.html#um-crit√©rio-estranho",
    "title": "Um pequeno exemplo sobre como utilizar dados para tomar melhores decis√µes",
    "section": "3.2 Um crit√©rio estranho",
    "text": "3.2 Um crit√©rio estranho\nPor√©m um dos v√°rios crit√©rios era um pouco estranho para n√≥s. Identificar exatamente qual crit√©rio era esse n√£o √© importante para o conte√∫do deste artigo. Portanto, vamos dizer apenas que, para que um usu√°rio consiga adquirir esse empr√©stimo, ele precisava necessariamente ser eleg√≠vel para tr√™s modalidades diferentes.\nCada modalidade correspondia a um tipo diferente de empr√©stimo. Se o usu√°rio n√£o fosse eleg√≠vel a todas essas tr√™s modalidades (ou tipos de empr√©stimo), n√≥s automaticamente rejeitamos a solicita√ß√£o de empr√©stimo desse usu√°rio.\nEm ess√™ncia, t√≠nhamos um fluxo que funcionava mais ou menos assim:\n\n\n\nUma pequena representa√ß√£o do fluxo de vendas\n\n\nCada vez que um usu√°rio entrava em nosso fluxo, n√≥s coletamos o CPF desse usu√°rio. Pois com esse CPF podemos usar a API para verificar m√∫ltiplas informa√ß√µes sobre essa pessoa de uma vez s√≥. Uma das muitas coisas que n√≥s verific√°vamos nessa chamada de API, era se esse usu√°rio se encaixava no crit√©rio estranho que descrevemos. Isto √©, n√≥s confer√≠amos se esse usu√°rio era eleg√≠vel ou n√£o √†s tr√™s modalidades de empr√©stimo."
  },
  {
    "objectID": "posts/2023/2023-08-31-data-decisions/pt/index.html#por-que-esse-crit√©rio-era-estranho",
    "href": "posts/2023/2023-08-31-data-decisions/pt/index.html#por-que-esse-crit√©rio-era-estranho",
    "title": "Um pequeno exemplo sobre como utilizar dados para tomar melhores decis√µes",
    "section": "3.3 Por que esse crit√©rio era estranho?",
    "text": "3.3 Por que esse crit√©rio era estranho?\nNa nossa cabe√ßa, este era um crit√©rio muito estranho, porque‚Ä¶ se uma usu√°ria chamada ‚ÄúAna‚Äù por exemplo for eleg√≠vel para a modalidade A, ent√£o, por que n√£o oferecer um empr√©stimo da modalidade A para a ‚ÄúAna‚Äù? Como um outro exemplo, se um o usu√°rio ‚ÄúMike‚Äù √© eleg√≠vel para ambas as modalidades A e B, ent√£o por que n√£o oferecer ambas as modalidades (A e B) para ele? Ou seja, por que n√£o oferecer ao usu√°rio qualquer tipo de empr√©stimo ao qual ele √© eleg√≠vel?\nPor que apenas usu√°rios que s√£o eleg√≠veis para todas as tr√™s modalidades (A, B e C) podem adquirir o empr√©stimo? Na nossa cabe√ßa esse crit√©rio n√£o fazia muito sentido, pois se um usu√°rio for eleg√≠vel a uma modalidade de empr√©stimo, ele deveria ser capaz de obter um empr√©stimo nesta modalidade ao qual ele tem direito."
  },
  {
    "objectID": "posts/2023/2023-08-31-data-decisions/pt/index.html#por-que-esse-crit√©rio-existia",
    "href": "posts/2023/2023-08-31-data-decisions/pt/index.html#por-que-esse-crit√©rio-existia",
    "title": "Um pequeno exemplo sobre como utilizar dados para tomar melhores decis√µes",
    "section": "3.4 Por que esse crit√©rio existia?",
    "text": "3.4 Por que esse crit√©rio existia?\nMas vamos encara um fato. Apesar de este ser um crit√©rio estranho, certamente existe uma raz√£o para ele existir. Nada existe sem uma raz√£o. N√≥s imaginamos que esse crit√©rio estranho existia provavelmente por causa de:\n\numa estrat√©gia MVP.\nou de uma estrat√©gia de risco, em outras palavras, por causa de um trade-off.\n\nUma estrat√©gia MVP significa que o banco decidiu incluir esse crit√©rio estranho porque inclu√≠-lo reduziria muito a complexidade do desenvolvimento do fluxo de vendas, e, portanto, ao simplificar o desenvolvimento, o banco poderia entregar um MVP (minimal viable product, ou, produto m√≠nimo vi√°vel) o mais r√°pido poss√≠vel e, como consequ√™ncia, ele poderia come√ßar a lucrar com esse produto mais rapidamente.\nPor outro lado, uma estrat√©gia de risco significa que o banco decidiu incluir este crit√©rio estranho, porque ele provavelmente estimou que o risco √© consideravelmente maior para pessoas que n√£o s√£o eleg√≠veis para as tr√™s modalidades. Se o risco estimado for muito alto, ent√£o o banco tem um bom motivo para n√£o oferecer esse empr√©stimo a pessoas que s√£o eleg√≠veis a apenas uma ou duas modalidades diferentes.\nOs bancos enfrentam constantemente uma escolha (ou uma balan√ßa) entre risco e lucro. Ou seja, um empr√©stimo de qualquer tipo √© sempre uma boa oportunidade de lucro para o banco. No entanto, esta oportunidade de lucro tem sempre um custo atrelado a ela. Esse custo aparece principalmente na forma de risco para o banco.\n√â por isso que os bancos s√£o normalmente muito bons em analisar e estimar riscos. Quando uma pessoa busca adquirir um empr√©stimo, o banco passa a analisar diversos fatores para estimar o quanto arriscado √© conceder um empr√©stimo a essa pessoa."
  },
  {
    "objectID": "posts/2023/2023-12-12-zig-strings/en/index.html",
    "href": "posts/2023/2023-12-12-zig-strings/en/index.html",
    "title": "How strings work in Zig?",
    "section": "",
    "text": "Zig is a new general-purpose and low level programming language that is being very promising. The documentation of the language1 is very good in quality, but I missed some more specific details about strings in there.\nThat is why, I decided to write this article, to discuss in more depth how strings work in Zig, and give you some more specific details about it."
  },
  {
    "objectID": "posts/2023/2023-12-12-zig-strings/en/index.html#footnotes",
    "href": "posts/2023/2023-12-12-zig-strings/en/index.html#footnotes",
    "title": "How strings work in Zig?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://ziglang.org/documentation/0.11.0/.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023/2023-03-06-storytelling/en/index.html",
    "href": "posts/2023/2023-03-06-storytelling/en/index.html",
    "title": "A case study about Data Storytelling at some of the largest brazilian banks",
    "section": "",
    "text": "This article discusses how I improved my Data Storytelling process within my presentations and reports delivered to two of the largest commercial banks in the Brazilian market. I try to share what I learned in this process, and I give some tips that will possibly help you to build more intuitive, captivating, clear and effective data reports.\nIn summary, we will discuss in this article the following tips:\n\nData Storytelling is about telling stories through data;\nStories have structure, use these structures in your favor;\nAvoid sharing too much attention with long texts on your slides;\nTrace relationships between your data;\nBuild your story around a main message;\nDeliver your story at small pieces;\n\nMost of this knowledge was built through intense research, reflection and planning about my presentations, and later, getting feedback, and making small adjustments here and there.\nAs you might expect with every great work like this, it also involved other peoples. During this process, I received help and feedback from my co-workers. Specially from Andressa de Souza Freitas and Guilherme Gomes. I also had great help from the UX Designer Al√™ Fernandes. Most of the knowledge presented here, I learned in practice with Al√™. This knowledge revolutionized the way I build my presentations, and for that I am immensely grateful to her ‚ù§Ô∏è.\n\n\n\n\n\n\nImportant\n\n\n\nAll data, charts and images shown in this article are merely illustrative. All numbers shown were randomly generated by a computer! Therefore, they do not represent the actual data of TakeBlip or the comercial banks involved in any way or dimension!"
  },
  {
    "objectID": "posts/2023/2023-03-06-storytelling/en/index.html#sec-guerra-atencao",
    "href": "posts/2023/2023-03-06-storytelling/en/index.html#sec-guerra-atencao",
    "title": "A case study about Data Storytelling at some of the largest brazilian banks",
    "section": "4.1 A constant war for attention",
    "text": "4.1 A constant war for attention\nWhen we‚Äôre presenting something, we‚Äôre constantly battling for our viewers attention. This is a hard battle, not only because we can (unintentionally) attract attention to the wrong places, but also, because there are so many sources of distraction in the modern world (e.g.¬†cell phones, emails, etc.)!\nA good data storytelling depends on telling a captivating story that manages to capture the attention of your viewers. Therefore, the next sections will focus a lot on tips that contribute to this capture, or that help you not to dissipate, reduce or disturb the attention of these spectators.\n\n4.1.1 Why you should avoid long text on your slides?\nTry to be parsimonious in your slides! That is, try to include as little information as possible within it. If you need to convey a lot of information on a single slide, then try to incorporate most of this content into your speech, and as little of it as possible in written form on this slide. In general, avoid including very long texts in your slides as much as possible.\nIt‚Äôs weird to think about, but generally, managers will jump in and watch your presentation because they‚Äôre interested in what you have to say about their business. Therefore, your slides are only support material, they must be secondary, a supporting element of your presentation. Because the main piece of the presentation should always be your speech and the story you want to tell through it.\nSee the slide at Figure¬†2 as an example. The main problem with this slide is that it divides the viewer‚Äôs attention a lot.\n\n\n\n\n\n\nFigure¬†2: Slide with a long paragraph\n\n\n\nWhen presenting a slide, your viewers have to pay attention to your speech. That is, what you are verbally communicating during the presentation. And at the same time, they also need to pay attention to the slide content. However, that long paragraph in the left corner of the slide shown in Figure¬†2 is problematic. Because he draws too much attention!\nThe long paragraph element arouses our curiosity so much that, when you saw the above slide, you (the reader of this article) probably tried to read that long paragraph, even before reading what I am describing right now in this paragraph. The same will happen with the viewers of your presentation. Meaning your viewers will immediately try to read that long paragraph.\nHowever, reading and interpreting a long text requires some effort and a lot of attention. As a result, while your viewers are trying to read that text, they won‚Äôt be able to pay attention to other elements of your presentation. For example, in your speech.\nThis can be crucial, as you may bring extra information, or an extremely important connection in your speech, and they may end up missing it while they are trying to read this text. Therefore, avoid as much as possible including very long texts in your slides.\n\n\n4.1.2 Deliver your story in small pieces\nThe human brain can process a limited amount of information at once. As a result, if you try to explain a lot of information to your viewers, in a single slide, they will end up hitting that limit ü§Ø, and they simply won‚Äôt be able to reason, understand or assimilate what you‚Äôre explaining.\nSo deliver your story in small pieces. Avoid condensing a lot of information into a single slide! Divide the content into parts, and explain one part at a time!\nThis helps make the content simpler, and as a result, it helps your viewers better understand what you‚Äôre talking about.\nThink about this for a while. When you‚Äôre looking to learn about a complex subject (e.g.¬†linear regression), you‚Äôre likely to divide the content into many small pieces, and learn one piece at a time. Isn‚Äôt that how you do it? So bring that strategy into your presentations as well."
  },
  {
    "objectID": "posts/2023/2023-03-06-storytelling/en/index.html#sec-historia-conteudo",
    "href": "posts/2023/2023-03-06-storytelling/en/index.html#sec-historia-conteudo",
    "title": "A case study about Data Storytelling at some of the largest brazilian banks",
    "section": "4.2 Discussing stories and content",
    "text": "4.2 Discussing stories and content\nNow that we‚Äôve seen two basic rules about keeping your viewers attention (i.e.¬†avoid long texts and deliver your story at small pieces), let‚Äôs discuss some tips about the content of your presentation, and how to build stories with data.\n\n4.2.1 Data Storytelling is not about choosing ‚Äúthe best chart‚Äù\nA few analysts understand ‚Äúdata storytelling‚Äù as a visualization problem, or, as the science of ‚Äúchoosing the best chart‚Äù for your presentation, or how to design innovative, beautiful, and complex graphic designs.\nHowever, data storytelling is about telling stories with data. Not about building charts/graphics. Choosing the right visualization to present your data, making it better, prettier and cleaner is only part of the process. A very important part, as this will help you tell your story more clearly and effectively, and thus reach a larger audience.\n\n\n4.2.2 Trace relationships between indicators\nNow, let‚Äôs analyze the slide shown in Figure¬†3. Notice that this slide, again, is very reminiscent of a dashboard page. The slide doesn‚Äôt look so captivating at first glance, as it only shows the indicators, it doesn‚Äôt build a relationship, or a story, between them.\n\n\n\n\n\n\nFigure¬†3: Lack of relationship between indicators\n\n\n\nIf we pay attention to these indicators, we can identify some effects that are happening along them. And if we think about these effects a little more, we will realize that these effects are relatable! And that together, they can tell a story.\nFor example, notice that there is a significant increase in sales. Both in the number of proposals sold and in the total value that these proposals generated. However, notice that this growth in sales did not occur in the ‚ÄúPayroll Loan‚Äù product, but in the ‚ÄúVehicle secured loan‚Äù product. In other words, the product ‚ÄúLoan Payroll‚Äù had a drop in sales at that month, however, the product ‚ÄúLoan with vehicle guarantee‚Äù obtained a big result that surpassed this drop by a lot, and in the end, managed to increase the bank sales as a whole.\nIn addition, we can also see other effects like the increases in both conversion rate and in the API (Application Programming Interface) success rate. These are also factors that contributed to the increase in sales. Because an increase in the conversion rate means that a larger portion of our customers are purchasing our products. And an increase in the success rate in the API means that we have fewer errors in the registration of sales on the platform, and this is obviously positive, as we have a smaller loss of sales due to crashes and errors in this registration system.\nNote that all these relationships together help us to build a story about how sales increased at this specific month, and that‚Äôs exactly what we want to achieve. So always try to build relationships between your metrics to form a story about a key result that you perceived.\n\n\n4.2.3 Build your story around a key message\nJust to make this idea clear, when we identify the various effects that we described at Section¬†4.2.2, on the slide shown in Figure¬†3, it is interesting to ask ourselves: which of these various effects is the main result? In other words, which one of these effects is of most interest to the bank managers who are watching your presentation?\nCertainly the increase in sales is the main effect. It is the key result. It‚Äôs the effect that most interests managers watching your presentation. So try to build your story around this result, or this main message. Use the other indicators to explain how this key result came about.\nThis is also very important! Every manager is very fond of hearing the word ‚Äúincrease in sales‚Äù. However, he is also always interested in knowing the ‚Äúhow was this increase generated?‚Äù. That is, he needs to know what actions were taken that generated this positive impact.\nBecause by identifying these actions, this manager has the ability to apply these actions to other parts of his business, and, hopefully, he can end up spreading this positive effect that you described to other areas, and, as a result, he can end up further increasing the company‚Äôs sales."
  },
  {
    "objectID": "posts/2023/2023-03-06-storytelling/en/index.html#the-four-cs-model",
    "href": "posts/2023/2023-03-06-storytelling/en/index.html#the-four-cs-model",
    "title": "A case study about Data Storytelling at some of the largest brazilian banks",
    "section": "6.1 The four C‚Äôs model",
    "text": "6.1 The four C‚Äôs model\nThe four C‚Äôs model refers to these four words in Portuguese:\n\nContexto, Conflito, Consequ√™ncia, Conselho.\n\nThese four words can be translated into Context, Conflict, Consequence, Advice, and they help you bring a consistent structure to your story. In addition, these words also help you to stir up your viewers‚Äô emotions a little, and, with that, capture their attention more.\nBy following this model, your story will always break down into four parts: context, conflict, consequence, and advice. Precisely in this order. This framework revolves around a major problem or conflict that you have identified in your client‚Äôs business.\nThe interesting thing about this structure is that you can stake several problems in sequence. As a result, you have a block of 4 C‚Äôs (context, conflict, consequence and advice), followed by another block with 4 more C‚Äôs (context, conflict, consequence and advice). Or, you can start the presentation with a context, and then two blocks of 3 C‚Äôs (conflict, consequence and advice) in sequence.\nAnyway, enough talking, and let‚Äôs describe in more detail each one of these four parts of this structure.\n\n6.1.1 Start with a context\nTherefore, when following this model, your story will always start with a context. Something just to contextualize the viewer on what the current state of the business is.\nRemember, the 4 C‚Äôs model is built around a conflict, or a core problem. Bearing this in mind, if, for example, the conflict that you are going to discuss in your story is a problem that affects the service for selling card machines, it is important that you focus on giving an overview about the service for selling card machines.\nIn other words, avoid bringing contexts in this part that are not related to the problem/conflict that you will talk in the next section. Because this conflict is the central part of the story.\n\n\n6.1.2 Present a conflict/problem/challenge to be overcome\nThen you must show a conflict. That is, a problem, challenge or barrier that you have identified in the product/service you are looking at. This is where we‚Äôre going to play with the viewer‚Äôs emotions a bit, and use that to our advantage to capture their attention.\nLet‚Äôs reflect on this a little. By putting words like ‚Äúproblem‚Äù, ‚Äúcare‚Äù, ‚Äúchallenge‚Äù, ‚Äúalert‚Äù, especially in bold letters, in addition to including emojis that convey this purpose, such as ‚ö†Ô∏è and ‚õî. This quickly draws anyone‚Äôs attention, as it gives you a sense of danger, and you enter a state of alert.\nIf you think about it a little more, you‚Äôll probably realize that you have that same instinct when you‚Äôre watching a movie, or a series, and the hero of that story suddenly gets into a dangerous situation. You quickly pay more attention to what is happening, because you want to see how the hero is going to get out of this hole, or you are really rooting for him to survive and overcome this problem.\nWhen we present an issue about the product/service you are reviewing, we want to cause the same effect on our viewers. When we say that we have a challenge/problem that is affecting the company‚Äôs sales, managers quickly start to pay more attention to what you are saying, as they want to know how they can get out of this hole!\n\n\n6.1.3 Hiding the problems is a bad idea\nThis is very important! Some analysts are afraid or apprehensive of shedding light on existing problems, and therefore, end up hiding them or omitting them from their presentations. However, you are not delivering any value to your customer that way! You are not helping your customer to solve their problems and to grow their business!\nAnd if the problems are not solved, if they continue to exist, they will grow, and grow, until they explode, thus generating generalized chaos. The sooner you identify the problem, notify managers about it, and present possible solutions for it, the better for managers, who already leave with an action plan to solve this problem, and it is better for you too, because you are delivering value and solutions to your customer.\nTherefore, a good presentation, or a good data report, is one that delivers value to your customer! By showing new business opportunities (e.g.¬†reaching a new portion of the public with a product), and also by presenting solutions to current problems that are limiting or preventing business growth.\nHowever, a presentation that only comments on positive points, that says that everything is fine‚Ä¶ does not bring any value to managers. Managers didn‚Äôt hire you to tell you everything is fine. They hired you to help them discover and solve problems in their business through data analysis.\n\n\n6.1.4 Present the consequences of the conflict/problem/challenge you have identified\nTherefore, after presenting a conflict/problem/challenge that is affecting the business, it is important that you present the consequence of this problem right away. This helps managers to have a dimension of the size that this problem represents for their business.\nIt‚Äôs okay if you can‚Äôt measure in numbers the size of the impact that this conflict had on the business. Try to measure that impact as best you can. An approximate value of the impact can already bring enough clarity about the size of the danger that this conflict represents for the business.\nYou can also provide a range, or a possible range of the impact if you can (e.g.¬†the estimated impact is between $20 thousand and $340 thousand). This is also a valid way of exposing the size of the problem.\nIf it is really impossible to measure this impact in numbers, then explain in this part, which are the points of the product/service sales process that are affected by this problem. In other words, present which are the places in the business that are being, in theory, affected by this conflict.\n\n\n6.1.5 Advise your customer, present possible solutions to the problem\nOkay, we present a problem, or a conflict for our viewers. We also discussed the impacts of this conflict on our client‚Äôs business. Now, we need to present possible solutions to this problem.\nTherefore, understand the problem/conflict you are presenting, and try to list what would be the main solutions for this problem, and include these solutions in this part of your presentation. It‚Äôs worth explaining and discussing this issue with other co-workers as well, as they might also come up with interesting solutions that were off your radar.\nIt is also interesting to include a list of trade-offs for each solution, especially in terms of complexity and effort for each solution. Managers are constantly interested in this relationship, and always want to choose the solution that is simpler and faster to implement."
  },
  {
    "objectID": "posts/2023/2023-03-06-storytelling/en/index.html#character-evolution-as-another-alternative",
    "href": "posts/2023/2023-03-06-storytelling/en/index.html#character-evolution-as-another-alternative",
    "title": "A case study about Data Storytelling at some of the largest brazilian banks",
    "section": "6.2 Character evolution as another alternative",
    "text": "6.2 Character evolution as another alternative\nCharacter evolution (or the ‚Äúhero‚Äôs journey‚Äù) is a popular story structure. You start with a character, or a hero for the story, which in our case here, could be our client‚Äôs business, or a specific product of this business.\nOur mission is to show how this character/hero has evolved in recent months. Therefore, this evolution structure is very suitable for year-end reports. Because these reports are interesting to show the evolution of the business (or the evolution of the character) over the last year.\n\n6.2.1 The beginning and end of the journey\nIf you prefer, you can start your presentation by showing how your character was at the beginning of the journey, and end this presentation by showing how that character ended that journey.\nThis is an option, however, I prefer to start the presentation by showing the start and end at the same time. In this way, viewers begin the presentation with an idea of how much the business has evolved during the year. Therefore, I start by showing how the character (or the business/product) started the year, and then how he ended the year.\n\n\n6.2.2 Presenting the challenges and pitfalls\nIn every business/product, whatever it may be, we always face major challenges that can threaten the success, or limit the evolution of this business/product.\nTherefore, along any journey, we always face challenges and setbacks, and we apply actions to try to overcome these challenges. By overcoming these challenges, we can hopefully generate the evolution and improvement of this business/product. That is, reaching the end of this journey with a better, more robust and more profitable business/product.\nAll of this means that ‚Äúhow we get through the middle of the journey‚Äù can be much more important/interesting than the beginning or end of that journey. Therefore, reserve a part of your presentation to present the main challenges we faced during this journey, and how we overcame them.\n\n\n6.2.3 If possible, take the opportunity to value your team‚Äôs work\nAt this particular point in the presentation, you usually have a very interesting opportunity! Because you can deliver value to your customer and, at the same time, also value the work of your team, especially if this was the team that discovered the challenges, and applied the actions that overcame the challenges you are describing.\nThis is a golden opportunity! Therefore, if you have this opportunity in your hands, take advantage of it! Remember that you don‚Äôt work alone. You‚Äôre almost always working within a team of people, and it‚Äôs always important to know how to value your colleagues‚Äô work.\nHowever, this opportunity will not always appear for you. Perhaps the challenges you are presenting were overcome in another way, by another team that you do not know, or that you do not have a direct connection with.\n\n\n6.2.4 Reinforce the results achieved\nIt is also useful to end your presentation by showing a summary of the results achieved during the year. This could be a simple bulletpoint summarizing the main results. This helps to jog your viewers‚Äô memory, showing them not only how we ended last year‚Äôs journey, but also, how we begin next year‚Äôs journey üòâ.\n\n\n6.2.5 A summary of the structure\nTherefore, over the previous sections we discussed an idea of structure that would be similar to a ‚Äúcharacter evolution‚Äù. In the end, we have a story that follows the sequence below:\n\nquickly present how we started and how we ended the journey;\npresent the main challenges we faced during the year;\nwhat were the solutions we applied to solve the problems;\nreinforce the evolution of the character, by showing again the results achieved with the solutions above;"
  },
  {
    "objectID": "posts/2023/2023-03-06-storytelling/en/index.html#footnotes",
    "href": "posts/2023/2023-03-06-storytelling/en/index.html#footnotes",
    "title": "A case study about Data Storytelling at some of the largest brazilian banks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is worth remembering that the numbers and graphs presented in this image are merely illustrative, and were defined in a completely random way.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2020/2020-10-17-formulario-pesquisa/pt/index.html",
    "href": "posts/2020/2020-10-17-formulario-pesquisa/pt/index.html",
    "title": "Resultados do Formul√°rio de Pesquisa de Interesse - Curso Introdut√≥rio de R",
    "section": "",
    "text": "Esse post faz parte de um projeto meu, de um Curso Introdut√≥rio de R. Esse projeto j√° tem um ano, que venho aperfei√ßoando-o, e a primeira vez que ele foi posto em pr√°tica (ou seja, a primeira vez que dei um curso da linguagem) foi no instituto de pesquisa onde trabalho, a Funda√ß√£o Jo√£o Pinheiro1. Foi um momento muito proveitoso, pois uma das melhores formas de se aprender uma mat√©ria, √© tentando ensin√°-la para outras pessoas.\nAprimorei e venho aprimorando constatemente o meu conhecimento da linguagem R, e agora, uma nova oportunidade de ensinar essa linguagem, surgiu atrav√©s de um convite da CORECON-MG2 Acad√™mico. Com a ideia de compreendermos melhor as prefer√™ncias do potencial p√∫blico desse curso, n√≥s lan√ßamos um formul√°rio de pesquisa no dia 10/10/2020.\nNeste post, estarei analisando rapidamente as respostas que coletamos por esse formul√°rio. Por motivos √≥bvios, as informa√ß√µes pessoais daqueles que responderam ao formul√°rio, foram omitidas nesse artigo."
  },
  {
    "objectID": "posts/2020/2020-10-17-formulario-pesquisa/pt/index.html#footnotes",
    "href": "posts/2020/2020-10-17-formulario-pesquisa/pt/index.html#footnotes",
    "title": "Resultados do Formul√°rio de Pesquisa de Interesse - Curso Introdut√≥rio de R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttp://novosite.fjp.mg.gov.br/‚Ü©Ô∏é\nConselho Regional de Economia Acad√™mico de Minas Gerais:http://corecon-mg.org.br/academico/‚Ü©Ô∏é\nhttps://www.bioconductor.org/‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2021/2021-01-02-tabela-destinataria-fonte/en/index.html",
    "href": "posts/2021/2021-01-02-tabela-destinataria-fonte/en/index.html",
    "title": "Recipient table and source table",
    "section": "",
    "text": "Introduction\nOuter joins are a simple topic of understanding for most students. However, this article proposes a second approach on the subject. This approach was built during a recent reformulation of the chapter ‚ÄúIntrodu√ß√£o a base de dados relacionais‚Äù, from the book Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica. The point of view presented here seeks to be strict, and uses this rigidity as a mechanism that facilitates the student`s memorization of the behaviors in each type of outer join.\n\n\nJoins have direction\nA join operation is nothing more than a union between two tables. But, instead of an union of two tables, we have another option for interpreting the result of this operation. So that to produce a table that represents the union between two tables, we could simply extract all the columns from one of the tables, and insert them into the other table.\nFor example, suppose you have a table A, which contains two columns, called x and y; and a table B, which in turn holds 4 different columns, named x, z, r, and t. Note that one of the columns in table B corresponds to the same column x that we found in table A.\n\nlibrary(tibble)\n\nA &lt;- tibble(\n  x = 1:5,\n  y = round(rnorm(5, 2, 1), 2)\n)\n\nB &lt;- tibble(\n  x = 1:5,\n  z = letters[1:5],\n  r = c(3.5, 2.1, 1, 5.6, 7.2),\n  t = \"tzu\"\n)\n\nIf you want to join tables A and B, you basically want to create a new table, which contains all five columns of these two tables (x, y, z, r and t). Therefore, we could imagine a join process, as if we were extracting all the columns from table B, and inserting all of these columns in table A. Hence, we have the table below as a result:\n\nfull_join(A, B, by = \"x\")\n\n# A tibble: 5 √ó 5\n      x     y z         r t    \n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;\n1     1  1.11 a       3.5 tzu  \n2     2  3.3  b       2.1 tzu  \n3     3  3.53 c       1   tzu  \n4     4  0.4  d       5.6 tzu  \n5     5  1.78 e       7.2 tzu  \n\n\nWith this, we are creating the idea that a join always has a direction. In other words, we first extract the columns from table B, and then we add those columns to table A. Note that we are always starting from table B towards table A.\n\n\n\n\n\n\n\n\n\n\n\nRecipient table and source table\nFrom this perspective, we can interpret that, in a join, we are bringing all the columns of a secondary table into our main table (or our table of interest). We have the option to call these tables :source table (secondary table) and recipient table (main table). With this, a join always starts at source table and go towards the recipient table.\nThis perspective makes sense with the practice of joins. Because in any analysis, we commonly work with a ‚Äúmain‚Äù table, or a table that contains the key data we‚Äôre analyzing. And when we use some join, we‚Äôre usually bringing columns from other tables into this ‚Äúmain‚Äù table (or ‚Äúrecipient‚Äù table, according to that perspective). So keep in mind that a join always part from the source table toward the recipient table.\n\n\nTypes of outer join\nA natural join (inner join) usually generates a loss of observations from both tables involved in the process. In contrast, a join of type outer (that is, an outer join), seeks to delimit which of the two tables will be preserved in the result. That is, an outer join seeks to keep in the join result, the rows of at least one of the tables involved.\nWe have three main types of outer joins, which are left join, right join and full join. A full join is the simplest to understand, as it seeks to keep all rows of both tables employed. Therefore, even if there is some observation not found in one of the tables, it will be preserved in the final product of the operation.\nHowever, left join and right join seek to keep rows from only one of the tables used in join. At this point, many teachers would say something like: ‚Äúif we want to apply a join between tables A and B, a left join will keep the rows of table A, and a right join will keep the rows of table B‚Äù. Other teachers would still try to say, ‚Äúleft join will keep the table rows on the left, while a right join will keep the table rows to the right‚Äù.\nHowever, some confusion can be easily applied in both alternatives. I mean, a student can easily face the following question: ‚Äúü§î Uhmm‚Ä¶ I don‚Äôt remember very well. Does a left join keep the rows in table A? Or are the rows in table B?‚Äù; or else, ‚Äúü§î Wait! But which of the two tables is on the right side?‚Äù\n\n\nConclusion\nWith this, according to the perspective adopted in this article, we can understand that a left join and a right join seek to keep the rows of the recipient table and the source table, respectively. Thus, when using a right join or a left join, you should ask yourself the following: ‚ÄúDo I want to keep the rows in my main table (recipient table)? Or the secondary table (source table), where I am extracting the new columns from?‚Äù So if you want to keep, for example, the rows in your main table (recipient table), which is what occurs most of the time, you now know that you need to use a left join.\nIn a visual representation, we can reproduce below the initial image of this article, which marks the lines maintained by each of these two types of join."
  },
  {
    "objectID": "posts/pt.html",
    "href": "posts/pt.html",
    "title": "Posts em Portugu√™s",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nUm pequeno exemplo sobre como utilizar dados para tomar melhores decis√µes\n\n\n\n\n\n\nData Storytelling\n\n\nData Science\n\n\n\nEste artigo descreve um exemplo real em que eu e meu time usamos dados para convencer o nosso cliente na √©poca a tomar uma melhor decis√£o.\n\n\n\n\n\nOct 28, 2023\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nMelhorando a tomada de decis√£o com Git e porqu√™ voc√™ deveria se importar\n\n\n\n\n\n\nGit\n\n\nDecisions\n\n\nDocumentation\n\n\n\nEsse artigo usa um caso real que eu e meu time enfrentamos para mostrar como voc√™ pode melhorar a tomada de decis√£o de seu time, ao rastrear as suas decis√µes com Git\n\n\n\n\n\nJul 31, 2023\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nUm estudo de caso sobre Data Storytelling em alguns dos maiores bancos brasileiros\n\n\n\n\n\n\nStorytelling\n\n\nData Storytelling\n\n\nData Science\n\n\n\nNeste artigo discuto a minha experi√™ncia sobre como eu construi e melhorei o Data Storytelling de minhas apresenta√ß√µes para dois dos maiores bancos do mercado brasileiro.\n\n\n\n\n\nMar 6, 2023\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nJOINs s√£o importantes fontes de duplica√ß√µes em seus dados\n\n\n\n\n\n\nJOINs\n\n\n\nSe os seus dados crescerem de forma repentina, sem explica√ß√£o aparente, verifique se os seus JOINs s√£o a fonte desse problema\n\n\n\n\n\nJan 10, 2023\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nNovidades da 4¬∞ edi√ß√£o de Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica\n\n\n\n\n\n\nBook\n\n\nR\n\n\n\nEsta quarta edi√ß√£o traz algumas melhorias que buscam manter um dos principais objetivos deste livro, que √© ser uma refer√™ncia moderna, introdut√≥ria e t√©cnica sobre a Linguagem R.\n\n\n\n\n\nDec 26, 2022\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nErro de mem√≥ria no Power BI Online? Hora de repensar o seu dashboard\n\n\n\n\n\n\nPower BI\n\n\nPower BI Online\n\n\n\nNeste post explico como um erro de mem√≥ria no Power BI Online pode ser um forte sinal de que voc√™ est√° errando no design de seu dashboard\n\n\n\n\n\nDec 14, 2022\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nNovidades da 3¬∞ edi√ß√£o de Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica\n\n\n\n\n\n\nBook\n\n\nR\n\n\n\nUma grande reforma sobre a obra foi feita para esta nova edi√ß√£o. Como resultado, temos seis novos cap√≠tulos para voc√™!\n\n\n\n\n\nApr 6, 2022\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nNovidades da 2¬∞ edi√ß√£o de Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica\n\n\n\n\n\n\nBook\n\n\nR\n\n\n\nEsse post descreve as novas adi√ß√µes √† segunda edi√ß√£o do livro Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica. Fatores (factors); Datas e vari√°veis de tempo (date, POSIXct); al√©m da introdu√ß√£o de exerc√≠cios em cada cap√≠tulo, est√£o entre as principais novidades da pr√≥xima edi√ß√£o.\n\n\n\n\n\nMay 26, 2021\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nTabela destinat√°ria e tabela fonte\n\n\nUma segunda vis√£o sobre outer joins\n\n\n\nJOINs\n\n\nTeaching\n\n\n\nUm vis√£o diferente (talvez maluca) sobre como ensinar/explicar Outer joins para seus alunos\n\n\n\n\n\nJan 2, 2021\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nResultados do Formul√°rio de Pesquisa de Interesse - Curso Introdut√≥rio de R\n\n\n\n\n\n\nR\n\n\nTeaching\n\n\n\nUma r√°pida an√°lise sobre o impacto e as sugest√µes fornecidas ao Curso Introdut√≥rio de R\n\n\n\n\n\nOct 17, 2020\n\n\nPedro Duarte Faria\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "All Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nProducing evenly-spaced and non-overlapping curves with Jobard-Lefer Algorithm in C\n\n\n\nFlow Fields\n\n\nC\n\n\nAlgorithm\n\n\nComputer Graphcis\n\n\n\nIn this article, I want to implement the Jobard-Lefer Algorithm in C. With this algorithm, we can produce evenly-spaced and non-overlapping curves in a flow field (i.e.¬†a‚Ä¶\n\n\n\nPedro Duarte Faria\n\n\nFeb 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHave you ever SSH before?\n\n\n\nSSH\n\n\nServers\n\n\nNetwork\n\n\n\nDo you know how SSH works? Have you ever SSH into a remote computer before? In this post I want to describe how can you login into a remote computer using SSH.\n\n\n\nPedro Duarte Faria\n\n\nJan 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use the cache effectively on Apache Spark\n\n\n\nApache Spark\n\n\nPerformance\n\n\n\nDid you ever used the DataFrame method cache() to cache your Spark DataFrame? In this post, I want to describe how you can use this method to get better performance.\n\n\n\nPedro Duarte Faria\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy experience with Typst, the potential sucessor of LaTex\n\n\n\nTypst\n\n\nTypesetting system\n\n\nDocuments\n\n\n\nLet me quickly describe my recent experience with Typst, which is a new markup-based typesetting system for composing documents and articles for science, and also, the most‚Ä¶\n\n\n\nPedro Duarte Faria\n\n\nDec 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeoVim commands and shortcuts cheatsheet\n\n\n\nNeoVim\n\n\nVim\n\n\nCheatsheet\n\n\n\nI have been using NeoVim in this past year. However, is hard to remember all commands and shortcuts available, so I dedided to write a cheatsheet with all Vim commands and‚Ä¶\n\n\n\nPedro Duarte Faria\n\n\nDec 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow strings work in Zig?\n\n\n\nZig\n\n\nStrings\n\n\nEncoding\n\n\n\nA quick introduction to string literals in Zig, and how strings in Zig differ from strings in other programming languages.\n\n\n\nPedro Duarte Faria\n\n\nDec 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping a parser for Python with Python\n\n\n\nPython\n\n\nParser\n\n\n\nThis past week I had to develop a parser for Python expressions with Python. In this article, I want to use this experience to introduce the subject of parsing to beginners.\n\n\n\nPedro Duarte Faria\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA small example on how to make better decisions with data\n\n\n\nData Storytelling\n\n\nData Science\n\n\n\nThis article describes a real world situation where me and my team used data to convince our client to do a better decision.\n\n\n\nPedro Duarte Faria\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUm pequeno exemplo sobre como utilizar dados para tomar melhores decis√µes\n\n\n\nData Storytelling\n\n\nData Science\n\n\n\nEste artigo descreve um exemplo real em que eu e meu time usamos dados para convencer o nosso cliente na √©poca a tomar uma melhor decis√£o.\n\n\n\nPedro Duarte Faria\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImproving decision making with Git and why you should care\n\n\n\nGit\n\n\nDecisions\n\n\nDocumentation\n\n\n\nThis article uses a real world situation that me and my team encountered to show how you can improve decision making in your team by tracking the decisions you make with Git\n\n\n\nPedro Duarte Faria\n\n\nJul 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMelhorando a tomada de decis√£o com Git e porqu√™ voc√™ deveria se importar\n\n\n\nGit\n\n\nDecisions\n\n\nDocumentation\n\n\n\nEsse artigo usa um caso real que eu e meu time enfrentamos para mostrar como voc√™ pode melhorar a tomada de decis√£o de seu time, ao rastrear as suas decis√µes com Git\n\n\n\nPedro Duarte Faria\n\n\nJul 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA case study about Data Storytelling at some of the largest brazilian banks\n\n\n\nStorytelling\n\n\nData Storytelling\n\n\nData Science\n\n\n\nThis article discuss my experience on how I improved the Data Storytelling of my presentations and reports to two of the largest banks on the financial brazilian market\n\n\n\nPedro Duarte Faria\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUm estudo de caso sobre Data Storytelling em alguns dos maiores bancos brasileiros\n\n\n\nStorytelling\n\n\nData Storytelling\n\n\nData Science\n\n\n\nNeste artigo discuto a minha experi√™ncia sobre como eu construi e melhorei o Data Storytelling de minhas apresenta√ß√µes para dois dos maiores bancos do mercado brasileiro.\n\n\n\nPedro Duarte Faria\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing the {ggfunnel} R package\n\n\n\nR\n\n\nPackage\n\n\nPower BI\n\n\nFunnel charts\n\n\n\nIn this post, I want to introduce an experimental R package üì¶ which you can use to build Power BI like funnel charts in R.\n\n\n\nPedro Duarte Faria\n\n\nFeb 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat I learned from developing my first Python package\n\n\n\nPython\n\n\nPackage\n\n\nDevelopment workflow\n\n\n\nIn this post, I want to share some of the challenges, and what I learned from developing my first Python package üì¶ published at PyPI\n\n\n\nPedro Duarte Faria\n\n\nFeb 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJOINs s√£o importantes fontes de duplica√ß√µes em seus dados\n\n\n\nJOINs\n\n\n\nSe os seus dados crescerem de forma repentina, sem explica√ß√£o aparente, verifique se os seus JOINs s√£o a fonte desse problema\n\n\n\nPedro Duarte Faria\n\n\nJan 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovidades da 4¬∞ edi√ß√£o de Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica\n\n\n\nBook\n\n\nR\n\n\n\nEsta quarta edi√ß√£o traz algumas melhorias que buscam manter um dos principais objetivos deste livro, que √© ser uma refer√™ncia moderna, introdut√≥ria e t√©cnica sobre a‚Ä¶\n\n\n\nPedro Duarte Faria\n\n\nDec 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing the {spark_map} Python package\n\n\n\nPython\n\n\nPackage\n\n\npyspark\n\n\nApache Spark\n\n\n\nWith this package, you can easily apply a function over multiple columns of a Spark DataFrame\n\n\n\nPedro Duarte Faria\n\n\nDec 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nErro de mem√≥ria no Power BI Online? Hora de repensar o seu dashboard\n\n\n\nPower BI\n\n\nPower BI Online\n\n\n\nNeste post explico como um erro de mem√≥ria no Power BI Online pode ser um forte sinal de que voc√™ est√° errando no design de seu dashboard\n\n\n\nPedro Duarte Faria\n\n\nDec 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing the {figma} R package\n\n\n\nR\n\n\nPackage\n\n\nFigma\n\n\nAPI\n\n\n\nWith this package, you can access the Figma API to bring your design files to R üé®!\n\n\n\nPedro Duarte Faria\n\n\nNov 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovidades da 3¬∞ edi√ß√£o de Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica\n\n\n\nBook\n\n\nR\n\n\n\nUma grande reforma sobre a obra foi feita para esta nova edi√ß√£o. Como resultado, temos seis novos cap√≠tulos para voc√™!\n\n\n\nPedro Duarte Faria\n\n\nApr 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovidades da 2¬∞ edi√ß√£o de Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica\n\n\n\nBook\n\n\nR\n\n\n\nEsse post descreve as novas adi√ß√µes √† segunda edi√ß√£o do livro Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica. Fatores (factors); Datas e vari√°veis de tempo‚Ä¶\n\n\n\nPedro Duarte Faria\n\n\nMay 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecipient table and source table\n\n\n\nJOINs\n\n\nTeaching\n\n\n\nA different (maybe crazy) view on how to teach/explain Outer joins to students\n\n\n\nPedro Duarte Faria\n\n\nJan 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTabela destinat√°ria e tabela fonte\n\n\n\nJOINs\n\n\nTeaching\n\n\n\nUm vis√£o diferente (talvez maluca) sobre como ensinar/explicar Outer joins para seus alunos\n\n\n\nPedro Duarte Faria\n\n\nJan 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResultados do Formul√°rio de Pesquisa de Interesse - Curso Introdut√≥rio de R\n\n\n\nR\n\n\nTeaching\n\n\n\nUma r√°pida an√°lise sobre o impacto e as sugest√µes fornecidas ao Curso Introdut√≥rio de R\n\n\n\nPedro Duarte Faria\n\n\nOct 17, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/pt.html",
    "href": "publications/pt.html",
    "title": "Publica√ß√µes em Portugu√™s",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nIntrodu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica\n\n\n\n\n\n\nR\n\n\nBook\n\n\nBrazil\n\n\n\nEste livro oferece uma descri√ß√£o profunda sobre os fundamentos da linguagem R, e como eles se aplicam no contexto da an√°lise de dados.\n\n\n\n\n\nDec 1, 2022\n\n\nPedro Duarte Faria\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "All Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nIntroduction to pyspark\n\n\n\nPython\n\n\nApache Spark\n\n\nBook\n\n\n\nA book for anyone who wants to learn quickly how to use pyspark to effectively load, process, and transform large volumes of data using Python.\n\n\n\nPedro Duarte Faria\n\n\nJan 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntrodu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica\n\n\n\nR\n\n\nBook\n\n\nBrazil\n\n\n\nEste livro oferece uma descri√ß√£o profunda sobre os fundamentos da linguagem R, e como eles se aplicam no contexto da an√°lise de dados.\n\n\n\nPedro Duarte Faria\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/book/introd-pyspark/en/index.html",
    "href": "publications/book/introd-pyspark/en/index.html",
    "title": "Introduction to pyspark",
    "section": "",
    "text": "About the book\nThis book is for anyone who wants to learn quickly how to use pyspark to effectively load, process, and transform large volumes of data using Python.\n Buy a copy of the book!  Read online  Official repository\nIn more detail, this is a quick and introductory book about pyspark, which is the Python API for Apache Spark. Apache Spark is the de facto standard engine for big-data analytics. It is largely used to build data processing, data ingestion, and machine learning applications that process very large volumes of data.\nOne of the many reasons why Apache Spark became popular is because of its APIs. You can build Spark applications using different programming languages, such as Python, R, and Scala. But this book focuses solely on the Python API.\nIn this book, you will learn about:\n\nHow an Apache Spark application works?\nWhat are Spark DataFrames?\nHow to build, transform and model your Spark DataFrame.\nHow to import data into (or export data out of) Apache Spark.\nHow to work with SQL inside pyspark.\nTools for manipulating specific data types (e.g.¬†strings, dates and datetimes).\nHow to use window functions.\n\n\n\nMotivation for the book\nIn summary, this book aims to give a solid introduction (for python and not python users) to the pyspark package, and on how to use it to build Spark applications for data pipelines and interactive data analysis.\nAlthough we have a good range of materials about Apache Spark in general, such as Damji et al. (2020) and Chambers and Zaharia (2018), we do not have much abundance of materials about the APIs of Spark in ‚Äúforeign‚Äù languages, like the Python (pyspark) and R (SparkR) APIs.\nThe reason for this is simple: the Spark API have a consistent structure across all languages. As consequence, a general book about Spark can fairly cover all languages at once. In other words, Spark code in Scala can be easily translated into python code with pyspark. Because the structure of the code is very similar between all languages.\nSo why this book? First, Python is a more popular and friendly language than Scala or Java. If the reader is not interested in learning Java or Scala, why show Java/Scala code to him? Is very important to focus solely on what interest the reader, specially if it is in a language that he is familiar with. Second, I had some time to spent, and a lot of practical experience with pyspark on production to share (so‚Ä¶ why not write a book about it?).\n\n\nCover\n\n\n\n\n\n\nReferences\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive Guide: Big Data Processing Made Simple. Sebastopol, CA: O‚ÄôReilly Media.\n\n\nDamji, Jules, Brooke Wenig, Tathagata Das, and Denny Lee. 2020. Learning Spark: Lightning-Fast Data Analytics. Sebastopol, CA: O‚ÄôReilly Media."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi there üñêÔ∏è!",
    "section": "",
    "text": "In this page, you can quickly scan through my latest published books üìö and blog posts üìå. But, if you prefer, you can read a quick resume of me and my work in the About Me section too. Be free to contact me via email if you have an interesting proposal üòâ. If you want to support me monetarily, you can send me a Pix (more info in the Donate or Sponsor Me section)."
  },
  {
    "objectID": "index.html#my-books",
    "href": "index.html#my-books",
    "title": "Hi there üñêÔ∏è!",
    "section": "My books",
    "text": "My books\n\n\n\n\n\n\n\n\n\n\nIntroduction to pyspark\n\n\nA book for anyone who wants to learn quickly how to use pyspark to effectively load, process, and transform large volumes of data using Python.\n\n\n\nPedro Duarte Faria\n\n\nJan 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntrodu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica\n\n\nEste livro oferece uma descri√ß√£o profunda sobre os fundamentos da linguagem R, e como eles se aplicam no contexto da an√°lise de dados.\n\n\n\nPedro Duarte Faria\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#most-recent-posts-in-english",
    "href": "index.html#most-recent-posts-in-english",
    "title": "Hi there üñêÔ∏è!",
    "section": "Most recent Posts in English",
    "text": "Most recent Posts in English\n\n\n\n\n\n\n\n\n\n\nProducing evenly-spaced and non-overlapping curves with Jobard-Lefer Algorithm in C\n\n\nIn this article, I want to implement the Jobard-Lefer Algorithm in C. With this algorithm, we can produce evenly-spaced and non-overlapping curves in a flow field (i.e.¬†a‚Ä¶\n\n\n\nPedro Duarte Faria\n\n\nFeb 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHave you ever SSH before?\n\n\nDo you know how SSH works? Have you ever SSH into a remote computer before? In this post I want to describe how can you login into a remote computer using SSH.\n\n\n\nPedro Duarte Faria\n\n\nJan 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use the cache effectively on Apache Spark\n\n\nDid you ever used the DataFrame method cache() to cache your Spark DataFrame? In this post, I want to describe how you can use this method to get better performance.\n\n\n\nPedro Duarte Faria\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy experience with Typst, the potential sucessor of LaTex\n\n\nLet me quickly describe my recent experience with Typst, which is a new markup-based typesetting system for composing documents and articles for science, and also, the most‚Ä¶\n\n\n\nPedro Duarte Faria\n\n\nDec 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeoVim commands and shortcuts cheatsheet\n\n\nI have been using NeoVim in this past year. However, is hard to remember all commands and shortcuts available, so I dedided to write a cheatsheet with all Vim commands and‚Ä¶\n\n\n\nPedro Duarte Faria\n\n\nDec 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow strings work in Zig?\n\n\nA quick introduction to string literals in Zig, and how strings in Zig differ from strings in other programming languages.\n\n\n\nPedro Duarte Faria\n\n\nDec 12, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#posts-mais-recentes-em-portugu√™s",
    "href": "index.html#posts-mais-recentes-em-portugu√™s",
    "title": "Hi there üñêÔ∏è!",
    "section": "Posts mais recentes em Portugu√™s",
    "text": "Posts mais recentes em Portugu√™s\n\n\n\n\n\n\n\n\n\n\nUm pequeno exemplo sobre como utilizar dados para tomar melhores decis√µes\n\n\nEste artigo descreve um exemplo real em que eu e meu time usamos dados para convencer o nosso cliente na √©poca a tomar uma melhor decis√£o.\n\n\n\nPedro Duarte Faria\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMelhorando a tomada de decis√£o com Git e porqu√™ voc√™ deveria se importar\n\n\nEsse artigo usa um caso real que eu e meu time enfrentamos para mostrar como voc√™ pode melhorar a tomada de decis√£o de seu time, ao rastrear as suas decis√µes com Git\n\n\n\nPedro Duarte Faria\n\n\nJul 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUm estudo de caso sobre Data Storytelling em alguns dos maiores bancos brasileiros\n\n\nNeste artigo discuto a minha experi√™ncia sobre como eu construi e melhorei o Data Storytelling de minhas apresenta√ß√µes para dois dos maiores bancos do mercado brasileiro.\n\n\n\nPedro Duarte Faria\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJOINs s√£o importantes fontes de duplica√ß√µes em seus dados\n\n\nSe os seus dados crescerem de forma repentina, sem explica√ß√£o aparente, verifique se os seus JOINs s√£o a fonte desse problema\n\n\n\nPedro Duarte Faria\n\n\nJan 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovidades da 4¬∞ edi√ß√£o de Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica\n\n\nEsta quarta edi√ß√£o traz algumas melhorias que buscam manter um dos principais objetivos deste livro, que √© ser uma refer√™ncia moderna, introdut√≥ria e t√©cnica sobre a‚Ä¶\n\n\n\nPedro Duarte Faria\n\n\nDec 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nErro de mem√≥ria no Power BI Online? Hora de repensar o seu dashboard\n\n\nNeste post explico como um erro de mem√≥ria no Power BI Online pode ser um forte sinal de que voc√™ est√° errando no design de seu dashboard\n\n\n\nPedro Duarte Faria\n\n\nDec 14, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hey! I‚Äôm Pedro Duarte Faria. A brazilian economist, data engineer and software engineer, working mainly with R, SQL, Python and Apache Spark. I do a lot of data engineering these days, but I do love teaching software development and building open-source software too."
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About Me",
    "section": "",
    "text": "Hey! I‚Äôm Pedro Duarte Faria. A brazilian economist, data engineer and software engineer, working mainly with R, SQL, Python and Apache Spark. I do a lot of data engineering these days, but I do love teaching software development and building open-source software too."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nData Engineer, Blip, Feb 2023 - present.\nData Analyst, Blip, May 2021 - Feb 2023.\nResearch Engineering Intern, Jo√£o Pinheiro Foundation, August 2019 - March 2021.\nBusiness Intelligence Intern, Beltech, June 2019 - August 2019."
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About Me",
    "section": "Projects",
    "text": "Projects\nI‚Äôm the author of R package {figma} and Python package spark_map. I‚Äôm also author of a technical and introductory book about the R language. I also made contributions to R package {knitr}, which is a big open-source project in the R community."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nFederal University of Ouro Preto - UFOP, Brazil\nEconomics, B.S., March 2017 - February 2022."
  },
  {
    "objectID": "publications/book/introducao_linguagem_R/pt/index.html",
    "href": "publications/book/introducao_linguagem_R/pt/index.html",
    "title": "Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica",
    "section": "",
    "text": "1 Sobre o livro\n Compre uma vers√£o do livro  Leia online  Exerc√≠cios de cada cap√≠tulo  Respostas dos exerc√≠cios\nEste livro oferece uma descri√ß√£o profunda sobre os fundamentos da linguagem R, e como eles se aplicam no contexto da an√°lise de dados. Sua principal contribui√ß√£o para a literatura brasileira hoje, est√° no combate de dois problemas recorrentes nos materiais dispon√≠veis em portugu√™s sobre a linguagem: 1) a falta de profundidade de muitos materiais, que tentam abordar muitos assuntos em um espa√ßo muito curto; 2) a alta especializa√ß√£o de muitos materiais, que s√£o de dif√≠cil transposi√ß√£o para aplica√ß√µes gerais em an√°lises de dados.\n\n\n2 O que voc√™ aprende ?\nAtrav√©s deste livro, voc√™ pode aprender sobre os fundamentos da linguagem R, e como eles se aplicam a √°rea de an√°lise de dados. Em mais detalhes:\n\nIntroduzindo a linguagem: aprenda sobre como definir e como trabalhar com objetos; conhe√ßa as estruturas e tipos de dados oferecidos pela linguagem; entenda como os processos de coer√ß√£o e os valores especiais da linguagem podem afetar os seus resultados.\nImporta√ß√£o e transforma√ß√£o: aprenda a importar, transformar e formatar suas tabelas atrav√©s dos pacotes readr, readxl, haven, dplyr e tidyr. Aplicando opera√ß√µes de ordena√ß√£o, filtro, sele√ß√£o e expans√£o, al√©m de opera√ß√µes de piv√¥ e join‚Äôs.\nVisualiza√ß√£o de dados: aprenda a utilizar o pacote ggplot2 para produzir gr√°ficos elegantes e efetivos para apresentar os seus dados e as suas conclus√µes.\nProgramando a sua an√°lise: aprenda a utilizar controles de fluxo, fun√ß√µes e loops para automatizar tarefas e solucionar os seus problemas de maneira simples e clara.\nFunctional programming: aprenda a utilizar o pacote purrr para distribuir rapidamente os seus c√°lculos ao longo de m√∫ltiplos inputs.\nDebugging e environments: conhe√ßa as principais t√©cnicas de debugging existentes na linguagem R, e, aprenda como investigar erros em suas fun√ß√µes. Al√©m disso, entenda como a linguagem procura pelos seus objetos e, como voc√™ pode produzir resultados inesperados durante essa busca.\n\n\n\n3 Compre uma vers√£o do livro\nHoje, o livro est√° dispon√≠vel para compra em duas vers√µes: como ebook (Kindle), e como livro f√≠sico (Amazon, Estante Virtual e Clube dos Autores). Abaixo temos os links para todas essas vers√µes.\n\nVers√£o ebook (Amazon): https://www.amazon.com.br/dp/B0BNW4K232\nVers√£o f√≠sica (Amazon): https://www.amazon.com.br/dp/6500578724\nVers√£o f√≠sica (Estante Virtual): https://www.estantevirtual.com.br/clube-de-autores/pedro-duarte-faria-introducao-a-linguagem-r-3850927716\nVers√£o f√≠sica (Clube dos Autores): https://clubedeautores.com.br/livro/introducao-a-linguagem-r\n\nCaso voc√™ n√£o possa comprar uma vers√£o do livro, voc√™ ainda pode ler gratuitamente a obra completa atrav√©s de seu website.\n\n\n4 Formas de contribuir para o livro\nCaso voc√™ queira contribuir monetariamente para o livro e para o autor, mas, n√£o queira/possa pagar o pre√ßo total da obra, voc√™ ainda pode mandar um PIX como uma doa√ß√£o.\nAl√©m disso, voc√™ tamb√©m pode contribuir com conte√∫do (imagens, cap√≠tulos, artes, etc.) diretamente para a obra, ao postar pull requests ou issues no reposit√≥rio oficial do livro no GitHub.\n\n\n5 Citando o livro\nArquivo BibTex:\n\n\n@book{pedro2022,\n  title = {Introdu√ß√£o √† Linguagem R},\n  subtitle = {seus fundamentos e sua pr√°tica},\n  author = {Pedro Duarte Faria},\n  year = {2022},\n  edition = {4},\n  address = {Belo Horizonte},\n  month = {Dezembro},\n  isbn = {978-65-00-57872-0},\n  note = {https://pedro-faria.netlify.app/pt/publication/book/introducao_linguagem_r/}\n}\n\n\n\n\n6 Capa do livro"
  },
  {
    "objectID": "publications/en.html",
    "href": "publications/en.html",
    "title": "Publications in English",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nIntroduction to pyspark\n\n\n\n\n\n\nPython\n\n\nApache Spark\n\n\nBook\n\n\n\nA book for anyone who wants to learn quickly how to use pyspark to effectively load, process, and transform large volumes of data using Python.\n\n\n\n\n\nJan 9, 2024\n\n\nPedro Duarte Faria\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "open_source_contrib.html",
    "href": "open_source_contrib.html",
    "title": "Open source contributions",
    "section": "",
    "text": "I love open source. So once upon a time I try to contribute to it. But I do not have a good memory in general, so I decided to create this page just to track down all of my contributions to various open source projects."
  },
  {
    "objectID": "open_source_contrib.html#knitr",
    "href": "open_source_contrib.html#knitr",
    "title": "Open source contributions",
    "section": "3.1 knitr",
    "text": "3.1 knitr\nThe knitr project is a general-purpose literate programming engine, and a big open source project from the R language community.\n\nPR 2140\nPR 2180\nPR 2294"
  },
  {
    "objectID": "open_source_contrib.html#dev_guide",
    "href": "open_source_contrib.html#dev_guide",
    "title": "Open source contributions",
    "section": "3.2 dev_guide",
    "text": "3.2 dev_guide\ndev_guide is the project for ‚ÄúrOpenSci Packages: Development, Maintenance, and Peer Review‚Äù, which is an open source community-driven book that describes the best practices for development of R packages. These contributions are related to the Multilingual Publishing project of ROpenSci.\n\nPR 691\nPR 690\nPR 687\nPR 686\nPR 685\nPR 684\nPR 682\nPR 681\nPR 679\nPR 676\nPR 675\nPR 674\nand many others in this project‚Ä¶"
  },
  {
    "objectID": "open_source_contrib.html#dt",
    "href": "open_source_contrib.html#dt",
    "title": "Open source contributions",
    "section": "3.3 DT",
    "text": "3.3 DT\nDT is an R package that provides an R Interface to the jQuery Plug-in DataTables.\n\nPR 1089"
  },
  {
    "objectID": "posts/en.html",
    "href": "posts/en.html",
    "title": "Posts in English",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nProducing evenly-spaced and non-overlapping curves with Jobard-Lefer Algorithm in C\n\n\n\n\n\n\nFlow Fields\n\n\nC\n\n\nAlgorithm\n\n\nComputer Graphcis\n\n\n\nIn this article, I want to implement the Jobard-Lefer Algorithm in C. With this algorithm, we can produce evenly-spaced and non-overlapping curves in a flow field (i.e.¬†a vector field).\n\n\n\n\n\nFeb 19, 2024\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nHave you ever SSH before?\n\n\n\n\n\n\nSSH\n\n\nServers\n\n\nNetwork\n\n\n\nDo you know how SSH works? Have you ever SSH into a remote computer before? In this post I want to describe how can you login into a remote computer using SSH.\n\n\n\n\n\nJan 22, 2024\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use the cache effectively on Apache Spark\n\n\n\n\n\n\nApache Spark\n\n\nPerformance\n\n\n\nDid you ever used the DataFrame method cache() to cache your Spark DataFrame? In this post, I want to describe how you can use this method to get better performance.\n\n\n\n\n\nJan 13, 2024\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nMy experience with Typst, the potential sucessor of LaTex\n\n\n\n\n\n\nTypst\n\n\nTypesetting system\n\n\nDocuments\n\n\n\nLet me quickly describe my recent experience with Typst, which is a new markup-based typesetting system for composing documents and articles for science, and also, the most promissing sucessor of the famous LaTex typeset system.\n\n\n\n\n\nDec 31, 2023\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nNeoVim commands and shortcuts cheatsheet\n\n\n\n\n\n\nNeoVim\n\n\nVim\n\n\nCheatsheet\n\n\n\nI have been using NeoVim in this past year. However, is hard to remember all commands and shortcuts available, so I dedided to write a cheatsheet with all Vim commands and keyboard shortcuts that I use the most.\n\n\n\n\n\nDec 30, 2023\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nHow strings work in Zig?\n\n\n\n\n\n\nZig\n\n\nStrings\n\n\nEncoding\n\n\n\nA quick introduction to string literals in Zig, and how strings in Zig differ from strings in other programming languages.\n\n\n\n\n\nDec 12, 2023\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping a parser for Python with Python\n\n\n\n\n\n\nPython\n\n\nParser\n\n\n\nThis past week I had to develop a parser for Python expressions with Python. In this article, I want to use this experience to introduce the subject of parsing to beginners.\n\n\n\n\n\nNov 18, 2023\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nA small example on how to make better decisions with data\n\n\n\n\n\n\nData Storytelling\n\n\nData Science\n\n\n\nThis article describes a real world situation where me and my team used data to convince our client to do a better decision.\n\n\n\n\n\nOct 28, 2023\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nImproving decision making with Git and why you should care\n\n\n\n\n\n\nGit\n\n\nDecisions\n\n\nDocumentation\n\n\n\nThis article uses a real world situation that me and my team encountered to show how you can improve decision making in your team by tracking the decisions you make with Git\n\n\n\n\n\nJul 31, 2023\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nA case study about Data Storytelling at some of the largest brazilian banks\n\n\n\n\n\n\nStorytelling\n\n\nData Storytelling\n\n\nData Science\n\n\n\nThis article discuss my experience on how I improved the Data Storytelling of my presentations and reports to two of the largest banks on the financial brazilian market\n\n\n\n\n\nMay 22, 2023\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing the {ggfunnel} R package\n\n\n\n\n\n\nR\n\n\nPackage\n\n\nPower BI\n\n\nFunnel charts\n\n\n\nIn this post, I want to introduce an experimental R package üì¶ which you can use to build Power BI like funnel charts in R.\n\n\n\n\n\nFeb 11, 2023\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nWhat I learned from developing my first Python package\n\n\n\n\n\n\nPython\n\n\nPackage\n\n\nDevelopment workflow\n\n\n\nIn this post, I want to share some of the challenges, and what I learned from developing my first Python package üì¶ published at PyPI\n\n\n\n\n\nFeb 6, 2023\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing the {spark_map} Python package\n\n\n\n\n\n\nPython\n\n\nPackage\n\n\npyspark\n\n\nApache Spark\n\n\n\nWith this package, you can easily apply a function over multiple columns of a Spark DataFrame\n\n\n\n\n\nDec 21, 2022\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing the {figma} R package\n\n\n\n\n\n\nR\n\n\nPackage\n\n\nFigma\n\n\nAPI\n\n\n\nWith this package, you can access the Figma API to bring your design files to R üé®!\n\n\n\n\n\nNov 6, 2022\n\n\nPedro Duarte Faria\n\n\n\n\n\n\n\n\n\n\n\n\nRecipient table and source table\n\n\nA second view on outer joins\n\n\n\nJOINs\n\n\nTeaching\n\n\n\nA different (maybe crazy) view on how to teach/explain Outer joins to students\n\n\n\n\n\nJan 2, 2021\n\n\nPedro Duarte Faria\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021/2021-05-26-2nd-edition-rbook/pt/index.html#adi√ß√£o-de-exerc√≠cios-em-cada-cap√≠tulo",
    "href": "posts/2021/2021-05-26-2nd-edition-rbook/pt/index.html#adi√ß√£o-de-exerc√≠cios-em-cada-cap√≠tulo",
    "title": "Novidades da 2¬∞ edi√ß√£o de Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica",
    "section": "Adi√ß√£o de exerc√≠cios em cada cap√≠tulo",
    "text": "Adi√ß√£o de exerc√≠cios em cada cap√≠tulo\nDesde o in√≠cio, o livro se prop√¥s a ser uma refer√™ncia introdut√≥ria e, principalmente, t√©cnica, sobre a linguagem. Isto significa que, o livro n√£o tenta atingir o p√∫blico que procura por algo sucinto e eficiente, mas sim, o p√∫blico iniciante que busca se aprofundar (ou ter uma base mais s√≥lida) em conceitos, m√©todos e outras partes importantes da linguagem.\nApesar desse prop√≥sito, o livro inicialmente n√£o oferece exerc√≠cios, os quais s√£o uma caracter√≠stica essencial de qualquer obra t√©cnica. Por esse motivo, a introdu√ß√£o de exerc√≠cios em cada cap√≠tulo dessa segunda edi√ß√£o, busca ajudar o livro a caminhar para esse prop√≥sito, se tornando uma obra mais consolidada.\nAtualmente, a constru√ß√£o dos exerc√≠cios para cada cap√≠tulo √© a √∫nica parte (dessa nova edi√ß√£o) que ainda est√° em constru√ß√£o. A partir do momento em que essa etapa for finalizada, a segunda edi√ß√£o ser√° lan√ßada para todo o p√∫blico brasileiro."
  },
  {
    "objectID": "posts/2021/2021-05-26-2nd-edition-rbook/pt/index.html#fatores-factors",
    "href": "posts/2021/2021-05-26-2nd-edition-rbook/pt/index.html#fatores-factors",
    "title": "Novidades da 2¬∞ edi√ß√£o de Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica",
    "section": "Fatores (factor‚Äôs)",
    "text": "Fatores (factor‚Äôs)\nNo cap√≠tulo 2 do livro, s√£o abordados os quatro tipos de dados b√°sicos da linguagem R. Sendo eles: integer; double; character; e logical. Por√©m, h√° dois outros tipos de dados, que s√£o ‚Äúmais complexos‚Äù, e que tamb√©m s√£o muito importantes hoje em dia. S√£o eles os fatores (ou factor‚Äôs) e os tipos voltados para vari√°veis de tempo (date, POSIXct, POSIXlt).\nFatores s√£o especialmente √∫teis para classificarmos vari√°veis categ√≥ricos. Isto √©, definirmos a ordem de seus ‚Äún√≠veis‚Äù (ou ‚Äúgrupos‚Äù). Dito de outra forma, v√°rias caracter√≠sticas podem ser descritas atrav√©s de uma vari√°vel categ√≥rica, mas nem sempre essas caracter√≠sticas seguem uma ordena√ß√£o num√©rica, ou uma ordem alfab√©tica. Por exemplo, ao dar a sua avalia√ß√£o de um certo governo, voc√™ pode responder ‚ÄúP√©ssimo‚Äù, ‚ÄúRuim‚Äù, ‚ÄúBom‚Äù ou ‚ÄúMuito Bom‚Äù. Perceba pela demonstra√ß√£o abaixo, que caso eu empregue uma ordena√ß√£o alfab√©tica (crescente) sobre as minhas respostas, ‚ÄúBom‚Äù aparece antes de ‚ÄúRuim‚Äù, ‚ÄúP√©ssimo‚Äù e ‚ÄúMuito Bom‚Äù.\n\nrespostas &lt;- c(\"Muito Bom\", \"Muito Bom\", \"Bom\", \"Ruim\", \"P√©ssimo\",\n               \"Bom\", \"Ruim\", \"Ruim\", \"Muito Bom\", \"Ruim\")\nsort(respostas)\n\n [1] \"Bom\"       \"Bom\"       \"Muito Bom\" \"Muito Bom\" \"Muito Bom\" \"P√©ssimo\"  \n [7] \"Ruim\"      \"Ruim\"      \"Ruim\"      \"Ruim\"     \n\n\nCom o uso de fatores, n√≥s podemos definir qual a ordem correta desses diferentes n√≠veis e corrigir esse problema de ordena√ß√£o. Perceba abaixo que a ordena√ß√£o dos valores foi corrigida.\n\nrespostas &lt;- factor(respostas)\nlevels(respostas) &lt;- c(\"P√©ssimo\", \"Ruim\", \"Bom\", \"Muito Bom\")\nsort(respostas)\n\n [1] P√©ssimo   P√©ssimo   Ruim      Ruim      Ruim      Bom       Muito Bom\n [8] Muito Bom Muito Bom Muito Bom\nLevels: P√©ssimo Ruim Bom Muito Bom"
  },
  {
    "objectID": "posts/2021/2021-05-26-2nd-edition-rbook/pt/index.html#vari√°veis-de-tempo",
    "href": "posts/2021/2021-05-26-2nd-edition-rbook/pt/index.html#vari√°veis-de-tempo",
    "title": "Novidades da 2¬∞ edi√ß√£o de Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica",
    "section": "Vari√°veis de tempo",
    "text": "Vari√°veis de tempo\nFirmas registram o hor√°rio em que vendas s√£o realizadas, o hor√°rio em que cargas de insumos chegam a suas instala√ß√µes, o tempo de trabalho de seus funcion√°rios, al√©m da evolu√ß√£o de v√°rios de seus indicadores ao longo do tempo. Pois tempo √© dinheiro, e, em uma economia capitalista o dinheiro √© o que faz a diferen√ßa.\nPor essa raz√£o, vari√°veis de tempo s√£o fundamentais em diversas an√°lises de dados e, √© muito importante que voc√™ saiba como trabalhar com elas em qualquer linguagem de programa√ß√£o que voc√™ venha a trabalhar. Por isso, essa √© uma lacuna atual muito importante do livro e, que est√° sendo preenchida nessa segunda edi√ß√£o.\nA linguagem R nos oferece tr√™s tipos principais de dados focados em vari√°veis de tempo, sendo eles: Date, POSIXct, POSIXlt. Al√©m disso, temos ainda um quarto tipo de dado chamado difftime, que busca dar suporte √†s opera√ß√µes aritm√©ticas entre os 3 tipos anteriores. Valores desses tr√™s tipos de dados, s√£o criados a partir das fun√ß√µes as.POSIXct(), as.POSIXlt() e as.Date().\nO tipo Date nos ajuda a armazenar datas no R. Poder√≠amos armazenar datas como simples strings, mas com isso, ter√≠amos uma ordena√ß√£o incorreta dos valores, al√©m da falta de acesso a diversos m√©todos aritm√©ticos e computacionais voltados para esse tipo de dado. Logo, tratar as suas datas por meio do tipo Date pode fazer a diferen√ßa em sua an√°lise.\n\nd &lt;- as.Date(c(\"2020-08-10\", \"2020-08-11\", \"2020-08-12\"))\nd\n\n[1] \"2020-08-10\" \"2020-08-11\" \"2020-08-12\"\n\n\nJ√° o tipo POSIXct √© particularmente √∫til para interpretar datas acompanhadas de um hor√°rio (isto √©, um dado do tipo date-time). Certas transa√ß√µes econ√¥micas precisam ser registradas com um n√≠vel de precis√£o alto. Ou seja, os registros de muitas empresas n√£o se contentam com datas, pois eles tamb√©m necessitam saber o momento exato (ou o hor√°rio exato) dessa data em que um evento ocorre. O tipo POSIXct foi feito justamente para tratar e lidar com esse tipo de informa√ß√£o.\n\nh &lt;- as.POSIXct(c(\"2020-03-21 15:52:29\", \"2020-03-22 10:30:02\"))\nh\n\n[1] \"2020-03-21 15:52:29 -03\" \"2020-03-22 10:30:02 -03\"\n\n\nPor outro lado, o tipo POSIXlt pode ser especialmente √∫til quando desejamos extrair os componentes de uma vari√°vel de tempo espec√≠fica. Por exemplo, podemos estar interessados apenas no dia do m√™s presente em cada valor, ou ainda, na hora do dia guardada em cada um desses valores. Tal ponto est√° demonstrado no exemplo abaixo:\n\nhl &lt;- as.POSIXlt(c(\"2020-03-21 15:52:29\", \"2020-03-22 10:30:02\"))\n\n## O dia de cada data:\nhl$mday\n\n[1] 21 22\n\n## A hora do dia de cada data:\nhl$hour\n\n[1] 15 10"
  },
  {
    "objectID": "posts/2021/2021-05-26-2nd-edition-rbook/pt/index.html#footnotes",
    "href": "posts/2021/2021-05-26-2nd-edition-rbook/pt/index.html#footnotes",
    "title": "Novidades da 2¬∞ edi√ß√£o de Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nComo os materiais produzidos pelo Curso-R, ou este √≠ndice de materiais constru√≠do por Beatriz Milz.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2021/2021-01-02-tabela-destinataria-fonte/pt/index.html",
    "href": "posts/2021/2021-01-02-tabela-destinataria-fonte/pt/index.html",
    "title": "Tabela destinat√°ria e tabela fonte",
    "section": "",
    "text": "Introdu√ß√£o\nOuter joins s√£o um t√≥pico de simples compreens√£o para a maioria dos alunos. Entretanto, esse artigo prop√µe uma segunda abordagem sobre o tema. Tal abordagem foi constru√≠da durante uma reformula√ß√£o recente do cap√≠tulo ‚ÄúIntrodu√ß√£o a base de dados relacionais‚Äù do livro Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica. O ponto de vista apresentado aqui, busca ser estrito, e utiliza essa rigidez como um mecanismo que facilite a memoriza√ß√£o do aluno sobre os comportamentos de cada tipo de outer join.\n\n\nJoins possuem sentido e dire√ß√£o\nUma opera√ß√£o de join √© nada mais do que uma uni√£o entre duas tabelas. Por outro lado, temos uma outra op√ß√£o de interpreta√ß√£o do resultado dessa opera√ß√£o. De modo que, para produzirmos uma tabela que represente a uni√£o entre duas tabelas, poder√≠amos simplesmente extrair todas as colunas de uma das tabelas, e inser√≠-las na outra tabela.\nPor exemplo, suponha que voc√™ possua uma tabela A, que cont√©m duas colunas, chamadas x e y; e uma tabela B, que por sua vez, guarda 4 colunas diferentes, denominadas x, z, r, e t. Perceba que uma das colunas na tabela B, corresponde a mesma coluna x que encontramos na tabela A.\n\nlibrary(tibble)\n\nA &lt;- tibble(\n  x = 1:5,\n  y = round(rnorm(5, 2, 1), 2)\n)\n\nB &lt;- tibble(\n  x = 1:5,\n  z = letters[1:5],\n  r = c(3.5, 2.1, 1, 5.6, 7.2),\n  t = \"tzu\"\n)\n\nSe voc√™ deseja unir as tabelas A e B, voc√™ basicamente deseja criar uma nova tabela, que cont√©m todas as cinco colunas dessas duas tabelas (x, y, z, r e t). Portanto, poder√≠amos imaginar um processo de join, como se estiv√©ssemos transportando todas as colunas da tabela B, para dentro da tabela A. Dessa maneira, temos a tabela abaixo como resultado:\n\nlibrary(dplyr)\n\n\nfull_join(A, B, by = \"x\")\n\n# A tibble: 5 √ó 5\n      x     y z         r t    \n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;\n1     1  2.8  a       3.5 tzu  \n2     2  3.58 b       2.1 tzu  \n3     3  2.95 c       1   tzu  \n4     4  1.56 d       5.6 tzu  \n5     5  1.98 e       7.2 tzu  \n\n\nCom isso, estamos criando a ideia de que um join possui sempre um sentido e uma dire√ß√£o. Em outras palavras, primeiro, extra√≠mos as colunas da tabela B, e em seguida, acrescentamos essas colunas √† tabela A. Repare que estamos sempre partindo da tabela B em dire√ß√£o a tabela A.\n\n\n\n\n\n\n\n\n\n\n\nTabela destinat√°ria e tabela fonte\nPor essa perspectiva, podemos interpretar que, em um join, estamos trazendo todas as colunas de uma tabela secund√°ria, para dentro de nossa tabela principal (ou a nossa tabela de interesse). Temos a op√ß√£o de chamarmos essas tabelas de: tabela fonte (tabela secund√°ria) e tabela destinat√°ria (tabela principal). Com isso, temos que um join sempre parte da tabela fonte em dire√ß√£o √† tabela destinat√°ria.\nEssa perspectiva faz sentido com a pr√°tica de joins. Pois em qualquer an√°lise, n√≥s comumente trabalhamos com uma tabela ‚Äúprincipal‚Äù, ou uma tabela que cont√©m os principais dados que estamos analisando. E quando utilizamos algum join, estamos geralmente trazendo colunas de outras tabelas para dentro dessa tabela ‚Äúprincipal‚Äù (ou tabela ‚Äúdestinat√°ria‚Äù, segundo essa perspectiva). Por isso, tenha em mente que um join sempre parte da tabela fonte em dire√ß√£o √† tabela destinat√°ria.\n\n\nTipos de outer join\nUm join natural (inner join) usualmente gera uma perda de observa√ß√µes de ambas as tabelas envolvidas no processo. Em contrapartida, um join do tipo outer (isto √©, um outer join), busca delimitar qual das duas tabelas ser√° preservada no resultado. Ou seja, um outer join busca manter as linhas de pelo menos uma das tabelas envolvidas, no resultado do join.\nTemos tr√™s tipos principais de outer joins, que s√£o left join, right join e full join. O full join √© o mais simples de se compreender, pois ele busca manter todas as linhas de ambas as tabelas empregadas. Logo, mesmo que haja alguma observa√ß√£o n√£o encontrada em uma das tabelas, ela ser√° preservada no produto final da opera√ß√£o.\nNo entanto, left join e right join buscam conservar as linhas de apenas uma das tabelas utilizadas no join. Nesse ponto, muitos professores diriam algo como: ‚Äúse temos desejamos aplicar um join entre as tabelas A e B, um left join ir√° manter as linhas da tabela A e um right join vai manter as linhas da tabela B‚Äù. Outros professores ainda tentariam dizer: ‚Äúleft join ir√° manter as linhas da tabela √† esquerda, enquanto um right join vai manter as linhas da tabela √† direita‚Äù.\nPor√©m, certa confus√£o pode ser facilmente aplicada em ambas alternativas. Digo, um aluno pode facilmente enfrentar a seguinte quest√£o: ‚Äúü§î Uhmm‚Ä¶ Eu n√£o me lembro muito bem. Um left join mant√©m as linhas da tabela A? Ou s√£o as linhas da tabela B?‚Äù; ou ent√£o, ‚Äúü§î Pera! Mas qual das duas tabelas est√° a direita?‚Äù.\n\n\nConclus√£o\nCom isso, segundo a perspectiva adotada nesse artigo, podemos entender que, um left join e um right join buscam manter as linhas da tabela destinat√°ria e da tabela fonte, respectivamente. Dessa forma, ao utilizar um right join ou um left join, voc√™ deve se questionar o seguinte: ‚ÄúEu quero manter as linhas de minha tabela principal (tabela destinat√°ria)? Ou da tabela secund√°ria (tabela fonte), de onde estou extraindo as novas colunas?‚Äù. Logo, se voc√™ deseja manter, por exemplo, as linhas de sua tabela principal (tabela destinat√°ria), que √© o que ocorre na maioria das vezes, voc√™ sabe agora que voc√™ precisa utilizar um left join.\nEm uma representa√ß√£o visual, podemos reproduzir abaixo a imagem inicial desse artigo, que marca as linhas mantidas por cada um desses dois tipos de join."
  },
  {
    "objectID": "posts/2023/2023-11-18-python-parser/en/index.html",
    "href": "posts/2023/2023-11-18-python-parser/en/index.html",
    "title": "Developing a parser for Python with Python",
    "section": "",
    "text": "Me and my team are currently working in a massive migration (similar to a cloud migration). This migration involves many process, but one of them is to redirect every table reference that we find in more than 130 thousand lines of Python code.\nHowever, this task proved to be so complex that I had to develop a small parser for Python expressions. In this article, I want to use this experience to introduce the subject of parsing expressions to beginners."
  },
  {
    "objectID": "posts/2023/2023-11-18-python-parser/en/index.html#the-quick-and-dirty-approach",
    "href": "posts/2023/2023-11-18-python-parser/en/index.html#the-quick-and-dirty-approach",
    "title": "Developing a parser for Python with Python",
    "section": "2.1 The quick and dirty approach",
    "text": "2.1 The quick and dirty approach\nThis does not look so bad, right? I mean, considering the above example, I could just use a simple REGEX (regular expression) to find the places where I have an spark.table() call, capture the reference given as input, alter it to the new reference, and replace the text with the new reference.\nThis approach would involve some code similar to this:\n\nimport re\nnotebook_content = '''from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\ndf = spark.table(\"blip.events\")\ndf.show()\n'''\n\nspark_table_regex = r'spark[.]table[(][a-zA-Z0-9.\\'\\\"]+[)]'\nnew_table_call = \"spark.table(\\\"platform.blipraw.events\\\")\"\nnew_notebook_content = re.sub(spark_table_regex, new_table_call, notebook_content)\nprint(new_notebook_content)\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\ndf = spark.table(\"platform.blipraw.events\")\ndf.show()"
  },
  {
    "objectID": "posts/2023/2023-11-18-python-parser/en/index.html#sec-challenge",
    "href": "posts/2023/2023-11-18-python-parser/en/index.html#sec-challenge",
    "title": "Developing a parser for Python with Python",
    "section": "2.2 The size of the challenge",
    "text": "2.2 The size of the challenge\nIt would be great if it was that simple, but unfortunately, it is not. What makes this problem so challenging is that the table reference used in spark.table() appears in too many different formats across the 130 thousand lines in our codebase. For example, we might use a formatted string to actually compute the table reference:\ndatabase = \"blip\"\ntable_name = \"events\"\ndf = spark.table(f\"{database}.{table_name}\")\nOr maybe, we call a variable that contains the computed reference:\ntable_ref = database + '.' + table_name\ndf = spark.table(table_ref)\nThese two examples demonstrates that too much variation exists in the use of a table reference. So much variation, that using multiple REGEX‚Äôs to solve this problem would be impractical, and probably too much complex.\nHere is where the parser comes into place."
  },
  {
    "objectID": "posts/2023/2023-11-18-python-parser/en/index.html#what-is-a-parser",
    "href": "posts/2023/2023-11-18-python-parser/en/index.html#what-is-a-parser",
    "title": "Developing a parser for Python with Python",
    "section": "3.1 What is a parser?",
    "text": "3.1 What is a parser?\nParsers (or the process of parsing expressions) are core components of every existing compiler, like gcc or rustc, as well as the R and Python compilers. In essence, a parser is a piece of software that analyzes expressions following the rules of a grammar. A parser is the main part of compilers responsible for analyzing and comprehend the structure of your source code.\nThe process of parsing is usually made in two steps, which are: 1) breaking (or ‚Äúsplitting‚Äù) the input expression into smaller pieces, building a list of tokens, or a list of small components; 2) analyzing this sequence of tokens to build a tree that is equivalent to the input expression. The first step above is usually made by a component called lexer or tokenizer (both names are commom to find), and the second step is made by the parser itself.\nBasically, the process of parsing takes a string (which contains the expression, or the source code your want to parse) as input. Then, the lexer (or tokenizer) breaks the input string into smaller pieces, which are usually called tokens. Then, the parser receives this stream of tokens produced by the tokenizer as input, and starts to analyze this sequence of tokens, to understand the structure of the input expression (or source code). As output, the parser produces a tree that is equivalent to the input expression, which is usually called abstract syntax tree (or AST for short). Figure¬†1 below exposes this process.\n\n\n\n\n\n\nFigure¬†1: The process of parsing\n\n\n\nSo the process of parsing takes an expression as input, and builds a tree that is equivalent to that expression as output. This process of parsing is always one of the first operations that a compiler performs.\nBecause trees are a much more suitable and efficient data structure for the different tasks a compiler performs such as: type and syntax checking, evaluating the result of expressions and assignments, or compiling the input tree into machine code to be executed.\nProbably, the most important data structures for every compiler are trees and stacks."
  },
  {
    "objectID": "posts/2023/2023-11-18-python-parser/en/index.html#sec-kind",
    "href": "posts/2023/2023-11-18-python-parser/en/index.html#sec-kind",
    "title": "Developing a parser for Python with Python",
    "section": "3.2 What kind of expressions we want to parse?",
    "text": "3.2 What kind of expressions we want to parse?\nJust to be clear, we neither need (or want) to develop a complete parser capable of parsing every expression in the Python language. That would be a much larger task, that would involve a great amount of effort. We just want to build a small parser capable of parsing a very small and specific subset of Python expressions.\nWe are particularly interested in the expressions that are related to the table references that we find in our codebase. We already showed examples of these expressions at Section¬†2 and Section¬†2.2.\nBut just to state clearly, we want to build a parser capable of parsing:\n\nexpressions that involves only string constants (other types of constants or structures such as lists, integers, booleans are not important for us, so let‚Äôs ignore them). Example: \"platform.blipraw.events\".\nexpressions that concatenate strings with the plus operator. Example: \"blip\" + \".\" + \"events\".\nexpressions that contains identifiers (that is, variable names). Example: database + \".\" + table_name.\nformatted strings which contain expressions that fit the above cases. Example: f\"{database}.{table_name}\".\n\nLets store these examples of expressions inside a list that we can easily access:\n\nEXPRESSION_EXAMPLES = [\n    \"'platform.blipraw.events'\",\n    '\"blip\" + \".\" + \"events\"',\n    \"database + \\\".\\\" + table_name\",\n    'f\"{database}.{table_name}\"'\n]"
  },
  {
    "objectID": "posts/2023/2023-11-18-python-parser/en/index.html#building-the-lexer-or-tokenizer",
    "href": "posts/2023/2023-11-18-python-parser/en/index.html#building-the-lexer-or-tokenizer",
    "title": "Developing a parser for Python with Python",
    "section": "3.3 Building the Lexer (or Tokenizer)",
    "text": "3.3 Building the Lexer (or Tokenizer)\nLets begin by building a lexer (or tokenizer) for our parser. Just be aware that, from now on, I will use the term tokenizer instead of lexer (this is just a personal preference).\nBut how can we split our input string into small pieces? Well, there are different approaches to do this. However, one particular approach that fit‚Äôs perfectly our example here is to iterate through the characters of our input string, and look for single characters that represent ‚Äúelegible break points‚Äù, or points where we can split the string.\nThis approach is probably the easiest of all to implement, and it fit‚Äôs perfectly our example here because we are interested in parsing just a very small subset of simple Python expressions. If we wanted to parse more complex expressions, then, it would probably be better to use another approach to break the input string.\nSo, the tokenizer will iterate through each character in the input string, and will mark any place that contains a single character the we interpret as an elegible place to break the string. Considering the type of expressions we stated at Section¬†3.2, the characters \", ', +, {, } are good candidates for ‚Äúelegible break points‚Äù. Also, the character f is important for identifying formatted strings, as a consequence, he is also a good candidate.\nLet‚Äôs consider the following tokenizer:\n\n\n\nListing¬†3: The function that represents our tokenizer\n\n\nfrom typing import List\nimport re\n\nis_not_blank = lambda x: x != \"\" and not re.search(r\"^ +$\", x)\ndef tokenizer(input_string: str) -&gt; List[str]:\n    candidates = [\"'\", '\"', '+', '{', '}']\n    break_points = list()\n    for i in range(len(input_string)):\n        current_char = input_string[i]\n        if current_char in candidates:\n            break_points.append(i)\n        if current_char == \"f\" and (i + 1) &lt; len(input_string):\n            next_char = input_string[i + 1]\n            if next_char in ['\"', \"'\"]:\n                break_points.append(i)\n\n    if len(break_points) == 0:\n        return [input_string]\n\n    tokens = list()\n    last_index = 0\n    for index in break_points:\n        tokens.append(input_string[last_index:index])\n        tokens.append(input_string[index:(index+1)])\n        last_index = index + 1\n\n    tokens.append(input_string[last_index:])\n    return list(filter(is_not_blank, tokens))\n\n\n\n\nIf current character is f check if the next character is the beginning of a string (characters \" and '), if it is the beginning of a string, then, it is a formatted string and should be included in the ‚Äúbreak points‚Äù. If the next character is not the beginning of a string, then, we should not consider it as an elegible breakpoint, because it probably is just a letter ‚Äúf‚Äù inside a variable name, such as platform_database.\nIf no break point position was found, then, the input expression is likely an expression with a single component. For example, a single string constant (e.g.¬†\"blip.events\") or a single variable name (events_table). In this case, the tokenizer should return this single component itself as the only token present in the input string.\nEvery iteration of the loop generates two different tokens, which are: 1) a token with the part of the string from the previous break point index until the current elegible break point index; 2) and another token containing the single character that identifies the current elegible breakpoint. For example, the text database\" will generate the break point index 8, so, in the first iteration of the loop, the tokens 'database' and '\"' will be generated.\nEmpty tokens (i.e.¬†tokens that are empty strings, or, that contains only spaces) can be generated during the process. So we use filter() with a lambda function to eliminate them from the output.\n\nThis tokenizer() function generates a list of tokens to be analyzed by the parser:\n\nfor example in EXPRESSION_EXAMPLES:\n    tokens = tokenizer(example)\n    print(\"================================================================\")\n    print(\"  * Input expression: \", example)\n    print(\"  * Tokens produced: \", tokens)\n\n================================================================\n  * Input expression:  'platform.blipraw.events'\n  * Tokens produced:  [\"'\", 'platform.blipraw.events', \"'\"]\n================================================================\n  * Input expression:  \"blip\" + \".\" + \"events\"\n  * Tokens produced:  ['\"', 'blip', '\"', '+', '\"', '.', '\"', '+', '\"', 'events', '\"']\n================================================================\n  * Input expression:  database + \".\" + table_name\n  * Tokens produced:  ['database ', '+', '\"', '.', '\"', '+', ' table_name']\n================================================================\n  * Input expression:  f\"{database}.{table_name}\"\n  * Tokens produced:  ['f', '\"', '{', 'database', '}', '.', '{', 'table_name', '}', '\"']"
  },
  {
    "objectID": "posts/2023/2023-11-18-python-parser/en/index.html#building-the-actual-parser",
    "href": "posts/2023/2023-11-18-python-parser/en/index.html#building-the-actual-parser",
    "title": "Developing a parser for Python with Python",
    "section": "3.4 Building the actual parser",
    "text": "3.4 Building the actual parser\nBuilding the actual parser is definitely the hard part. Because a parser usually involves: 1) having a variable to store the current state of the parser, or, the current state of the AST; 2) and also some level of recursion to traverse the AST, or to decide which move or production rule should be applied. Making these two components working well together can be difficult depending on how you implement it.\n\n3.4.1 The different types of strategies\nRemember, the end goal of a parser, is to build an abstract syntax tree (AST) by analizing a sequence of tokens. There are different strategies to build this tree, and they are usually divided into two categories: 1) top-down. These are strategies that builds the tree from the root, and work they way to the bottom of the tree, which contains the ‚Äúleaves of the tree‚Äù; 2) bottom-up (the inverse). These are strategies which starts to build the tree from the leaves, and work they way up until it hits the root of the tree.\nIn the subject of parsing, top-down strategies are the most popular (notice that this does not mean that bottom-up strategies are bad or not commom). The most popular strategy of all is the Recursive Descent, which is a top-down strategy. But I will not describe these strategy here, specially to avoid all the technical jargon that they bring with them. Anyway, just be aware that there are different strategies out there, and that in this article I‚Äôm only presenting one of them.\n\n\n3.4.2 The smaller components of a parser\nEvery parser usually involves two smaller components: 1) a cache that holds the current state of the parser. This cache stores variables that are essential to the parsing process, such as the current state of the AST builded by the parser; 2) and a control (or ‚Äúdecision-making‚Äù) mechanism, which is usually a collection of methods or functions that decides which move the parser should make considering it‚Äôs current state (Gries and Schneider (2008)).\nSome degree of recursion is found very frequently on some of these methods or functions that decides which move the parser should make. Although this recursion is not something mandatory, because there are some available strategies to build the AST that do not involves any recursion at all.\n\n\n3.4.3 The cache to store the current parser‚Äôs state\nLets build a Python class to represent the cache of the parser. This ParserCache class will be responsible for storing all variables that are vital to the parsing process. These variables are:\n\nast: stores the current state of the AST.\ntokens: stores the full sequence of tokens that the parser needs to analyze.\nindex: stores the index on the sequence of tokens that the parser is currently on.\ntoken: stores a copy of the current token that the parser is analyzing at the moment.\n\nIt is not really vital to have the token variable, because we could easily access the current token by using tokens[index]. But‚Ä¶ having a variable called token is easier to type, and it makes most of the parser‚Äôs code more readable, so, it is woth it.\nWhile the parser is analyzing the sequence of tokens stored at the tokens variable, he will slowly grow (or complete) the AST, by replacing and adding elements to the ast variable. At the end, when the parser finishs the parsing process, the ast variable will store the complete AST that is equivalent to the input expression.\nYou can see below at the __init__() method that to initialize a new object of class ParserCache, you need to give the sequence of tokens to analyze as input. After that, all variables of the class are initialized, and you can start to use the two methods of the class, which are len(), current_token() and next_token(). The len() method returns the length of the tokens sequence that the parser is analyzing, current_token() method returns the current token that the parser is analyzing at the moment, and the next_token() method will effectively advance the parser to the next token in the sequence.\n\nclass ParserCache:\n    '''Class that represents a cache to store the current state of the parser.'''\n    def __init__(self, tokens: List[str]) -&gt; None:\n        self.ast = list()\n        self.tokens = tokens\n        self.index = 0\n        if len(tokens) &gt; 0:\n            self.token = tokens[self.index]\n        else:\n            self.token = None\n        return\n\n    def len(self) -&gt; int:\n        return len(self.tokens)\n\n    def current_token(self) -&gt; str:\n        return self.token\n\n    def current_index(self) -&gt; int:\n        return self.index\n\n    def next_token(self) -&gt; None:\n        self.index += 1\n        if self.index &lt; len(self.tokens):\n            self.token = self.tokens[self.index]\n        return\n\n\n\n3.4.4 The main function or entrypoint for the parser\nWe need a main function for the parser, and that is the parse() function. In other words, this is the public API or the main entrypoint of the parser. That is the function that we should use to effectively parse any input and get the resulting AST.\nYou can see below that all of this function does is: get the input string; generate the tokens with tokenizer(); initialize the parser‚Äôs cache with these tokens; and then, it calls a private function called _parse_input() to initiate the parsing process.\nOnce _parse_input() is done with the parsing process, parse() gets the parsing results and simply return the complete AST produced, which is stored in the ast variable of the parser‚Äôs cache.\n\nfrom typing import Dict, Any\ndef parse(input_string: str) -&gt; List[Dict[str, Any]]:\n    tokens = tokenizer(input_string)\n    parser_cache = ParserCache(tokens)\n    parsing_result = _parse_input(parser_cache)\n    return parsing_result.ast\n\n\n\n3.4.5 Building the main logic of the parser\nThe true magic starts with the private function _parse_input(). Basically, _parse_input() starts by looking at the current of token in the sequence that is being analyzed.\nIf this current token represents the beginning of a string (characters \" and '), _parse_string() is called to effectively parse the next tokens in the sequence, which (very likely) are the contents of that string. If this current token is f, then, we have the beginning of a formatted string, so we just skip this f token with parse_cache.next_token(), and call _parse_string() to parse the formatted string. But if this token is a plus sign, then, he will call _parse_addition() to parse this token. If the current token is neither the beginning of a string or a plus sign, then, _parse_input() will assume that this current token is an identifier, and he will call _parse_identifier() to parse it.\nNotice that all of these three parsing functions (_parse_string(), _parse_addition() and _parse_identifier()) returns the ParserCache object back as output. In other words, these functions receives the parser cache as input, they use this cache to parse the current token, and they add new elements (or replace the existing ones) to the AST, which is stored inside this cache object. Once they finished their businesses, they simply return this cache back as their output.\nAfter we know that we parsed the current token, then, we ask the parser to advance to the next token, with next_token() method. Then, we check if there is really a next token in the sequence to parse/analyze, by checking if the current index is greater than the number of tokens in the sequence.\nIf there are still some remaining tokens in the sequence to analyze, then, _parse_input() will call itself recursively to parse the remaining tokens in the sequence. However, if there is not remaining tokens to analyze anymore, then, the function finally returns the parser cache (which contains the complete AST inside of the ast variable) as output.\n\ndef _parse_input(parser_cache: ParserCache) -&gt; ParserCache:\n1    if parser_cache.len() == 0:\n        return parser_cache\n\n    if parser_cache.current_token() in ['\"', \"'\"]:\n        parser_cache = _parse_string(parser_cache)\n    elif parser_cache.current_token() == 'f':\n        parser_cache.next_token()\n        parser_cache = _parse_string(parser_cache)\n    elif parser_cache.current_token() == '+':\n        parser_cache = _parse_addition(parser_cache)\n    else:\n        parser_cache = _parse_identifier(parser_cache)\n\n    parser_cache.next_token()\n    if parser_cache.current_index() &lt;= parser_cache.len() - 1:\n        parser_cache = _parse_input(parser_cache)\n\n    return parser_cache\n\n\n1\n\nIf the sequence of tokens is empty, then, we have no tokens to analyze, and the function should simply return right away.\n\n\n\n\nYes! I know what you are thinking. If the current token is neither the beginning of a string or a plus sign, then, he could be anything else. For example, he could be the beginning of a function call, or, a logic comparison, or it could be the beginning of an assignment expression. Anyway, it could be many things! So in theory, it is not safe to simply assume that it is an identifier.\nHowever, you need to remember what we proposed at Section¬†3.2. We want to parse only a very small subset of Python expressions. We do not want (or care) to parse other kinds of expressions. If you look at the expressions examples that fitted the description in Section¬†3.2, you will see that the only components that appears in those expressions are identifiers, additions and string constants.\nThat is why we care only about these components. That is why, for this specific case, it is safe to assume that the last option has to be an identifier.\n\n\n3.4.6 Parsing identifiers\nLets begin by discussing what the _parse_identifier() function should do. Because parsing an identifier component is by far the simplest of all three components. I mean‚Ä¶ how would you parse an identifier? To answer this question, is probably best to think what an identifier is.\nWell, an identifier is just a name to a variable, right? So, the value of an identifier object should be the identifier itself! In other words, the name of the variable is the identifier itself. So why not just simply add to the AST an object of type IDENTIFIER and the value of that object being the current token which is the identifier (or the variable name) itself? Right? That should be sufficient.\nSo we have the following body for _parse_identifier():\n\ndef _parse_identifier(parser_cache: ParserCache) -&gt; ParserCache:\n    parser_cache.ast.append({\n        'type': 'IDENTIFIER',\n        'value': str(parser_cache.current_token())\n    })\n    return parser_cache\n\n\n\n3.4.7 Parsing strings\nNow, things get worse when it is time to parse strings. Not because strings are complicated, but because we need to account for formatted strings. In other words, the string that we analyzing might contain expressions inside of it. And if it does have them, we need to parse these expressions in a separate process.\nHowever, if you come back and look at the body of _parse_input() you will see that we call _parse_string() right after we encounter a \" or ' character. But we do not need to parse this single character exactly. We just know that these characters delimit the beginning of a string, so everything that goes after it should be tokens that together composes the actual content of the string. That is why, we start _parse_string() by calling the next_token() method, to skip this \" or ' character that we are not particularly interested in.\nNow, after that, the next tokens in the sequence will very likely represent the contents of this string that we are currently parsing. So all we need to do, is to use a loop to iterate trough these next tokens and stop (or break) the loop in the moment that we hit a token that contains the \" or ' character, because these tokens will represent the end of the string.\nWe could use a stack to accumulate the tokens that represents the contents of the string while we are looping, and then, we analyze these tokens separately after we break the loop. Then, we could simply append to the AST a new object of type STRING containing this stack with all of the components of this string element. It would be a fair strategy. But it would also make the parsing of subexpressions inside of formatted strings harder.\nAs a result, I decided to opt for a placeholder strategy. Before we start to loop through the sequence of tokens that represents the contents of the string, we add a placeholder at the top level of the AST. At each iteration of the while loop we update the value field of this placeholder by adding the current token to it. By doing this, at the end of the while loop we will have a string object at the top of the AST, and its value field will contain the full/complete list of contents of that string.\nNow‚Ä¶ if we do find a opening bracket ({) inside the string, that likely means that the current string that we are analyzing now is a formatted string, and whatever sequence of tokens that is right after this opening bracket, they are a new expression to be parsed. That is why we call the _parse_formatted_string() in case we do find an opening bracket inside the string.\n\ndef _parse_string(parser_cache: ParserCache) -&gt; ParserCache:\n1    char_that_opens_the_string = parser_cache.current_token()\n    parser_cache.next_token()\n    # Add a placeholder in the top of the AST\n    parser_cache.ast.append({\n        'type': 'STRING',\n        'value': list()\n    })\n\n    while parser_cache.current_index() &lt; parser_cache.len() - 1:\n        if parser_cache.current_token() == char_that_opens_the_string:\n            break\n        if parser_cache.current_token() == '{':\n            parser_cache.next_token()\n            parser_cache = _parse_formatted_string(parser_cache)\n            continue\n        \n        elem_ref = parser_cache.ast[-1]\n        elem_ref['value'].append(parser_cache.current_token())\n        parser_cache.next_token()\n    \n    return parser_cache\n\n\n1\n\nWe copy the character (' or \") that openned the string that we are analyzing now, because then, we can search for a new occurence of this same character in the tokens sequence to detect the end of the string.\n\n\n\n\n\n\n\n3.4.8 Parsing expressions inside formatted strings\nWe call _parse_formatted_string() if we find a { character inside sequence of tokens that are inside a string. Inside the pair of brackets ({}) we will always have an expression. And because this is a new expression, we need to parse it too, in a separate process.\nFirst, we use a loop and a stack object to accumulate all the tokens in the sequence that are inside of the pair of brackets ({}). This way, we have inside the stack object all the tokens that represents the subexpression that is inside of the brackets that we need to parse.\n\ndef _parse_formatted_string(parser_cache: ParserCache) -&gt; ParserCache:\n    stack = list()\n    while parser_cache.current_index() &lt; parser_cache.len() - 1:\n1        if parser_cache.current_token() == '}':\n            parser_cache.next_token()\n            break\n        stack.append(parser_cache.current_token())\n        parser_cache.next_token()\n\n    parsed_subexpression = ParserCache(stack)\n    parsed_subexpression = _parse_input(parsed_subexpression)\n    elem_ref = parser_cache.ast[-1]\n    elem_ref['value'].append({\n        'type': 'EXPR',\n        'value': parsed_subexpression.ast\n    })\n    return parser_cache\n\n\n1\n\nWe break the loop, at the moment that we find a closing bracket in the sequence of tokens, because this character represents the end of the pair of brackets that contains the subexpression that we are analyzing.\n\n\n\n\nAfter we accumulated the sequence of tokens that represents the subexpression that we are analyzing, we need to parse this subexpression separately. To do that, we create a new ParserCache object with this sequence of tokens that represents this subexpression, or, in other words, with all of the tokens that are between the openning and closing brackets. Then, we just call the _parse_input() over this expression to effectively parse this expression.\nAfter that, we add to the top of the AST a new expression object (EXPR) that contains the AST of parsed subexpression inside of it.\n\n\n3.4.9 Parsing addition operations\nNow, things get a little bit more complicated when we find a plus sign in the middle of the sequence of tokens. Because if we do find a plus sign, that means that this plus sign is surrounded by operands in the original expression. In other words, this plus sign connects the left operand to the right operand of this addition operation.\nIn theory, the previous token in the sequence is the left operand of this addition, and the next token in the sequence is the right operand. That means that, just in theory, at the moment we find a plus sign in the sequence of tokens, we can collect the left operand of the addition operation by looking at the previous token, and, we could also collect the right operand by advancing the parser with next_token() and using current_token() to get the next token in the sequence.\n\n3.4.9.1 The challenge\nHowever, this is not really true, or, it would be too easy to be true. The real problem, is that the left and right operands could be splitten by the tokenizer into multiple tokens. In other words, there are very commom cases where single calls to current_token() would be insufficient to actually get the complete right or left operands.\nThat means that the tokenizer step might make the part of identifying where the left and right operands are in the sequence of tokens a difficult task. If you think about how the tokenizer works (see Listing¬†3), you will probably be able to find examples where the left or right operands could be composed of multiple tokens, and not just a single token.\nTake the expression database + '.events' as an example. Take a look at the tokenizer‚Äôs output over this expression. The left operand is the first token (database), but the right operand, is actually the three last tokens in the sequence (['\"', '.events', '\"']). This means that these three tokens together compose the right operand.\n\nprint(tokenizer('database + \".events\"'))\n\n['database ', '+', '\"', '.events', '\"']\n\n\nAs a result, at the moment where the parser is at the plus sign, if we advance the parser with next_token(), and run current_token() we would get only the token \", so the right operand would be incomplete. How do we know that we need to accumulate the next three tokens instead of looking to just one token in the sequence?\n\n\n3.4.9.2 Solving the issue for the left operand\nFor now, lets ignore the right operand and think about the left operand. The left operand is whatever sequence of tokens that are before the plus sign. But, as we saw in the previous section, in some cases, we might need to look 3, 4 or 5 tokens behind to actually get the full left operand, instead of looking at just 1 token behind.\nBut instead of looking at the previous tokens in the sequence, we could get the left operand by looking at the top of the AST. In other words, the last element in the AST is the previous token already parsed, and we can easily access this last element by calling parser_cache.ast[-1].\nPerfect, after we saved a copy of the left operand inside a left_operand variable, we can delete him from the AST in the parser cache, with del parser_cache.ast[-1]. If we do not delete him now, then, he would become duplicated in the AST at the moment that we add our object for the addition operation to the AST, which contains both left and right operands.\n\ndef _parse_addition(parser_cache: ParserCache) -&gt; ParserCache:\n    left_operand = parser_cache.ast[-1]\n    del parser_cache.ast[-1]\n    # Now that we managed the left operando, we will deal\n    # with the right operand in the next section\n    return parser_cache\n\n\n\n3.4.9.3 Solving the issue for the right operand\nAfter we deleted the last element in the AST (which contained the left operand), we can now look for the right operand, which is whatever sequence of tokens that are after the plus sign.\nBut getting the right operand is unfortunately more complicated. Because of that, let‚Äôs concentrate the main logic of getting the right operand into a new separate function called _parse_right_operand(). After we get the right operand, we just return from _parse_addition() with an ADDITION object which contains both left and right operands.\n\ndef _parse_addition(parser_cache: ParserCache) -&gt; ParserCache:\n    left_operand = parser_cache.ast[-1]\n    del parser_cache.ast[-1]\n    parser_cache.next_token()\n    right_operand = _parse_right_operand(parser_cache)\n    parser_cache.ast.append({\n        'type': 'ADDITION',\n        'left_operand': left_operand,\n        'right_operand': right_operand\n    })\n    return parser_cache\n\nNow, what _parse_right_operand() should do? I think it should iterate through the sequence of tokens that is after the plus sign, parse this sequence of tokens and then, return the parsed AST of this sequence. That is basically what _parse_right_operand() is doing in the function definition below.\nWe start with a while loop which will use a new stack object to accumulate all the tokens after the plus sign. This loop stops in two situations:\n\nIf it hits the end of the stream of tokens that the parser is analyzing.\nIf it hits a new plus sign in the sequence of tokens, which basically means that, the loop found a new addition operation in the sequence of tokens.\n\nWhen the loop stops in situation 1, then, the stack object will contain all of the tokens that represents the right operand. Having that in mind, all we need to do is to create a new temporary ParserCache object with these tokens, and call _parse_input() over it to parse the right operand. Then, _parse_right_operand() will simply return the parsed AST from this operation.\nIn the other side, if the loop stops in situation 2, that means that, the right operand that we are currently trying to parse, is in fact, the left hand to a new addition operation. In other words, the tokens that we accumulated in the stack object until this very moment, are in fact, the tokens that represents the left operand of this new addition operation that we found.\nAt Figure¬†2 we can see this idea visually. Everything that is marked by the light red rectangle is the left operand of that particular addition operation, and everything that is marked by the dark blue rectangle is the right operand of that particular addition operation.\n\n\n\n\n\n\nFigure¬†2: Marking the right and left operands in each addition operation\n\n\n\nSo, lets consider as an example, that the parser was parsing the expression database + '.' + 'events'. When _parse_right_operand() is parsing the right operand of the first addition (which is '.'), the function is looping through the sequence of tokens. The tokens [\"'\", '.', \"'\"] are accumulated inside the stack object, until the loop hits the token +, which is second plus sign in the entire input expression. At this point the function understands that it hitted a new addition operation to be parsed.\nThat is why _parse_right_operand(), in this situation, will call itself recursively to parse the remaining tokens in the sequence, which are the right operand of this new/second addition it founded. The tokens that are accumulated in the stack object, are the left operand of this new/second addition, and they are parsed by _parse_input().\nSo in this situation, to get the actual right operand of the first addition, _parse_right_operand() needs to parse this new/second addition operation that if founded. The right operand of the first addition, is the second addition in its entirety, or, you could also interpret that the right operand of the first addition is the second addition already parsed. That is why, in this situation, _parse_right_operand() will call both _parse_input() and itself recursively to parse the right and left operands of this new/second addition operation, and return the ADDITION object that represents this parsed new/second addition.\nHaving all of these aspects in mind, here is the source code for _parse_right_operand():\n\ndef _parse_right_operand(parser_cache: ParserCache) -&gt; ParserCache:\n    stack = list()\n    while parser_cache.current_index() &lt;= parser_cache.len() - 1:\n        if parser_cache.current_token() == \"+\":\n            parser_cache.next_token()\n            second_addition_right_operand = _parse_right_operand(parser_cache)\n            temp_parse_cache = ParserCache(stack)\n            second_addition_left_operand = _parse_input(temp_parse_cache)\n            return {\n                'type': 'ADDITION',\n                'left_operand': second_addition_left_operand.ast[0],\n                'right_operand': second_addition_right_operand\n            }\n\n        stack.append(parser_cache.current_token())\n        parser_cache.next_token()\n    \n    temp_parse_cache = ParserCache(stack)\n    parsed_right_operand = _parse_input(temp_parse_cache)\n    return parsed_right_operand.ast[0]"
  },
  {
    "objectID": "posts/2023/2023-11-18-python-parser/en/index.html#finally-the-parser-is-complete",
    "href": "posts/2023/2023-11-18-python-parser/en/index.html#finally-the-parser-is-complete",
    "title": "Developing a parser for Python with Python",
    "section": "3.5 Finally! The parser is complete!",
    "text": "3.5 Finally! The parser is complete!\nNow our parser is finally complete! We can now test it, and see what AST‚Äôs are produced for each example of expressions we have:\n\nfor example in EXPRESSION_EXAMPLES:\n    parsed_expr = parse(example)\n    print(\"====================================\")\n    print(\"  * Input expression: \", example)\n    print(\"  * Parsed AST: \", parsed_expr)\n\n====================================\n  * Input expression:  'platform.blipraw.events'\n  * Parsed AST:  [{'type': 'STRING', 'value': ['platform.blipraw.events']}]\n====================================\n  * Input expression:  \"blip\" + \".\" + \"events\"\n  * Parsed AST:  [{'type': 'ADDITION', 'left_operand': {'type': 'STRING', 'value': ['blip']}, 'right_operand': {'type': 'ADDITION', 'left_operand': {'type': 'STRING', 'value': ['.']}, 'right_operand': {'type': 'STRING', 'value': ['events']}}}]\n====================================\n  * Input expression:  database + \".\" + table_name\n  * Parsed AST:  [{'type': 'ADDITION', 'left_operand': {'type': 'IDENTIFIER', 'value': 'database '}, 'right_operand': {'type': 'ADDITION', 'left_operand': {'type': 'STRING', 'value': ['.']}, 'right_operand': {'type': 'IDENTIFIER', 'value': ' table_name'}}}]\n====================================\n  * Input expression:  f\"{database}.{table_name}\"\n  * Parsed AST:  [{'type': 'STRING', 'value': [{'type': 'EXPR', 'value': [{'type': 'IDENTIFIER', 'value': 'database'}]}, '.', {'type': 'EXPR', 'value': [{'type': 'IDENTIFIER', 'value': 'table_name'}]}]}]\n\n\nYou can see in the above output, that parser I presented here produces an AST which is composed by a list of dict (or JSON) objects which describes the elements of the three. Now that we have this three, we might do a lot of different things with it. As I mentioned before, every compiler uses trees like this to do type checking, to identify dependencies of your source code, and also, to perform optimization processes.\nBut these operations are out of the scope of this article. Our objective here is complete, which was to show a basic example of a parser written in Python. If you want to take a closer look, you can see the full source code of the parser at parser.py."
  },
  {
    "objectID": "posts/2023/2023-03-06-storytelling/pt/index.html",
    "href": "posts/2023/2023-03-06-storytelling/pt/index.html",
    "title": "Um estudo de caso sobre Data Storytelling em alguns dos maiores bancos brasileiros",
    "section": "",
    "text": "Este artigo discute como eu melhorei o meu processo de Data Storytelling dentro das minhas apresenta√ß√µes e relat√≥rios entregues para dois dos maiores bancos comerciais do mercado brasileiro. Busco compartilhar o que aprendi nesse processo, e dou algumas dicas que possivelmente v√£o ajud√°-lo a construir relat√≥rios de dados mais intuitivos, cativantes, claros e efetivos.\nEm resumo, vamos discutir neste artigo as seguintes dicas:\n\nData Storytelling √© sobre contar hist√≥rias atrav√©s de dados;\nHist√≥rias tem estrutura, use essas estruturas ao seu favor;\nEvite dividir muito a aten√ß√£o com textos longos em seus slides;\nTrace rela√ß√µes entre os seus dados;\nConstrua sua hist√≥ria em torno de uma mensagem principal;\nEntregue sua hist√≥ria aos poucos;\n\nA maior parte desses conhecimentos foram constru√≠dos atrav√©s de intensa pesquisa, reflex√£o e planejamento sobre as minhas apresenta√ß√µes, e, posteriormente, adquirindo feedbacks, e realizando pequenos ajustes aqui e ali.\nComo voc√™ pode esperar de todo grande trabalho como este, ele tamb√©m envolveu outras pessoas. Durante esse processo, eu recebi ajuda e feedbacks de meus colegas de trabalho, Andressa de Souza Freitas e Guilherme Gomes. Eu tamb√©m tive uma grande ajuda da UX Designer Al√™ Fernandes. V√°rios dos conhecimentos apresentados aqui, eu aprendi na pr√°tica com a Al√™. Esses conhecimentos revolucionaram a maneira como eu construo minhas apresenta√ß√µes, e, por isso, eu sou imensamente grato a ela ‚ù§Ô∏è.\n\n\n\n\n\n\nImportant\n\n\n\nTodos os dados, gr√°ficos e imagens mostrados neste artigo s√£o meramente ilustrativos. Todos os n√∫meros apresentados foram gerados de forma aleat√≥ria por um computador! Portanto, eles n√£o representam os dados reais de TakeBlip ou dos bancos comerciais brasileiros envolvidos de nenhuma forma ou dimens√£o!"
  },
  {
    "objectID": "posts/2023/2023-03-06-storytelling/pt/index.html#sec-guerra-atencao",
    "href": "posts/2023/2023-03-06-storytelling/pt/index.html#sec-guerra-atencao",
    "title": "Um estudo de caso sobre Data Storytelling em alguns dos maiores bancos brasileiros",
    "section": "4.1 Uma guerra constante pela aten√ß√£o",
    "text": "4.1 Uma guerra constante pela aten√ß√£o\nQuando estamos apresentando algo, estamos constantemente batalhando pela aten√ß√£o dos nossos espectadores. Essa √© uma batalha dif√≠cil, n√£o apenas porque n√≥s podemos (sem querer) atrair a aten√ß√£o para os lugares errados, mas tamb√©m porque existem muitas fontes de distra√ß√£o no mundo moderno (e.g.¬†celulares, emails, etc.)!\nUm bom data storytelling depende de voc√™ contar uma hist√≥ria cativante, que consiga capturar a aten√ß√£o de seus espectadores. Por isso, as pr√≥ximas se√ß√µes v√£o focar bastante em dicas que contribuam para essa captura, ou que te ajudam a n√£o dissipar, reduzir ou atrapalhar a aten√ß√£o desses espectadores.\n\n4.1.1 Por que evitar textos longos em seus slides?\nTente ser parsimonioso nos seus slides! Isto √©, tente incluir o m√≠nimo poss√≠vel de informa√ß√£o dentro dele. Se voc√™ precisa repassar v√°rias informa√ß√µes em um slide, tente incorporar o m√°ximo poss√≠vel desse conte√∫do em sua fala, e o m√≠nino poss√≠vel dele em forma escrita neste slide. Em geral, evite ao m√°ximo incluir textos muito longos em seus slides.\n√â estranho pensar nisso, mas geralmente, os gestores v√£o participar e assistir √† sua apresenta√ß√£o porque eles est√£o interessados no que voc√™ tem a dizer sobre o neg√≥cio deles. Portanto, os seus slides s√£o apenas um material de suporte, eles devem ser secund√°rios, um coadjuvante de sua apresenta√ß√£o. Pois a pe√ßa principal da apresenta√ß√£o deve ser sempre a sua fala e a hist√≥ria que voc√™ quer contar atrav√©s dela.\nVeja o slide em Figura¬†2 como exemplo. O problema principal desse slide, √© que ele divide muito a aten√ß√£o de seu espectador.\n\n\n\n\n\n\nFigura¬†2: Slide com um par√°grafo longo\n\n\n\nAo apresentar um slide, os seus espectadores tem que prestar aten√ß√£o na sua fala. Isto √©, no que voc√™ est√° comunicando verbalmente durante a apresenta√ß√£o. E ao mesmo tempo, eles tamb√©m precisam prestar aten√ß√£o no conte√∫do do slide. Contudo, esse longo par√°grafo no canto esquerdo do slide mostrado em Figura¬†2 √© problem√°tico. Pois ele chama aten√ß√£o demais!\nEsse elemento desperta tanto a nossa curiosidade, que ao ver esse slide, voc√™ (leitor deste artigo) provavelmente tentou ler esse texto longo antes mesmo de ler o que estou descrevendo agora neste par√°grafo. O mesmo vai acontecer com os espectadores de sua apresenta√ß√£o. Ou seja, os seus espectadores v√£o imediatamente tentar ler esse par√°grafo longo.\nContudo, ler e interpretar um texto longo, exige certo esfor√ßo e muita aten√ß√£o. Como resultado, enquanto os seus espectadores le√™m esse texto, eles n√£o v√£o conseguir prestar aten√ß√£o em outros elementos de sua apresenta√ß√£o. Por exemplo, na sua fala.\nIsso pode ser crucial, pois talvez voc√™ traga uma informa√ß√£o a mais, ou uma conex√£o extremamente importante na sua fala, e eles podem acabar perdendo isso enquanto est√£o tentando ler esse texto. Portanto, evite ao m√°ximo incluir textos muito longos em seus slides.\n\n\n4.1.2 Entregue sua hist√≥ria aos poucos\nO c√©rebro humano consegue processar uma quantidade limitada de informa√ß√µes de uma vez s√≥. Como resultado, se voc√™ tentar explicar v√°rias informa√ß√µes para os seus espectadores, em um √∫nico slide, eles v√£o acabar atingindo esse limite ü§Ø, e simplesmente n√£o v√£o conseguir raciocinar, compreender ou assimilar o que voc√™ est√° explicando.\nPortanto, entregue a sua hist√≥ria aos poucos. Evite condensar v√°rias informa√ß√µes em um √∫nico slide! Divida o conte√∫do em partes, e explique uma parte de cada vez!\nIsso ajuda a tornar o conte√∫do mais simples, e, como resultado, isso ajuda os seus espectadores a entenderem melhor sobre o que voc√™ est√° falando.\nPense um pouco sobre isso. Quando voc√™ busca aprender sobre um assunto complexo (e.g.¬†regress√£o linear), voc√™ provavelmente divide o conte√∫do em v√°rias partes pequenas, e, vai aprendendo uma parte de cada vez. N√£o √© assim que voc√™ faz? Ent√£o traga essa experi√™ncia tamb√©m para as suas apresenta√ß√µes."
  },
  {
    "objectID": "posts/2023/2023-03-06-storytelling/pt/index.html#sec-historia-conteudo",
    "href": "posts/2023/2023-03-06-storytelling/pt/index.html#sec-historia-conteudo",
    "title": "Um estudo de caso sobre Data Storytelling em alguns dos maiores bancos brasileiros",
    "section": "4.2 Discutindo hist√≥rias e conte√∫do",
    "text": "4.2 Discutindo hist√≥rias e conte√∫do\nAgora que vimos duas regras b√°sicas sobre a aten√ß√£o de seus espectadores (i.e.¬†evite textos longos e entregue sua hist√≥ria aos poucos), vamos discutir algumas dicas sobre o conte√∫do de sua apresenta√ß√£o, e, sobre como construir hist√≥rias com dados.\n\n4.2.1 Data Storytelling n√£o √© sobre escolher ‚Äúo melhor gr√°fico‚Äù\nAlguns poucos analistas entendem ‚Äúdata storytelling‚Äù como um problema de visualiza√ß√£o, ou, como a ci√™ncia de ‚Äúescolher os melhores gr√°ficos‚Äù para sua apresenta√ß√£o, ou como projetar gr√°ficos inovadores, bonitos e complexos.\nContudo, data storytelling √© sobre contar hist√≥rias com dados. N√£o sobre como construir visualiza√ß√µes. Escolher a visualiza√ß√£o certa para apresentar os seus dados, torn√°-la melhor, mais bonita e mais limpa, √© apenas uma parte do processo. Uma parte muito importante, pois isso vai te ajudar a contar sua hist√≥ria de uma maneira mais clara e eficaz, e, com isso, atingir um p√∫blico maior.\n\n\n4.2.2 Trace rela√ß√µes entre os indicadores\nAgora, vamos analisar o slide mostrado em Figura¬†3. Perceba que esse slide, novamente, lembra muito uma p√°gina de um dashboard. O slide n√£o parece t√£o cativante √† primeira vista, pois ele s√≥ mostra os indicadores, ele n√£o constr√≥i uma rela√ß√£o, ou uma hist√≥ria entre eles.\n\n\n\n\n\n\nFigura¬†3: Falta de rela√ß√£o entre os indicadores\n\n\n\nSe prestarmos aten√ß√£o nesses indicadores, podemos identificar alguns efeitos que est√£o acontecendo ao longo deles. E se refletirmos um pouco mais sobre esses efeitos, vamos perceber que esses efeitos s√£o relacion√°veis! E que em conjunto, eles podem contar uma hist√≥ria.\nPor exemplo, perceba que h√° um aumento significativo nas vendas. Tanto no n√∫mero de propostas vendidas quanto no valor total em si que essas propostas geraram. Por√©m, perceba que esse crescimento nas vendas n√£o ocorreu no produto ‚ÄúEmpr√©stimo Consignado‚Äù, e sim, no produto ‚ÄúEmpr√©stimo com garantia de ve√≠culos‚Äù. Ou seja, o produto ‚ÄúEmpr√©stimo Consignado‚Äù teve uma queda de vendas nesse m√™s, por√©m, o produto ‚ÄúEmpr√©stimo com garantia de ve√≠culos‚Äù obteve um super resultado que conseguiu cobrir e muito essa queda, e no fim, conseguiu aumentar as vendas como um todo do banco.\nAl√©m disso, outros efeitos que podemos perceber s√£o os aumentos na taxa de convers√£o e na taxa de sucesso de API. Esses tamb√©m s√£o fatores que contribu√≠ram para o aumento nas vendas. Pois um aumento na taxa de convers√£o significa que uma parcela maior dos nossos clientes est√° adquirindo os nossos produtos. J√° um aumento na taxa de sucesso na API, significa que temos menos erros nos registros das vendas na plataforma, e isso √© obviamente positivo, pois n√≥s temos uma perda de vendas menor por causa de travamentos e erros nesse sistema de registro.\nPerceba que todas essas rela√ß√µes nos ajudam a construir uma hist√≥ria sobre como as vendas aumentaram nesse m√™s, e √© justamente isso que queremos atingir. Portanto, tente sempre construir rela√ß√µes entre os seus indicadores, de modo a formar uma hist√≥ria sobre um resultado principal.\n\n\n4.2.3 Construa sua hist√≥ria em torno de uma mensagem principal\nApenas para deixar claro essa ideia, ao identificar os v√°rios efeitos que descrevemos em Se√ß√£o¬†4.2.2, sobre o slide mostrado em Figura¬†3, √© interessante nos questionarmos: qual desses v√°rios efeitos √© o principal resultado? Em outras palavras, qual desses efeitos √© o que mais interessa os gerentes do banco que est√£o assistindo √† sua apresenta√ß√£o?\nCertamente o aumento sobre as vendas √© o efeito principal. √â o efeito que mais interessa os gerentes que est√£o assistindo √† sua apresenta√ß√£o. Portanto, tente construir a sua hist√≥ria em torno desse resultado, ou dessa mensagem principal. Use os outros indicadores para explicar como esse resultado principal aconteceu.\nIsso tamb√©m √© muito importante! Todo gerente gosta muito de ouvir a palavra ‚Äúaumento nas vendas‚Äù. Por√©m, ele tamb√©m est√° sempre interessado em saber o ‚Äúcomo esse aumento foi gerado?‚Äù. Ou seja, ele precisa saber quais foram as a√ß√µes realizadas que geraram esse impacto positivo.\nPois ao identificar essas a√ß√µes, esse gerente tem a capacidade de aplicar essas a√ß√µes em outras partes de seu neg√≥cio, e, com certa esperan√ßa, ele pode acabar disseminando esse efeito positivo que voc√™ descreveu para outras √°reas, e, como resultado, ele pode acabar aumentando ainda mais as vendas da empresa."
  },
  {
    "objectID": "posts/2023/2023-03-06-storytelling/pt/index.html#o-modelo-dos-quatro-cs",
    "href": "posts/2023/2023-03-06-storytelling/pt/index.html#o-modelo-dos-quatro-cs",
    "title": "Um estudo de caso sobre Data Storytelling em alguns dos maiores bancos brasileiros",
    "section": "6.1 O modelo dos quatro C‚Äôs",
    "text": "6.1 O modelo dos quatro C‚Äôs\nO modelo dos quatro C‚Äôs se refere a estas quatro palavras:\n\nContexto, Conflito, Consequ√™ncia, Conselho.\n\nEstas quatro palavras, te ajudam a trazer uma estrutura consistente para a sua hist√≥ria. Al√©m disso, essas palavras tamb√©m te ajudam a mexer um pouco com as emo√ß√µes de seus espectadores, e, com isso, capturar mais a aten√ß√£o deles.\nAo seguir esse modelo, sua hist√≥ria vai sempre se dividir em quatro partes: contexto, conflito, consequ√™ncia e conselho. Precisamente nesta ordem. Esse estrutura gira em torno de um problema, ou um conflito principal que voc√™ identificou no neg√≥cio de seu cliente.\nO interessante dessa estrutura, √© que voc√™ pode estacar v√°rios problemas em sequ√™ncia. Como resultado, voc√™ tem um bloco de 4 C‚Äôs (contexto, conflito, consequ√™ncia e conselho), seguido de um outro bloco com mais 4 C‚Äôs (contexto, conflito, consequ√™ncia e conselho). Ou tamb√©m, voc√™ pode come√ßar a apresenta√ß√£o por um contexto, e, em seguida, dois blocos de 3 C‚Äôs (conflito, consequ√™ncia e conselho) em sequ√™ncia.\nEnfim, chega de papo, e vamos descrever em mais detalhes cada uma das partes dessa estrutura.\n\n6.1.1 Come√ße por um contexto geral\nPortanto, ao seguir esse modelo, a sua hist√≥ria vai sempre se iniciar por um contexto geral. Algo apenas para contextualizar o espectador sobre qual √© o estado atual do neg√≥cio.\nLembre-se que, o modelo dos 4 C‚Äôs √© constru√≠do em torno de um conflito, ou de um problema central. Tendo isso em mente, se, por exemplo, o conflito que voc√™ for discutir na sua hist√≥ria, for um problema que afeta o servi√ßo de venda de maquininhas de cart√£o, √© importante que voc√™ foque nessa parte do contexto, em dar um overview sobre o produto ‚Äúmaquininhas de cart√£o‚Äù.\nEm outras palavras, evite trazer contextos nesta parte que n√£o est√£o relacionados ao problema/conflito que voc√™ vai apresentar na se√ß√£o seguinte. Pois esse conflito √© a parte central de sua hist√≥ria.\n\n\n6.1.2 Apresente um conflito/problema/desafio a ser superado\nEm seguida, voc√™ deve apresentar um conflito. Isto √©, um problema, um desafio ou uma barreira que voc√™ tenha identificado no produto/servi√ßo que voc√™ est√° analisando. √â aqui que vamos mexer um pouco com a emo√ß√£o do espectador, e usar isso ao nosso favor para capturar a sua aten√ß√£o.\nVamos refletir um pouco sobre isso. Ao colocarmos palavras como ‚Äúproblema‚Äù, ‚Äúcuidado‚Äù, ‚Äúdesafio‚Äù, ‚Äúalerta‚Äù, especialmente em letras garrafais, al√©m de incluir emojis que transmitem esse intuito, como ‚ö†Ô∏è e ‚õî. Isso rapidamente chama a aten√ß√£o de qualquer pessoa, pois te d√° uma sensa√ß√£o de perigo, e voc√™ entra em um estado de alerta.\nSe voc√™ refletir mais um pouco sobre isso, voc√™ provavelmente vai perceber que voc√™ tem esse mesmo instinto quando voc√™ est√° assistindo a um filme, ou a uma s√©rie, e o her√≥i dessa hist√≥ria de repente entra em uma situa√ß√£o de perigo. Voc√™ rapidamente presta mais aten√ß√£o no que est√° acontecendo, pois voc√™ quer ver como o her√≥i vai sair dessa enrascada, ou voc√™ est√° torcendo muito para que ele sobreviva e supere esse problema.\nAo apresentarmos um problema sobre o produto/servi√ßo que voc√™ est√° analisando, estamos querendo causar esse mesmo efeito em nossos espectadores. Ao dizermos que temos um desafio/problema que est√° afetando as vendas da empresa, os gerentes rapidamente come√ßam a prestar mais aten√ß√£o no que voc√™ est√° dizendo, pois eles querem saber como eles podem sair dessa enrascada!\n\n\n6.1.3 Esconder os problemas √© uma p√©ssima ideia\nIsso √© muito importante! Alguns analistas tem medo ou receio de jogar luz sobre os problemas existentes, e, por isso, acabam escodendo eles ou omitindo de suas apresenta√ß√µes. Por√©m, voc√™ n√£o est√° entregando valor nenhum para o seu cliente dessa forma! Voc√™ n√£o est√° ajudando o seu cliente a resolver os problemas e a crescer o neg√≥cio dele!\nE se os problemas n√£o s√£o solucionados, se eles continuam existindo, eles v√£o crescer, e crescer, at√© que eles explodirem, gerando assim um caos generalizado. Em vista disso, quanto mais cedo voc√™ identificar esse problema, avisar os gerentes sobre ele, e apresentar poss√≠veis solu√ß√µes para ele, melhor para os gerentes, que j√° saem com um plano de a√ß√£o para resolver esse problema, e √© melhor para voc√™ tamb√©m, pois estamos entregando valor e solu√ß√µes para o cliente.\nPortanto, uma boa apresenta√ß√£o, ou um bom relat√≥rio de dados, √© aquele que entrega valor para o seu cliente! Ao mostrar novas oportunidades de neg√≥cio (e.g.¬†atingir uma nova parcela do p√∫blico com um produto), e tamb√©m, apresentar solu√ß√µes para problemas atuais que est√£o limitando ou impedindo o crescimento do neg√≥cio.\nPor√©m, uma apresenta√ß√£o que apenas comenta pontos positivos, que fala que est√° tudo bem‚Ä¶ n√£o traz valor nenhum para os gerentes. Os gerentes n√£o te contrataram para falar que est√° tudo bem. Eles te contrataram para que voc√™ ajude eles a descobrir e solucionar problemas no neg√≥cio deles, atrav√©s da an√°lise de dados.\n\n\n6.1.4 Apresente as consequ√™ncias do conflito/problema/desafio que voc√™ identificou\nPortanto, ap√≥s apresentar um conflito/problema/desafio que est√° afetando o neg√≥cio para o seu espectador, √© importante que voc√™ apresente logo em seguida a consequ√™ncia desse problema. Isso ajuda os gerentes a terem uma dimens√£o do tamanho que esse problema representa para o neg√≥cio deles.\nTudo bem se voc√™ n√£o conseguir mensurar em n√∫meros o tamanho do impacto que esse conflito gerou no neg√≥cio. Tente medir esse impacto da melhor forma que voc√™ puder. Um valor aproximado do impacto j√° pode trazer bastante clareza sobre o tamanho do perigo que esse conflito representa para o neg√≥cio de seus espectadores.\nVoc√™ tamb√©m pode fornecer um range, ou um intervalo poss√≠vel do impacto se voc√™ puder (e.g.¬†o impacto estimado est√° entre R$ 20 mil e R$ 340 mil). Essa tamb√©m √© uma forma v√°lida de expor o tamanho do problema.\nCaso for realmente imposs√≠vel de mensurar esse impacto em n√∫meros, ent√£o, explique nesta parte, quais s√£o os pontos do processo de venda do produto/servi√ßo que s√£o afetados por esse problema. Em outras palavras, apresente quais s√£o os lugares do neg√≥cio que est√£o sendo, em teoria, afetados por esse conflito.\n\n\n6.1.5 Aconselhe o seu cliente, apresente poss√≠veis solu√ß√µes para o problema\nOk, apresentamos um problema, ou um conflito para os nossos espectadores. Tamb√©m discutimos os impactos desse conflito sobre o neg√≥cio de nosso cliente. Agora, precisamos apresentar poss√≠veis solu√ß√µes para esse problema.\nPortanto, entenda o problema/conflito que voc√™ est√° apresentando, e tente listar quais seriam as principais solu√ß√µes para esse problema, e inclua essas solu√ß√µes nesta parte de sua apresenta√ß√£o. Vale a pena explicar e discutir esse problema com outros colegas de trabalho tamb√©m, pois eles tamb√©m podem sugerir solu√ß√µes interessantes que estavam fora de seu radar.\n√â interessante tamb√©m incluir uma rela√ß√£o dos trade-offs de cada solu√ß√£o, principalmente em quest√£o de complexidade e esfor√ßo de cada solu√ß√£o. Gerentes est√£o constantemente interessados nessa rela√ß√£o, e querem sempre escolher a solu√ß√£o que seja mais simples e mais r√°pida de ser implementada."
  },
  {
    "objectID": "posts/2023/2023-03-06-storytelling/pt/index.html#evolu√ß√£o-do-personagem-como-uma-outra-alternativa",
    "href": "posts/2023/2023-03-06-storytelling/pt/index.html#evolu√ß√£o-do-personagem-como-uma-outra-alternativa",
    "title": "Um estudo de caso sobre Data Storytelling em alguns dos maiores bancos brasileiros",
    "section": "6.2 Evolu√ß√£o do personagem como uma outra alternativa",
    "text": "6.2 Evolu√ß√£o do personagem como uma outra alternativa\nA evolu√ß√£o do personagem (ou a ‚Äújornada do her√≥i‚Äù) √© uma estrutura de hist√≥ria bem popular. Voc√™ come√ßa por um personagem, ou, um her√≥i para a hist√≥ria, que no nosso caso aqui, pode ser o neg√≥cio do nosso cliente, ou um produto espec√≠fico dele.\nNossa miss√£o √© mostrar como esse personagem/her√≥i evoluiu nos √∫ltimos meses. Por isso, essa estrutura de evolu√ß√£o √© bastante apropriada para relat√≥rios no final do ano. Pois esses relat√≥rios s√£o interessantes para mostrarmos a evolu√ß√£o do neg√≥cio (ou a evolu√ß√£o do personagem) no √∫ltimo ano.\n\n6.2.1 O in√≠cio e o fim da jornada\nSe voc√™ preferir, voc√™ pode come√ßar a sua apresenta√ß√£o apresentando como o seu personagem estava no in√≠cio da jornada, e, terminar essa apresenta√ß√£o mostrando como esse personagem terminou essa jornada.\nEssa √© uma op√ß√£o, por√©m, eu prefiro come√ßar a apresenta√ß√£o mostrando o in√≠cio e o fim ao mesmo tempo. Desse modo, os espectadores j√° come√ßam a apresenta√ß√£o tendo uma no√ß√£o do quanto o neg√≥cio evoluiu durante o ano. Portanto, eu come√ßo apresentando como o personagem (ou o neg√≥cio/produto) come√ßou o ano, e, logo em seguida, como ele terminou o ano.\n\n\n6.2.2 Apresentando os desafios e percal√ßos\nEm todo neg√≥cio/produto, qualquer que ele seja, n√≥s sempre enfrentamos grandes desafios que podem amea√ßar o sucesso, ou, limitar a evolu√ß√£o deste neg√≥cio/produto.\nPortanto, ao longo de qualquer jornada, n√≥s sempre enfrentamos desafios e percal√ßos, e aplicamos a√ß√µes para tentar superar esses desafios. Ao superar esses desafios, n√≥s podemos com certa esperan√ßa gerar a evolu√ß√£o e melhoria desse neg√≥cio/produto. Isto √©, chegar ao final dessa jornada com um neg√≥cio/produto melhor, mais robusto e mais rent√°vel.\nTudo isso significa que o ‚Äúcomo percorremos o meio da jornada‚Äù pode ser muito mais importante/interessante que o in√≠cio ou o final dessa jornada. Sendo assim, reserve uma parte de sua apresenta√ß√£o para apresentar os principais desafios que enfrentamos durante essa travessia, e como superamos eles.\n\n\n6.2.3 Se poss√≠vel, aproveite para valorizar o trabalho de sua equipe\nNesse ponto espec√≠fico da apresenta√ß√£o, voc√™ geralmente tem uma oportunidade muito interessante! Pois voc√™ pode, entregar valor ao seu cliente, e, ao mesmo tempo, tamb√©m valorizar o trabalho de sua equipe, especialmente se foi essa a equipe que descobriu os desafios, e, aplicou as a√ß√µes que superaram esses desafios que voc√™ est√° descrevendo.\nIsso √© uma oportunidade de ouro! Portanto, se voc√™ tiver essa oportunidade em suas m√£os, aproveite ela! Lembre-se que voc√™ n√£o trabalha sozinho. Voc√™ quase sempre est√° trabalhando dentro de uma equipe de pessoas, e √© sempre importante saber como valorizar o trabalho de seus colegas.\nContudo, essa oportunidade nem sempre vai aparecer para voc√™. Talvez os desafios que voc√™ est√° apresentando, foram superados de outra forma, por uma outra equipe que voc√™ n√£o conhece, ou que n√£o possui conex√£o direta.\n\n\n6.2.4 Reforce os resultados alcan√ßados\nTamb√©m √© √∫til terminar a sua apresenta√ß√£o, mostrando um resumo dos resultados alcan√ßados ao longo do ano. Isto pode ser um bulletpoint simples, resumindo os principais resultados. Isso ajuda a refrescar a mem√≥ria de seus espectadores, mostrando a eles n√£o apenas sobre como terminamos a jornada do ano passado, mas tamb√©m, sobre como n√≥s come√ßamos a jornada do pr√≥ximo ano üòâ.\n\n\n6.2.5 Um resumo da estrutura\nPortanto, nas se√ß√µes anteriores discutimos uma ideia de estrutura que seria similar √† uma ‚Äúevolu√ß√£o do personagem‚Äù. No final, temos uma hist√≥ria que segue a seguinte sequ√™ncia:\n\napresente rapidamente como come√ßamos e como terminamos a jornada;\napresente os principais desafios que enfrentamos durante o ano;\nquais foram as solu√ß√µes que aplicamos para solucionar os problemas;\nreforce a evolu√ß√£o do personagem, ao mostrar novamente os resultados atingidos com as solu√ß√µes acima;"
  },
  {
    "objectID": "posts/2023/2023-03-06-storytelling/pt/index.html#footnotes",
    "href": "posts/2023/2023-03-06-storytelling/pt/index.html#footnotes",
    "title": "Um estudo de caso sobre Data Storytelling em alguns dos maiores bancos brasileiros",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVale relembrar que os n√∫meros e gr√°ficos apresentados nessa imagem s√£o meramente ilustrativos, e, foram definidos de forma completamente aleat√≥ria.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023/2023-08-31-data-decisions/en/index.html",
    "href": "posts/2023/2023-08-31-data-decisions/en/index.html",
    "title": "A small example on how to make better decisions with data",
    "section": "",
    "text": "This article describes a real-world situation where me and my team used data to convince our client (which is one of the biggest companies in the brazilian financial market) at the time to make a better decision. Special thanks to Guilherme Goes and Paulo Gon√ßalves. They both help me to present these ideas and insights to our client.\nIn essence, making decisions is hard. But you always make a better decision when you have data to guide you into a safer and better outcome. When you do not have data to back you up, you are basically in the dark. That is, you make the decision, but you do not know upfront what are the possible outcomes of that decision. You just hope for the best, and this is always a hard position to be in.\n\n\n\n\n\n\nImportant\n\n\n\nAll data, graphs and images indicated in this article are for illustrative purposes only. Therefore, they do not represent the real data of Blip or the bank involved in any shape or form!"
  },
  {
    "objectID": "posts/2023/2023-08-31-data-decisions/en/index.html#the-secured-loan-product",
    "href": "posts/2023/2023-08-31-data-decisions/en/index.html#the-secured-loan-product",
    "title": "A small example on how to make better decisions with data",
    "section": "3.1 The secured loan product",
    "text": "3.1 The secured loan product\nA secured loan was one of several products offered by the bank on WhatsApp. To acquire this product, the user needed to answer several questions, and also, fit into some criterias.\nDepending on the type of industry you work in, you might call this as the sales flow, or, the sales path, which is the path (or the steps) that the user needs to follow to acquire the product you are selling.\nMost companies want to make this path as short as possible so that the user gets to the product faster. However, we are talking about a loan, so the bank certainly needs a lot of personal and financial information about the user before it gives the loan.\nSo in this example, the user needed to answer a considerable amount of questions through WhatsApp to get to the final step of the sales path, that is, to acquire the loan.\nMost of these questions were asking for some personal information, to check whether this particular user fitted or not into some important criterias. Most of these criterias were very standard for any type of loan, like‚Ä¶ the person should not have any legal debts with the government. Some other criterias were purelly financial and assets related, and were also a very commom practice among banks, like‚Ä¶ the person needs to be fully employed, he/she needs to have a car which is fully paid, and, this car needs to be a personal property of the person, i.e.¬†it cannot be a borrowed car from another person."
  },
  {
    "objectID": "posts/2023/2023-08-31-data-decisions/en/index.html#the-weird-criteria",
    "href": "posts/2023/2023-08-31-data-decisions/en/index.html#the-weird-criteria",
    "title": "A small example on how to make better decisions with data",
    "section": "3.2 The weird criteria",
    "text": "3.2 The weird criteria\nBut one of the many criterias was a bit strange for us. Identify the exact criteria is not important for the content of this article. Let‚Äôs just say that, to be elegible for this loan, the user needed to be elegible to three different modalities.\nEach modality corresponded to a different type of loan. If the user was not elegible to all of these three modalities (or types of loan), then, we would automatically rejected the user‚Äôs request for the loan.\nIn essence, we had a flow that worked a bit like this:\n\n\n\nA small representation of the sales path\n\n\nEvery time a user entered our flow, we collected the social ID of this user. Because with this social ID we can use the API to check multiple informations about this person. One of the many things we checked, was whether or not this user was elegible or not to these three modalities of loan."
  },
  {
    "objectID": "posts/2023/2023-08-31-data-decisions/en/index.html#why-this-was-weird",
    "href": "posts/2023/2023-08-31-data-decisions/en/index.html#why-this-was-weird",
    "title": "A small example on how to make better decisions with data",
    "section": "3.3 Why this was weird?",
    "text": "3.3 Why this was weird?\nThis was a weird criteria, because‚Ä¶ if an user called ‚ÄúAna‚Äù is elegible to modality A, then, why not offer a loan of modality A to ‚ÄúAna‚Äù ? As another example, if an user ‚ÄúMike‚Äù is elegible to both modalities A and B, then, why not offer both of these modalities (A and B) to him? Let the user get whatever type of loan he is elegible to, right?\nWhy only users that are elegible to all three modalities (A, B and C) get to decide which loan they want to get? In our head, this criteria did not made much sense, because if an user is elegible to one modality of loan, he should be able to get a loan on this modality he is elegible to."
  },
  {
    "objectID": "posts/2023/2023-08-31-data-decisions/en/index.html#why-this-criteria-existed",
    "href": "posts/2023/2023-08-31-data-decisions/en/index.html#why-this-criteria-existed",
    "title": "A small example on how to make better decisions with data",
    "section": "3.4 Why this criteria existed?",
    "text": "3.4 Why this criteria existed?\nBut let‚Äôs face it. Despite this being a weird criteria, there certainly is a reason for it. Nothing exists without a reason. We thought this weird criteria probably existed either because:\n\na MVP strategy.\nor a ‚Äúrisk trade-off‚Äù strategy.\n\nThe MVP strategy means that the bank included this weird criteria because it greatly simplified the development of the sales path. By simplifying the development, the bank could deliver a MVP (minimal viable product) as fast as possible, and, as a consequence, he could make a profit out of it faster.\nOn the other side, the ‚Äúrisk trade-off‚Äù means that the bank included this weird criteria, because it probably estimated that the risk is considerably higher for people that are not elegible to all three modalities. If the estimated risk is higher, then, the bank have a good reason to not offer this loan to people that are elegible to only one or two modalities.\nBanks are constantly facing a trade-off between risk and profit. In other words, a secured loan like this is always a good profit opportunity. However, this profit opportunity always come at a cost, mostly in the form of risk.\nThat is why banks are usually very good at analysing and estimating risks. When a person looks to acquire a loan, the bank starts to analyze several factors in order to estimate how much risky is to give a loan to this person."
  },
  {
    "objectID": "posts/2023/2023-07-31-git-mark-decisions/en/index.html",
    "href": "posts/2023/2023-07-31-git-mark-decisions/en/index.html",
    "title": "Improving decision making with Git and why you should care",
    "section": "",
    "text": "Last week, me and my team encountered a situation where we need to find out why and how a certain problem occurred. We needed to understand which decisions led to this problem, and why they were made. Pretty normal stuff right?\nBut understanding what happened was only possible for us because we track every change and every decision that we make with Git, by signing commits and writing Pull Requests (or PRs for short). This article uses this real world situation that we faced to showcase how Git and formal processes to register changes in the codebase (like PRs) are a critical part for improving decision making and understanding how your past decisions are affecting you in the present.\nI begin the article describing what problem occurred, and in sequence, I explain what mistakes were made, and how Git helped us to identify those mistakes."
  },
  {
    "objectID": "posts/2023/2023-07-31-git-mark-decisions/en/index.html#footnotes",
    "href": "posts/2023/2023-07-31-git-mark-decisions/en/index.html#footnotes",
    "title": "Improving decision making with Git and why you should care",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhat is a pipeline? A pipeline is just a sequence of steps (or tasks) to be performed at a specific time of the day (or a specific day of the week or of the month, etc.). And a data pipeline is a pipeline that contains tasks that load, transform, send, or ingest data in some form.‚Ü©Ô∏é\nIn essence, a Pull Request (or PR) is a proposal to perform a git merge operation. In other words, you create a PR when you want to merge the changes you made in a branch into another branch (most of the times the main branch). The ‚Äúproposal‚Äù aspect of a PR means that it needs be approved to be effectively performed. A PR is not a feature from Git. It is actually a standard feature from most Git service providers. So you create a PR inside a Git services platform such as GitHub, GitLab and Azure DevOps, and not inside Git itself. If you are not familiar with PRs, the GitHub documentation has an excellent article about it‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023/2023-12-29-vim-cheatsheet/en/index.html",
    "href": "posts/2023/2023-12-29-vim-cheatsheet/en/index.html",
    "title": "NeoVim commands and shortcuts cheatsheet",
    "section": "",
    "text": "Introduction\n Access Cheatsheet PDF\nNeoVim is a highly extensible and ‚Äúkeyboard-based‚Äù text editor. Many consider NeoVim as an evolution of the famous Vim text editor, with focus on extensibility.\nWhat really makes NeoVim (and also Vim) an extremely powerful editor are the Vim commands and keyboards shortcuts. To make my life easier, I decided to write a quick cheatsheet that contains all of the Vim commands and keyboard shortcuts that I use the most."
  },
  {
    "objectID": "posts/2023/2023-02-11-funnel-pkg-v010/en/index.html",
    "href": "posts/2023/2023-02-11-funnel-pkg-v010/en/index.html",
    "title": "Introducing the {ggfunnel} R package",
    "section": "",
    "text": "Introduction\nAt my work, I use a lot of funnel charts. Because they are a very effective visualization to see losses trough a sequence of steps in a sales process (or a ‚Äúsales path‚Äù). I usually build these funnel charts in a very popular Microsoft tool called Power BI.\nThis tool is very popular to build interactive dashboards, and it offers many different native visuals and type of charts to build your visualizations. One visual in particular, is the native funnel chart visual which you can use to visualize your data in a funnel chart, like in the example below:\n\n\n\nA funnel chart at Power BI\n\n\nBut after working a lot with these charts in Power BI, I tought to myself: ‚Äúcould I make a funnel chart in R?‚Äù. The answer is obviously yes! You could definitely drawn a funnel chart with frameworks such as the ggplot2 package. However, there was no packages in the community that could build such visualization out of the box.\nSo I decided to develop a small and experimental R package that could drawn this type of chart. This is how {ggfunnel} was born. In essence, with {ggfunnel}, you can build Power BI like funnel charts in R. The {ggfunnel} package uses the {ggplot2} package (or, more specifically, the ggplot2::geom_tile() geom) to build the funnel chart.\n Official repository  Package website\n\n\nHow to get it ?\nFor now, {ggfunnel} is available only at GitHub. You can download and install the package with the devtools::install_github() function:\n\ndevtools::install_github(\"pedropark99/ggfunnel\")\n\n\n\nA small example of use\nThe main functionality of the package is at a function called ggfunnel::funnel(), which is responsible for producing the plot. You usually define 3 arguments in this function, which are:\n\ndata: the data.frame with the data you want to use in the plot;\nvalues: the column name where are the values you want to display in your funnel chart. In other words, the numerical data that you want to visualize in the chart;\nlevels: the column name with the ‚Äúlevels‚Äù (or the ‚Äúgroups‚Äù) you want to display in your funnel chart. In other words, the categorical data that identifies each level in the funnel;\n\nIn the example below, we are using the ggfunnel::aggregates data.frame to build a basic funnel chart:\n\nlibrary(ggfunnel)\nggfunnel::aggregates\n\n# A tibble: 5 √ó 2\n  Step  N_users\n  &lt;chr&gt;   &lt;dbl&gt;\n1 A        4389\n2 B        3100\n3 C        2005\n4 D         500\n5 E         120\n\n\nThe N_users column is the column with numerical data, so I give it to the values argument of the function. This way, these values will be used to determine the widths of each rectangle in the funnel chart.\nIn contrast, the Step column contains the categorical data of the dataset. That is why I gave this column to the levels argument of the function. As a result, the values of this column will be used to determine the ‚Äúlevels‚Äù of the funnel chart.\n\nplot &lt;- ggfunnel::funnel(\n    data = ggfunnel::aggregates,\n    values = N_users, levels = Step\n  )\n\nprint(plot)\n\n\n\n\n\n\n\n\nThe above plot is very simple. However, since the output of ggfunnel::funnel() is a native ggplot object, you can customize and extend the plot greatly with the {ggplot2} package. I give examples and details on how you can customize this output at the main vignette (vignette(\"funnel\")) of the package, which you can read at the website of the package.\nJust as a small demonstration, I can add titles to the plot, adjust the theme, and add some notes and arrows to emphasize some parts of the plot. I can also use other packages which extends {ggplot2} to add more custom outputs to the plot, such as the {ggtext} package:\n\nplot &lt;- ggfunnel::aggregates |&gt;\n  ggfunnel::funnel(\n    values = N_users, levels = Step,\n    text_specs = list(\n      nudge_x = c(rep(0, times = 4), 0.05),\n      colour = c(rep(\"white\", times = 4), \"black\")\n    )\n  )\n\nnote &lt;- \"We lost **75% of the users**&lt;br&gt;from step \\\"C\\\" to step \\\"D\\\"\"\n\nplot +\n  ggplot2::labs(\n    title = \"Funnel of users in each step of the sales path\",\n    subtitle = \"The biggest loss of users is at the \\\"D\\\" step\",\n    y = NULL\n  ) +\n  ggplot2::theme(\n    plot.title = ggplot2::element_text(face = \"bold\", size = 16),\n    plot.title.position = \"plot\"\n  ) +\n  ggtext::geom_richtext(\n    ggplot2::aes(x = 0.35, y = \"D\",\n    label = note),\n    label.color = NA\n  ) +\n  ggplot2::geom_segment(\n    ggplot2::aes(x = 0.17, xend = 0.07, y = \"D\", yend = \"D\"), \n    arrow = ggplot2::arrow(\n      length = ggplot2::unit(0.25, \"cm\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nA simple (but far from perfect) approach\n{ggfunnel} is a very simple (and kind of experimental) package, and it is far from perfect. This means that, currently, {ggfunnel} gives you the minimal code necessary to produce a decent funnel chart. But it does not give you much more functionality than that.\nIt also makes some assumptions about your data that might not hold, and it does not contain some features that you might find at Power BI (e.g.¬†percentage labels).\nBut, even being a very simple package, ggfunnel::funnel() always returns the raw ggplot object that describes the funnel chart. This means that the package gives you a lot of freedom to customize (or to complement) the output in the way you might need. See vignette(\"funnel\") for more details on how to customize/complement the ggfunnel::funnel() output.\nHowever, the package needs some work to be a more robust and complete piece of code, for sure. If you think you can make {ggfunnel} better I would be happy to review a PR from you!"
  },
  {
    "objectID": "posts/2023/2023-12-31-introducing-typst/en/index.html",
    "href": "posts/2023/2023-12-31-introducing-typst/en/index.html",
    "title": "My experience with Typst, the potential sucessor of LaTex",
    "section": "",
    "text": "Typst is a new markup-based typesetting system for the sciences written in Rust. The Typst works similarly to other typesetting systems. In resume, you write a Typst source file which describes the document you want to create, and then, you invoke the Typst compiler to build the document you described in the source file.\n\n\n\nProducing documents with Typst\n\n\nTypst is open-source and is freely available at GitHub, just download the release file according to your operational system and install the tool into your system."
  },
  {
    "objectID": "posts/2023/2023-12-31-introducing-typst/en/index.html#have-good-documentation",
    "href": "posts/2023/2023-12-31-introducing-typst/en/index.html#have-good-documentation",
    "title": "My experience with Typst, the potential sucessor of LaTex",
    "section": "2.1 Have good documentation!",
    "text": "2.1 Have good documentation!\nTypst have a very good documentation, and this makes all the difference in the world! Because learning how to use the tool becomes so much easier when you have a good support material to rely on."
  },
  {
    "objectID": "posts/2023/2023-12-31-introducing-typst/en/index.html#have-fast-compilation-time",
    "href": "posts/2023/2023-12-31-introducing-typst/en/index.html#have-fast-compilation-time",
    "title": "My experience with Typst, the potential sucessor of LaTex",
    "section": "2.2 Have fast compilation time!",
    "text": "2.2 Have fast compilation time!\nLaTex have a known weakness of being slow to compile the input file and producing the output PDF. Typst does not suffer from this problem, because it has a much faster compilation time.\nCompiling a very small Typst document takes about 0.05s in my machine, while compiling essentially the same document in LaTex take 0,26s:\n$ time typst compile example.typ\n0,05s user 0,06s system 88% cpu 0,121 total\n$ time xelatex example.tex\n0,26s user 0,16s system 80% cpu 0,522 total\nThis is an important advantage of Typst, because it delivers a much faster feedback loop into your workflow. In other words, if you reduce the compilation time, you will spend less time waiting to see if your code works! By spending less time trying to see the output of your work, you can spend more time in what really matters! Like writing more content or refining your work."
  },
  {
    "objectID": "posts/2023/2023-12-31-introducing-typst/en/index.html#understandable-error-messages",
    "href": "posts/2023/2023-12-31-introducing-typst/en/index.html#understandable-error-messages",
    "title": "My experience with Typst, the potential sucessor of LaTex",
    "section": "2.3 Understandable error messages",
    "text": "2.3 Understandable error messages\nFor me personally, one of the great weaknesses of LaTex is it‚Äôs error messages, which are very obscure, and honestly, useless in the majority of cases. And I mean this.\nIn contrast, Typst have much more understandable error messages. Messages that are easy to understand, and that are easy to relate back to your source code. Take this error messages from Typst as an example:\nerror: the character `#` is not valid in code\n  ‚îå‚îÄ posts/2023/2023-12-31-introducing-typst/example.typ:2:3\n  ‚îÇ\n2 ‚îÇ    #set text(font: \"Inconsolata\")\n  ‚îÇ    ^\nOr maybe this other error message:\nerror: expected function, found content\n  ‚îå‚îÄ posts/2023/2023-12-31-introducing-typst/example.typ:1:19\n  ‚îÇ  \n1 ‚îÇ   #show raw: code =&gt; {\n  ‚îÇ ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ^\n2 ‚îÇ ‚îÇ    set text(font: \"Inconsolata\")\n3 ‚îÇ ‚îÇ }[#code]\n  ‚îÇ ‚ï∞‚îÄ^\n\nhelp: error occurred while applying show rule to this raw\n  ‚îå‚îÄ posts/2023/2023-12-31-introducing-typst/example.typ:7:16\n  ‚îÇ\n7 ‚îÇ Testing if this `raw code element` uses the show rule.\n  ‚îÇ                 ^^^^^^^^^^^^^^^^^^"
  },
  {
    "objectID": "posts/2023/2023-12-31-introducing-typst/en/index.html#very-good-quality-output",
    "href": "posts/2023/2023-12-31-introducing-typst/en/index.html#very-good-quality-output",
    "title": "My experience with Typst, the potential sucessor of LaTex",
    "section": "2.4 Very good quality output",
    "text": "2.4 Very good quality output\nLaTex is known for producing very-high quality documents, and Typst leaves nothing to be desired in this aspect. Specially because the Typst development team ported some of the core algorithms behind LaTex into Typst.\nSo Typst also produces very-high quality documents because it learned from the great powers of LaTex."
  },
  {
    "objectID": "posts/2023/2023-12-31-introducing-typst/en/index.html#preview-and-incremental-compilation",
    "href": "posts/2023/2023-12-31-introducing-typst/en/index.html#preview-and-incremental-compilation",
    "title": "My experience with Typst, the potential sucessor of LaTex",
    "section": "2.5 Preview and Incremental compilation",
    "text": "2.5 Preview and Incremental compilation\nTypst have a preview mode which is very good and powerful. There is a great video in Typst‚Äôs website that demonstrates it.\nWhen you use this preview mode, Typst create a new window that shows the preview of the output document of your source code, and any changes that you make to your source code are instantly reflected (pratically in real time) into the previewed document.\nThis preview in real time of the output document is very good when you are trying to refine the aesthetics of your document, but you are not very sure yet on how to do it properly, and you want to test different options.\nThis power from the preview mode is only possible, because Typst has an incremental compilation engine inside Typst compiler, which is capable of compiling just the section of your code your changed, instead of recompiling the entire source file, and regenerate the entire PDF output again."
  },
  {
    "objectID": "posts/2023/2023-12-31-introducing-typst/en/index.html#show-rules-are-awesome",
    "href": "posts/2023/2023-12-31-introducing-typst/en/index.html#show-rules-are-awesome",
    "title": "My experience with Typst, the potential sucessor of LaTex",
    "section": "2.6 Show rules are awesome!",
    "text": "2.6 Show rules are awesome!\nShow rules with the #show directive are an amazing way of customizing specific elements of your document. But the true power of #show directives is that you can write more clear and organized code with them.\nWhen you want some parts of your document to have, for example, font Inconsolata with size 14pt, and other parts of the same document to have completely different settings, like font Times New Roman size 12pt, and coloured red, you usually end up redefining these settings over and over again.\nYou might for example set a ‚Äúdefault setting‚Äù to be applied over the entire document. Then, you overrule this default setting by setting specific settings every time that a specific element appears across the document. So you end up with a lot of duplicated code, that redefines the same settings over and over again, across the entire source file.\nShow rules with the #show directive eliminates this necessitty by allowing you to specify a set of settings to be applied to every element of type x that appears in your document. As an example, if I want to use font Inconsolata size 14pt in any raw code that is exposed inside my document, I can set a show rule to be applied over any element of type raw, like this:\n#show raw: code =&gt; {\n   set text(font: \"Inconsolata\", size : 14pt)\n   code\n}\n\nTesting if this `raw code element` uses the show rule.\nWith the show rule that I created above, any raw code that I create across my Typst document will be rendered in the PDF output using font Inconsolata size 14pt. To some extent, show rules in Typst are almost like CSS code that uses CSS selectors to apply certain rules to specific HTML elements."
  },
  {
    "objectID": "posts/2023/2023-12-31-introducing-typst/en/index.html#reusing-code-is-much-easier",
    "href": "posts/2023/2023-12-31-introducing-typst/en/index.html#reusing-code-is-much-easier",
    "title": "My experience with Typst, the potential sucessor of LaTex",
    "section": "2.7 Reusing code is much easier",
    "text": "2.7 Reusing code is much easier\nReusing code or settings across different parts in your document in Typst is much more straightforward than it is in LaTex. Because is very easy to create functions in Typst, or show rules that can spread a piece of code over multiple locations of your document.\nIn other words, Typst looks like a real programming language. You can easily create functions in Typst to reuse the same piece of code. In contrast, in LaTex, there is no notion of functions per se, you have macros instead. When you want to reuse a piece of code , you normally you create new commands (with the \\newcommand macro). This makes LaTex code harder to read and to comprehend in my opinion."
  },
  {
    "objectID": "posts/2023/2023-12-31-introducing-typst/en/index.html#csl-is-now-available-in-typst",
    "href": "posts/2023/2023-12-31-introducing-typst/en/index.html#csl-is-now-available-in-typst",
    "title": "My experience with Typst, the potential sucessor of LaTex",
    "section": "2.8 CSL is now available in Typst!",
    "text": "2.8 CSL is now available in Typst!\nTypst is very young, and until very recently, the use of different citation styles was very limited. But on the most recent version of Typst, a CSL (Citation Style Language) processor was added! And as consequence, we can use now any possible citation style in Typst documents by providing a CSL file to the style argument of the bibliography() function to be processed in conjunction with the Typst source file.\nFor example, in Brazil, academia have very rigorous rules about how citation should be written in a scientific article, and these rules are specified by the norms produced by the ABNT (Brazilian Agency of Technical Norms) agency. Now, with the new additions to Typst, we can easily write citations using the brazilian ABNT citation style, by providing a CSL file like the CSL styles produced by IBICT - Brazilian Institute for Science and Technology Information."
  },
  {
    "objectID": "posts/2023/2023-12-31-introducing-typst/en/index.html#syntax-similar-to-markdown",
    "href": "posts/2023/2023-12-31-introducing-typst/en/index.html#syntax-similar-to-markdown",
    "title": "My experience with Typst, the potential sucessor of LaTex",
    "section": "2.9 Syntax similar to Markdown",
    "text": "2.9 Syntax similar to Markdown\nTypst have a markup language whose syntax is similar to Markdown. And this is a great thing. Because the markup language in LaTex is‚Ä¶., well, it is not awfull to write, but it produces an awfull visual mess in the text you write.\nFor example, if your write some text in LaTex that have a lot of emphasized words, you need to add a lot of \\textbf{} or \\emph{} macros in it, and, as a result, your text looks very messy. In typst, you just surround each word by stars (*), and, as consequence, you leave much less visual noise in your text. In other words, your text looks cleaner (or clearer) in Typst."
  },
  {
    "objectID": "posts/2022/2022-12-21-spark-map-v0.2.3/en/index.html",
    "href": "posts/2022/2022-12-21-spark-map-v0.2.3/en/index.html",
    "title": "Introducing the {spark_map} Python package",
    "section": "",
    "text": "spark_map is a python package that offers some tools that help you to apply a function over multiple columns of Apache Spark DataFrames, using pyspark. The package offers two main functions (or ‚Äútwo main methods‚Äù) to distribute your calculations, which are spark_map() and spark_across(). Furthermore, the package offers several methods to map (or select) the columns to which you want to apply your calculations (these methods are called mapping methods in the package).\n Official repository  Package website  Page on PyPI"
  },
  {
    "objectID": "posts/2022/2022-12-21-spark-map-v0.2.3/en/index.html#overview",
    "href": "posts/2022/2022-12-21-spark-map-v0.2.3/en/index.html#overview",
    "title": "Introducing the {spark_map} Python package",
    "section": "",
    "text": "spark_map is a python package that offers some tools that help you to apply a function over multiple columns of Apache Spark DataFrames, using pyspark. The package offers two main functions (or ‚Äútwo main methods‚Äù) to distribute your calculations, which are spark_map() and spark_across(). Furthermore, the package offers several methods to map (or select) the columns to which you want to apply your calculations (these methods are called mapping methods in the package).\n Official repository  Package website  Page on PyPI"
  },
  {
    "objectID": "posts/2022/2022-12-21-spark-map-v0.2.3/en/index.html#how-to-install-it",
    "href": "posts/2022/2022-12-21-spark-map-v0.2.3/en/index.html#how-to-install-it",
    "title": "Introducing the {spark_map} Python package",
    "section": "How to install it ?",
    "text": "How to install it ?\nYou can install the package through PyPI, by using the pip tool on your terminal, like this:\npip install spark-map"
  },
  {
    "objectID": "posts/2022/2022-12-21-spark-map-v0.2.3/en/index.html#what-problem-spark_map-solves",
    "href": "posts/2022/2022-12-21-spark-map-v0.2.3/en/index.html#what-problem-spark_map-solves",
    "title": "Introducing the {spark_map} Python package",
    "section": "What problem spark_map solves?",
    "text": "What problem spark_map solves?\nI work a lot with data pipelines using Apache Spark and pyspark at Take Blip. Some day, I got myself writing a very long agg() statement to aggregate multiple columns of my Spark DataFrame with the same function, like this one below:\n\nfrom pyspark.sql.functions import sum, column\naggregates = (\n    spark.table('cards.detailed_sales_per_user')\n        .groupBy('day')\n        .agg(\n            sum(column('cards_lite')).alias('cards_lite'),\n            sum(column('cards_silver')).alias('cards_silver'),\n            sum(column('cards_gold')).alias('cards_gold'),\n            sum(column('cards_premium')).alias('cards_premium'),\n            sum(column('cards_enterprise')).alias('cards_enterprise'),\n            sum(column('cards_business')).alias('cards_business')\n        )\n)\n\nLooking at this code, I had the following thought: ‚Äúthis is not elegant, and is error-prone, because it involves copy and paste, and very subtle changes in each line‚Äù. Following the golden rule of DRY (do not repeat yourself), I had to find a better way to write this code.\nI just wanted to apply the sum() function over multiple columns of cards.detailed_sales_per_user grouped by day. Because of that, I decided to develop the spark_map package, which allows you to declare this operation in a much better, elegant and concise way, by using the spark_map() function.\n\nfrom spark_map.functions import spark_map\nfrom spark_map.mapping import starts_with\ngrouped_by_day = spark.table('cards.detailed_sales_per_user')\\\n    .groupBy('day')\n\naggregates = spark_map(grouped_by_day, starts_with('cards'), sum)"
  },
  {
    "objectID": "posts/2022/2022-12-21-spark-map-v0.2.3/en/index.html#how-spark_map-works",
    "href": "posts/2022/2022-12-21-spark-map-v0.2.3/en/index.html#how-spark_map-works",
    "title": "Introducing the {spark_map} Python package",
    "section": "How spark_map() works ?",
    "text": "How spark_map() works ?\nThe spark_map() function receives three inputs, which are table (i.e.¬†the Spark DataFrame you want to use), mapping (i.e.¬†a ‚Äúmapping‚Äù that describes which columns you want to apply your function), and function (i.e.¬†the function you want to apply to each column in the Spark DataFrame).\nIn short, the starts_with('cards') section tells spark_map() that you want to apply the input function on all columns of grouped_by_day whose name starts with the text 'cards'. In other words, all spark_map() does is to apply the function you want (in the above example this function is sum()) to whatever column it finds in the input DataFrame which fits in the description of your mapping method.\nYou can use different mapping methods to select the columns of your DataFrame, and the package offers several built-in methods which can be very useful for you, which are available through the spark_map.mapping module of the package. You can select columns based on:\n\nat_position(): their position (i.e.¬†3rd, 4th and 5th columns);\nmatches(): a regex to which their match;\nare_of_type(): the type of data their store (i.e.¬†all columns of type int);\nstarts_with() and ends_with(): its name starting or ending with a particular pattern;\nall_of(): its name being inside a specific list of options;"
  },
  {
    "objectID": "posts/2022/2022-12-21-spark-map-v0.2.3/en/index.html#the-differences-between-spark_map-and-spark_across",
    "href": "posts/2022/2022-12-21-spark-map-v0.2.3/en/index.html#the-differences-between-spark_map-and-spark_across",
    "title": "Introducing the {spark_map} Python package",
    "section": "The differences between spark_map() and spark_across()",
    "text": "The differences between spark_map() and spark_across()\nThere are two main functions in the package that performs the heavy work, which are spark_map() and spark_across().\nBoth of these functions perform the same work, which is to apply a function over a set of columns of a Spark DataFrame. But they differ in the method they use to apply this function. While spark_map() uses the agg() method of Spark DataFrame‚Äôs to apply the function, spark_across() uses the withColumn() method to do so.\nThis means that you will mainly use spark_map() when you want to calculate aggregates. Is worthy pointing out that spark_map() works perfectly with grouped DataFrames as well (i.e.¬†GroupedData). In the other hand, you will use spark_across() when you want to just transform the values of multiple colums at once by applying the same function over them."
  },
  {
    "objectID": "posts/2022/2022-12-26-4th-edition-rbook/pt/index.html",
    "href": "posts/2022/2022-12-26-4th-edition-rbook/pt/index.html",
    "title": "Novidades da 4¬∞ edi√ß√£o de Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica",
    "section": "",
    "text": "Introdu√ß√£o\n√â com muito prazer que venho compartilhar com voc√™ as novidades que estou trazendo para essa nova edi√ß√£o do livro Introdu√ß√£o √† Linguagem R: seus fundamentos e sua pr√°tica. Esta quarta edi√ß√£o busca principalmente fazer algumas corre√ß√µes e adi√ß√µes que buscam manter o livro como um refer√™ncia moderna, introdut√≥ria e t√©cnica sobre a linguagem R.\n Compre uma vers√£o do livro  P√°gina de publica√ß√£o  Leia online\n\n\nO que temos de novo?\nPrimeiro, a se√ß√£o do cap√≠tulo 4 que citava o pacote SAScii foi removida. Pois durante o desenvolvimento desta quarta edi√ß√£o, foi identificado que este pacote n√£o estava funcionando corretamente em vers√µes mais recentes do R.\nSegundo, uma nova se√ß√£o foi adicionada ao cap√≠tulo 5, para introduzir o novo operador pipe do R (|&gt;) - que est√° dispon√≠vel desde a vers√£o 4.1 da linguagem, al√©m de explicar as diferen√ßas deste novo operador com o operador pipe do pacote magrittr.\nTerceiro, v√°rias melhorias e pequenas corre√ß√µes foram aplicadas sobre o cap√≠tulo 8, com o objetivo de melhorar a clareza do conhecimento exposto e da organiza√ß√£o do cap√≠tulo.\nQuarto, a se√ß√£o Alterando as fontes do seu gr√°fico no cap√≠tulo 9 foi reformulada, com o objetivo de substituir o pacote extrafont (o qual era a solu√ß√£o descrita em edi√ß√µes passadas desta obra) pelo pacote ragg, o qual se tornou uma solu√ß√£o mais moderna e robusta para o uso de fontes em gr√°ficos do R.\nQuinto, tivemos algumas melhorias sobre o cap√≠tulo 4, especialmente sobre a descri√ß√£o de endere√ßos absolutos e relativos, assim como a se√ß√£o A fun√ß√£o guess_encoding() como um poss√≠vel guia que foi atualizada com o objetivo de acompanhar as mudan√ßas recentes sobre a fun√ß√£o readr::guess_encoding().\n\n\nSobre onde encontrar o livro\nVoc√™ pode ler toda a obra de maneira gratuita e aberta atrav√©s de seu website üìñ. Caso voc√™ queira contribuir para o projeto desse livro, voc√™ pode adquirir uma vers√£o f√≠sica ou em EBook do livro atrav√©s da loja da Amazon. Ao comprar essas vers√µes, voc√™ estar√° me ajudando a continuar contribuindo com a nossa comunidade ‚ù§Ô∏è. Al√©m disso, voc√™ tamb√©m pode doar um Pix para o autor (veja a p√°gina Donate or Sponsor Me).\n\n\nContribua para a obra ou fa√ßa sugest√µes!\nCaso seja de seu interesse, voc√™ pode contribuir diretamente para a obra, ao postar pull requests dentro de seu reposit√≥rio oficial. Para mais, voc√™ tamb√©m pode fazer sugest√µes ou coment√°rios, ao postar issues neste mesmo reposit√≥rio."
  },
  {
    "objectID": "posts/2024/2024-02-19-flow-even/en/index.html",
    "href": "posts/2024/2024-02-19-flow-even/en/index.html",
    "title": "Producing evenly-spaced and non-overlapping curves with Jobard-Lefer Algorithm in C",
    "section": "",
    "text": "I‚Äôm currently writing a book about R and Graphics (stay tuned for more news about this book soon üòâ). During my research for this book, I had to study a relatively famous and established algorithm, called as the Jobard-Lefer algorithm. It is an algorithm for producing evenly spaced curves in a flow field (or a vector field). This algorithm is thoroughly described in a scientific paper (Jobard and Lefer 1997).\nIn this article, I want to describe how you can implement this algorithm in C. These are the topics we are going to discuss:\n\nWhat are flow fields (or vector fields)?\nHow to draw curves in a flow field?\nWhat the algorithm does?\nHow does it work?\nHow to implement it in C?"
  },
  {
    "objectID": "posts/2024/2024-01-13-spark-cache/en/index.html",
    "href": "posts/2024/2024-01-13-spark-cache/en/index.html",
    "title": "How to use the cache effectively on Apache Spark",
    "section": "",
    "text": "Introduction\nI work a lot with Apache Spark on Databricks, and very recently, I encountered some cases of jobs failling because of cached DataFrames ocupying all the memory available, and, as consequence, raising OutOfMemory runtime errors.\nIn essence, the job was executing a Python notebook that contained some pyspark code. Many Spark DataFrames were being constantly cached by using the DataFrame method cache(). And this pattern was causing the memory to be crowed with more and more caches, until it became full, and caused the job to crash.\nIn this article, I want to describe how you should use cache() effectively on Apache Spark, and also, explain how this OutOfMemory error happenned.\n\n\nWhat is this cache() method?\nIn Apache Spark we work with Spark DataFrames. They are the core (or the essence) of any Spark application. We model, transform, load and export these objects to get the result we want.\nHowever, in some cases, generating a specific Spark DataFrame can take a long time. Maybe this DataFrame is defined by a heavy query, that involves many and many layers of calculations, or maybe, a huge amount of data needs to be read to calculate/generate this DataFrame.\nFor these specific cases, we can cache this specific Spark DataFrame. By caching it, we avoid the need to calculate/generate from scratch this DataFrame, over and over again. We calculate it once, and then, we reuse this same data in posterior cases.\nWe do this, by calling the cache() DataFrame method, to mark that specific DataFrame as a ‚Äúcached DataFrame‚Äù. As an example, in the code below, I‚Äôm creating a Spark DataFrame called df:\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nfrom datetime import date\nfrom pyspark.sql import Row\n\ndata = [\n  Row(id = 1, value = 28.3, date = date(2021,1,1)),\n  Row(id = 2, value = 15.8, date = date(2021,1,1)),\n  Row(id = 3, value = 20.1, date = date(2021,1,2)),\n  Row(id = 4, value = 12.6, date = date(2021,1,3))\n]\n\ndf = spark.createDataFrame(data)\nIf I want to cache this DataFrame, all I need to do is to call the cache() method with this df object, like this:\ndf.cache()\nNow, the df DataFrame is marked as a ‚ÄúDataFrame to be cached‚Äù by Spark, and if we use this df DataFrame over the next lines of our notebook, instead of Spark recalculating the entire DataFrame each time, it will reuse the data of this DataFrame that was cached. This can make our Spark application much faster, because Spark will not spend more time recalculating the DataFrame.\nBut is important to note, that in Apache Spark, cache operations are lazy operations. I quickly described this lazy aspect of Spark at Section 3.5 of my pyspark book.\n\nA key aspect of Spark is its laziness. In other words, for most operations, Spark will only check if your code is correct and if it makes sense. Spark will not actually run or execute the operations you are describing in your code, unless you explicit ask for it with a trigger operation, which is called an ‚Äúaction‚Äù (this kind of operation is described in Section 5.2). Faria (2024)\n\nTherefore, the cache operation of the DataFrame will only happen if you call an action over the next lines of your notebook. I listed what operations are considered as actions in Spark at Section 5.2 of my pyspark book. But essentially, the Spark DataFrame methods below are all examples of actions in Spark:\n\nshow()\ncollect()\ncount()\nwrite.csv()\nread.csv()\n\nSo, if you call any of the Spark DataFrame methods above, after you called cache() over the same Spark DataFrame, then, Spark will effectively cache your DataFrame.\n\n\nHow to use cache effectively\nThere are two commom situations where cache can be very effective, which are:\n\nConstantly use the same DataFrame over the notebook.\nFrequently access a subset of a DataFrame.\n\nEvery time you call an action on a Spark DataFrame in your notebook, Spark needs to read and load the DataFrame‚Äôs data from storage (this storage can be many things, like a data lake in the cloud, or a local static file, etc.). So if you repeateadly use the same Spark DataFrame, then, you are repeateadly reading from storage the same data over and over again.\nHaving this in mind, when you constantly use the same Spark DataFrame over and over again across your notebook, it might be a good idea to cache this specific DataFrame. For example, if you use a DataFrame called students in 15 locations in your notebook, then, by default, Spark will recalculate this students DataFrame, from scratch, 15 times. But if you cache this DataFrame, then, Spark will calculate the DataFrame on the first time, and reuse the cached data in the remaining 14 times.\nSo ‚Ä¶\n\ncaching is optimal when you need to perform multiple operations on the same dataset to avoid reading from storage repeatedly. Patidar (2023).\n\nThe same applies to when you are frequently using the same subset of a DataFrame. For example, you might have defined in your notebook a new object under_10 which contains all rows from students DataFrame that describes students that have an age below 10.\nfrom pyspark.sql.functions import col\nunder_10 = students.filter(col('age') &lt; 10)\nIf you use this subset of students DataFrame across multiple locations of your notebook, then, it might be also a good idea to cache this under_10 DataFrame, like this:\nfrom pyspark.sql.functions import col\nunder_10 = (\n  students\n    .filter(col('age') &lt; 10)\n    .cache()\n)\n\n\nHow to NOT use cache effectively\nIf your notebook have a very sequential logic, then, caching is usually wasteful or a bad idea. Take for example, a notebook that perform the following steps:\n\ntake DataFrame A, filter it, summarise it, and then, save it.\ntake DataFrame B, filter it, add new columns, group by and summarise it, and then, send it to the server.\ntake DataFrame C, filter the essential information you want, save this data into a new CSV file, and then, send this CSV file to an Amazon S3 bucket.\n\nIf you pay attention to these steps above, you will notice that they are independent from each other. These steps can be performed in any order, because the result from each step does not depend on the results from the other steps.\nIn a notebook like this, caching any of the three cited DataFrames (A, B or C) is usually unnecessary (or a bad idea), because each of these DataFrames is used only once across the notebook, so there is no really need to cache it. By caching it, you will be wasting not only time, but also, memory space.\nHaving this in mind, notebooks that have a more complex and interconnected logic are much more suitable candidates for caching. For example, if you have a notebook that produces two DataFrames (B and C) as output, and they both are produced from a JOIN operation with the same DataFrame A, then, it might be worth to cache DataFrame A, so that Spark calculates DataFrame A from scratch only once, instead of two.\nNow, another situation where caching might be a bad idea is when you do not have much memory available in your Spark cluster. As an example, let‚Äôs consider that you only had available a cluster with 2 nodes and 8 GB of RAM memory in your Spark environment.\nIf your notebook is working with a Spark DataFrame whose physical size is 7 GB worth of data, then, it might be a very bad idea to cache this DataFrame, because if you do cache it, then, 7 GB (almost 90%) of your memory will be occupied with the cached data, and this leaves only 1 GB avaialable to perform all the transformations and remaining operations of your notebook.\nSo caching can be more of a hindrance than a help (especially if you cache multiple DataFrames) when these caches occupy a too big chunk of memory. 1 GB might be (or might be not) enough to perform the remaining tasks in your notebook. But you should not take this risk. In general, when you have a low quantity of memory available, and you cache multiple DataFrames, two things can happen:\n\nmost likely, your job (or your Spark application) will crash with an OutOfMemory error.\nthe calculations and transformations become much, much, MUCH slower, because the memory does not have enough space available to perform these calculations in parallel. Your Spark application will succeed, and will produce the output you want‚Ä¶ only many, many, MANY times slower.\n\n\n\nWhat happened with the cases I‚Äôve seen\nI recently encountered some cases of jobs failing during runtime because of OutOfMemory errors, that were generated by multiple caching operations that polluted all memory available in the cluster.\nI want to use this practical example to demonstrate how caching was badly used in this example. So that you can learn from it.\nThe notebook I encountered, was essentially responsible for update 3 different tables in our SQL databases, and to do that, this notebook was defining 5 different Spark DataFrames, and all of them were being cached with the cache() DataFrame method.\nIf I execute this notebook in a more robust Spark cluster, with more worker nodes, with more RAM memory available, then, the notebook just executed fine.\nThe complete execution of the notebook took around 55 minutes, which is a long time‚Ä¶ But no runtime errors were raised. In essence, no OutOfMemory errors were raised because we were lucky. Because this more robust cluster had enough memory space to hold 5 DataFrames in cache. Figure¬†1 presents this process visually:\n\n\n\n\n\n\nFigure¬†1: DataFrames being cached in a more robust Spark cluster\n\n\n\nHowever, this same notebook was being executed every day by a scheduled job in Databricks (i.e.¬†a Databricks Workflow). But every time this notebook was executed through the scheduled job, it failed with OutOfMemory errors. The scheduled job was being executed by a much smaller cluster, that had only two worder nodes and 8 GB of RAM memory available.\nThe OutOfMemory errors were being raised right after we cached the third Spark DataFrame (let‚Äôs call this DataFrame of df3) defined in the notebook. So, in summary, what was happening is: Spark was caching each DataFrame defined in the notebook, but on the third DataFrame, the Spark process run out of memory. In other words, we did not had any more memory to do anything!\nFigure¬†2 summarizes this process visually:\n\n\n\n\n\n\nFigure¬†2: Multiple caches surpassing the limit of memory available in a less robust Spark cluster\n\n\n\n\n\nConclusion\nIn essence, using the Spark DataFrame cache() method can improve the performance of your Spark application. But you should be careful when using this resource, because it can be more of a hindrance than a help, depending on how you use it.\nAs a good rule of thumb, you usually want to use caches in Spark when:\n\nConstantly use the same DataFrame over the notebook.\nFrequently access a subset of a DataFrame.\n\nIn this article I also presented a situation where caches were responsible for crashing a Spark application, as a real example on how caches can be harmful.\n\n\n\n\n\nReferences\n\nFaria, Pedro Duarte. 2024. Introduction to Pyspark. 1st ed. Belo Horizonte. https://pedropark99.github.io/Introd-pyspark/.\n\n\nPatidar, Charchit. 2023. ‚ÄúHow Cache Works in Apache Spark.‚Äù https://medium.com/@charchitpatidar/how-cache-works-in-apache-spark-aea6eeb3fd03."
  },
  {
    "objectID": "posts/2024/2024-02-19-flow-even/en/index.html#the-first-part-of-the-algorithm",
    "href": "posts/2024/2024-02-19-flow-even/en/index.html#the-first-part-of-the-algorithm",
    "title": "Producing evenly-spaced and non-overlapping curves with Jobard-Lefer Algorithm in C",
    "section": "4.1 The first part of the algorithm",
    "text": "4.1 The first part of the algorithm\nThe first part of the algorithm is constantly making checks to make sure that the current curve that we are drawing is distant enough from the other neighboring curves around it.\nThis part of the algorithm works around a specific value, that represents the minimum distance allowed between two curves. In other words, given the distance between a point in a curve, and any point from another curve that is in the close surroundings. and\nIn essence, at each step we take drawing our curve in the flow field, we look at the curves that are around us, and we compare the distance between our current position and all of these neighboring curves. If this distance between the next step we take and any of the neighboring curves, is lower or equal to a specific value (Jobard and Lefer (1997) call this value as \\(d_{sep}\\) value, or, the ‚Äúseparating distance‚Äù) , then, we do not take this next step, and we stop drawing the current curve. At this point, if we have a next curve to draw, then, we start to this next curve in the field, and restart this process of checking if the distance of this next step we take and the neighboring curves will be less or equal to the ‚Äúseparating distance‚Äù.\nWith this algorithm you normally produce very nice and ‚Äúpleasing to the eyes‚Äù kinds of images. At Figure¬†3, I am showing the difference that this algorithm makes to the image. In both subfigures, we used the same colors for each curve, the same starting points for each curve, the same step length, the same number of steps, same flow field, etc. Anymway, all the configs used in both images are literally the same. The only difference between these two images, is the use (or not) of the Jobard-Lefer algorithm.\nIn the first image (without the Jobard-Lefer algorithm) I simply drawn the curves using the steps I described at Section¬†3, without caring if the curves were overlapping each other, or, if they were too close to neighboring curves. In contrast, in the second image, I used the Jobard-Lefer algorithm to draw the curves while checking if they were not overlapping each other, and also, if they were not too close to neighboring curves.\n\n\n\n\n\n\n\n\n\n\n\n(a) Without Jobard-Lefer algorithm\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) With Jobard-Lefer algorithm\n\n\n\n\n\n\n\nFigure¬†3: Demonstrating the effect of the Jobard-Lefer algorithm on curves drawn in the field"
  },
  {
    "objectID": "posts/2024/2024-02-19-flow-even/en/index.html#footnotes",
    "href": "posts/2024/2024-02-19-flow-even/en/index.html#footnotes",
    "title": "Producing evenly-spaced and non-overlapping curves with Jobard-Lefer Algorithm in C",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are other parts to the entire algorithm proposed by Jobard and Lefer (1997). But these other parts are not worth explaining in this article. Read the original scientific paper for full details about the algorithm.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2024/2024-02-19-flow-even/en/index.html#drawing-non-overlapping-curves",
    "href": "posts/2024/2024-02-19-flow-even/en/index.html#drawing-non-overlapping-curves",
    "title": "Producing evenly-spaced and non-overlapping curves with Jobard-Lefer Algorithm in C",
    "section": "4.1 Drawing non-overlapping curves",
    "text": "4.1 Drawing non-overlapping curves\nThis first part of the algorithm is the part that makes sure that only non-overlapping curves are drawn to the field. The idea behind it, is that we are constantly checking the distances between our next position and the other curves around us. We want to always make sure that the current curve is far enough from the other curves, and, if not, stop drawing this curve.\nThis specific part of the algorithm works around a constant value, that represents the minimum distance allowed between two curves. Jobard and Lefer (1997) call this value as \\(d_{sep}\\), or, as the ‚Äúseparating distance‚Äù. The end result of the algorithm is that any point in any curve in the field is, at least, at a distance of \\(d_{sep}\\) units from any other point from another curve in the field.\nIf you use this part of the algorithm, you normally produce some very nice and ‚Äúpleasing to the eyes‚Äù type of images. At Figure¬†3, I am showing the difference that this part of the algorithm makes to the image. In both subfigures, we used the same colors for each curve, the same starting points for each curve, the same step length, the same number of steps, same flow field, etc. Anymway, all the configs used in both images are literally the same. The only difference between these two images, is the use (or not) of this ‚Äúnon-overlaping‚Äù curve algorithm.\nIn the first image (without the Jobard-Lefer algorithm) I simply drawn the curves using the steps I described at Section¬†3, without caring if the curves were overlapping (or, if they were too close to) each other. In contrast, in the second image, I used the first part of the Jobard-Lefer algorithm to draw the curves while checking if they were overlapping (or, if they were too close to) each other.\n\n\n\n\n\n\n\n\n\n\n\n(a) Without Jobard-Lefer algorithm\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) With Jobard-Lefer algorithm\n\n\n\n\n\n\n\nFigure¬†3: Demonstrating the effect of the Jobard-Lefer algorithm on curves drawn in the field\n\n\n\n\n4.1.1 How this part of the algorithm works?\nIn essence, at each step we take while drawing our curve, we look at the curves that are around us, and we compare the distance between our next step and all of these neighboring curves. If this distance between is lower or equal to the separating value (\\(d_{sep}\\)), then, we do not take the next step, and we stop drawing the current line.\nNotice that we are still using the exact same steps I described at Section¬†3 to draw the curves into the field. We are still start with a starting point, then, we start to take sequential steps by following the direction of the angles we encounter through the field.\nHowever, the key difference here, is that we added a new step, or new check to these previous steps. Now, while we are walking through the field, we are constantly calculating distances, and checking these distances to make sure that we are not too close to a neighbour curve.\nFigure¬†4 demonstrates this process visually. And I know what you are thinking! ‚ÄúThese curves in Figure¬†4 doesn‚Äôt look like curves!‚Äù. Just remember the steps I described at Section¬†3. We are essentially drawing curves by connecting the positions where we walked through. Is like I applied a very big zoom over the image while the algorithm is actively drawing the curves.\nSo, at a very low scale, the curves we are drawing are not really curves at all. But at a normal scale, when we zoom out of the image, and look to the image with our own human eyes, the steps are so small, so small, that we cannot perceive the connected dots. The curve effectivelly looks very much like a nice and fluid curve.\nIn the example of Figure¬†4, we have one already existing curve (in blue), and we are currently drawing a second curve into the field. At each step, we calculate the coordinates of the next step we need to make, them, we compare the distance between the position of this next step, and the points from other curves that are close by. If this distance between this next step and any point from the surrounding curves is lower or equal than \\(d_{sep}\\), we ‚Äúcancel‚Äù the next step, and we stop drawing the current curve.\n\n\n\n\n\n\nFigure¬†4: The steps to check if the current curve is too close to a neighbour\n\n\n\nTo calculate the distance between two points in the Cartesian plane, we use another equation from geometry, which is commonly know as the ‚Äúdistance formula‚Äù:\n\\[\nd = \\sqrt{[(x_2 - x_1)^2 + (y_2 - y_1)^2]}\n\\tag{3}\\]\n\n\n4.1.2 The density grid\nI think this specific section of the paper is a little confusing. But if I did understand the authors correctly, then, at each step we take in the field, the Jobard-Lefer algorithm takes the coordinate of out next step, and calculates the distance between this point and all of the points that are near by and that belongs to others (already existing) curves.\nTo find the points that are near by, the algorithm uses a second Cartesian grid that is superposed over the Cartesian plane that our flow field lives. In other words, it uses a second Cartesian plane that uses a different scale, accordingly to the ‚Äúseparating distance‚Äù (\\(d_{sep}\\)). Jobard and Lefer (1997) call this second Cartesian grid as the ‚Äúdensity grid‚Äù.\nSo every coordinate in the Cartesian plane can be mapped to a specific position in the density grid. You do that by scaling the x and y coordinates in the Cartesian to fit the scale used in the density grid. As an example, let‚Äôs consider a Cartesian plane that is 100x100.\nFor example, if you have an x and y coordinates in the flow field, and you want to calculate the corresponding position in the density grid for this specific coordinate, you can use the functions get_density_col() and get_density_row() exposed below:\nint get_density_col (double x, double d_sep) {\n    double c = (x / d_sep) + 1;\n    return (int) c;\n}\n\nint get_density_row (double y, double d_sep) {\n    double r = (y / d_sep) + 1;\n    return (int) r;\n}\nAt Figure¬†5, we are using as an example, a Cartesian plane with dimensions 100x100. With \\(d_{sep} = 2\\), the corresponding density grid becomes a Cartesian grid with dimensions 50x50. You can see in this figure that, the density grid is a grid of \\(50 \\times 50 = 2500\\) cells. Each cell have width and height equal to \\(d_{sep}\\).\nThat is why the functions get_density_col() and get_density_row() always return an integer value as output. Because they calculate the coordinates of the cell in which the original coordinate in the Cartesian plane is mapped to.\nSo a single cell in the density grid can contain multiple points from the Cartesian plane where the flow field lives. Or, on the other side, you can say that multiple points in the Cartesian plane can be mapped to the same cell in the density grid.\nIn the example of Figure¬†5, the coordinate \\(x = 42.312\\) and \\(y = 72.112\\) is mapped to the cell at position \\(x = 22\\) and \\(y = 37\\) in the density grid.\n\n\n\n\n\n\n\n\n\n\n\n(a) 100x100 Cartesian plane\n\n\n\n\n\n\n\n\n\n\n\n(b) The equivalent density grid with dsep = 2\n\n\n\n\n\n\n\nFigure¬†5: The relationship between the Cartesian plane and the density grid."
  }
]